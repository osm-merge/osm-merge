{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"OSM Merge, a Community Project of OpenStreetMap US","text":"<p>\ud83d\udcd6 Documentation: https://osm-merge.github.io/osm-merge/</p> <p>\ud83d\udda5\ufe0f Source Code: https://github.com/osm-merge/osm-merge</p>"},{"location":"#background","title":"Background","text":"<p>This is a project for conflating external map datasets with OpenStreetMap with the ultimate goal of importing it into OpenStreetMap. It is oriented towards processing non OSM external datasets and supports conflation of field data collection using OpenDataKit, as well as other external datasets.</p> <p>The goal of this project is focused on improving remote highway and trail metadata to assist with emergency and recreational access in remote areas. This project uses several data sources to improve the existing highway features in OpenStreetMap. The current data in OpenStreetMap was often imported complete with bugs in the  original dataset, or the only details are highway=track. All of these have a US forest service reference number and name. Adding those makes it much easier to identify where you are and to communicate a location over a radio or phone.</p> <p>There is also access information in the datasets that is useful. This includes access for vehicle types, horses, public/private, etc... that is useful for OpenStreetMap.</p> <p></p> <p>The other goal of this project is to support field data collection using OpenDataKit. The osm-fieldwork project can be used to convert the ODK data files into GeoJson and OSM XML. This project then supports conflating that field collected data with current OpenStreetMap. Otherwise this is a time-consuming process to do manually. This can be used with the Field Mapping Tasking Manager (FMTM) to process the collected data for validation and uploading to OpenStreetMap.</p> <p></p>"},{"location":"#external-datasets","title":"External Datasets","text":"<p>I'm working on a website for all the converted and processed data files that covers every national park or forest for the entire US. From the US forest service there are several datasets with a good license for OpenStreetMap. Warning, these are large files, so hard to work with at first. This project uses a huge amount of data (and disk space) if you start from the original nation wide datasets, which are too large to edit. There is a contrib script in the git sources I use to start breaking down the huge files into managable pieces.</p> <p>The MVUM data hasn't changed in years, so splitting everything up is a one-time task. For OpenStreetMap extracts all the data extracts of highways can be regenerated in a few hours, so it's possible to stay closely in sync with upstream. The original files are available from these sources.</p>"},{"location":"#us-forest-service-data","title":"US Forest Service data","text":"<ul> <li>Motor Vehicle Use Maps (MVUM)</li> <li>USFS Trail maps</li> </ul>"},{"location":"#us-park-service-data","title":"US Park Service data","text":"<ul> <li>NPS Trails</li> </ul>"},{"location":"#for-openstreetmap","title":"For OpenStreetMap","text":"<ul> <li>Geofabrik</li> </ul> <p>Much of the process of conflation is splitting huge datasets into managable sized files for data processing. I have that process mostly automated so I can easily regenerate data extracts any time I make improvements to the conversion process. Currently there isn't any conflated data yet, just the convered data files chopped into manageable sized files. The processed map data is available from here. Please note the website is work in progress. Feedback on the data conversion to OpenStreetMap tagging is appreciated.</p> <p></p>"},{"location":"#programs","title":"Programs","text":"<p>This project comprises of a main program, and multiple utilities. The utilities are used to prepare the datasets for conflating by fixing known bugs. It is very difficult to conflate datasets with wildly different data schemas, so making all the data use a consistent schema is important so tags can be compared.</p>"},{"location":"#utilities","title":"Utilities","text":"<p>The utility programs are used for all data cleaning and other tasks needed to conflate the data files. Known bugs in the datasets are fixed where possible, for example, expanding abbreviations so Rd becomes Road, etc... and also drops all the extraneous fields that aren't for OpenStreetMap. The fields we are most interested in are the name, the official reference number, and the access values.</p> <ul> <li>tm-splitter.py<ul> <li>Generate task grids for the Tasking Manager</li> </ul> </li> <li>mvum.py<ul> <li>Convert forest service MVUM datasets to OpenStreetMap</li> </ul> </li> <li>trails.py<ul> <li>Convert forest service datasets to OpenStreetMap</li> </ul> </li> <li>usgs.py<ul> <li>Convert USGS topographical datasets to OpenStreetMap</li> </ul> </li> <li>osmhighways.py<ul> <li>Data janitor for OpenStreetMap, delete tiger:, etc...</li> </ul> </li> </ul>"},{"location":"#conflator-program","title":"Conflator Program","text":"<p>This program doesn't require a database, unlike the other conflation programs in this project, although adding database support is on the TODO list. It is focused on conflating rural highways and hiking trails. It can also conflate OpenDataKit with OpenStreetMap.</p> <p>It supports conflating any two datasets in either GeoJson or OSM format. While this is currently under heavy development and debugging. I've been processing large amounts of data to track down all the obscure bugs in the original datasets, or the conflation process.</p> <p></p>"},{"location":"#contributing","title":"Contributing","text":"<p>Anyone motivated is welcome to contribute to both the software, or just using these tools and the data for your own map improvements. This is a huge amount of data, as the original source files are nationwide. I'm just focused on my part of the western US. Help improving OpenStreetMap accuracy in remote areas of your own state is a good idea, and can save lives in an emergency. Be a data janitor!</p>"},{"location":"CHANGELOG/","title":"Changelog","text":""},{"location":"LICENSE/","title":"GNU AFFERO GENERAL PUBLIC LICENSE","text":"<p>Version 3, 19 November 2007</p> <p>Copyright (C) 2007 Free Software Foundation, Inc. https://fsf.org/</p> <p>Everyone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed.</p>"},{"location":"LICENSE/#preamble","title":"Preamble","text":"<p>The GNU Affero General Public License is a free, copyleft license for software and other kinds of works, specifically designed to ensure cooperation with the community in the case of network server software.</p> <p>The licenses for most software and other practical works are designed to take away your freedom to share and change the works. By contrast, our General Public Licenses are intended to guarantee your freedom to share and change all versions of a program--to make sure it remains free software for all its users.</p> <p>When we speak of free software, we are referring to freedom, not price. Our General Public Licenses are designed to make sure that you have the freedom to distribute copies of free software (and charge for them if you wish), that you receive source code or can get it if you want it, that you can change the software or use pieces of it in new free programs, and that you know you can do these things.</p> <p>Developers that use our General Public Licenses protect your rights with two steps: (1) assert copyright on the software, and (2) offer you this License which gives you legal permission to copy, distribute and/or modify the software.</p> <p>A secondary benefit of defending all users' freedom is that improvements made in alternate versions of the program, if they receive widespread use, become available for other developers to incorporate. Many developers of free software are heartened and encouraged by the resulting cooperation. However, in the case of software used on network servers, this result may fail to come about. The GNU General Public License permits making a modified version and letting the public access it on a server without ever releasing its source code to the public.</p> <p>The GNU Affero General Public License is designed specifically to ensure that, in such cases, the modified source code becomes available to the community. It requires the operator of a network server to provide the source code of the modified version running there to the users of that server. Therefore, public use of a modified version, on a publicly accessible server, gives the public access to the source code of the modified version.</p> <p>An older license, called the Affero General Public License and published by Affero, was designed to accomplish similar goals. This is a different license, not a version of the Affero GPL, but Affero has released a new version of the Affero GPL which permits relicensing under this license.</p> <p>The precise terms and conditions for copying, distribution and modification follow.</p>"},{"location":"LICENSE/#terms-and-conditions","title":"TERMS AND CONDITIONS","text":""},{"location":"LICENSE/#0-definitions","title":"0. Definitions.","text":"<p>\"This License\" refers to version 3 of the GNU Affero General Public License.</p> <p>\"Copyright\" also means copyright-like laws that apply to other kinds of works, such as semiconductor masks.</p> <p>\"The Program\" refers to any copyrightable work licensed under this License. Each licensee is addressed as \"you\". \"Licensees\" and \"recipients\" may be individuals or organizations.</p> <p>To \"modify\" a work means to copy from or adapt all or part of the work in a fashion requiring copyright permission, other than the making of an exact copy. The resulting work is called a \"modified version\" of the earlier work or a work \"based on\" the earlier work.</p> <p>A \"covered work\" means either the unmodified Program or a work based on the Program.</p> <p>To \"propagate\" a work means to do anything with it that, without permission, would make you directly or secondarily liable for infringement under applicable copyright law, except executing it on a computer or modifying a private copy. Propagation includes copying, distribution (with or without modification), making available to the public, and in some countries other activities as well.</p> <p>To \"convey\" a work means any kind of propagation that enables other parties to make or receive copies. Mere interaction with a user through a computer network, with no transfer of a copy, is not conveying.</p> <p>An interactive user interface displays \"Appropriate Legal Notices\" to the extent that it includes a convenient and prominently visible feature that (1) displays an appropriate copyright notice, and (2) tells the user that there is no warranty for the work (except to the extent that warranties are provided), that licensees may convey the work under this License, and how to view a copy of this License. If the interface presents a list of user commands or options, such as a menu, a prominent item in the list meets this criterion.</p>"},{"location":"LICENSE/#1-source-code","title":"1. Source Code.","text":"<p>The \"source code\" for a work means the preferred form of the work for making modifications to it. \"Object code\" means any non-source form of a work.</p> <p>A \"Standard Interface\" means an interface that either is an official standard defined by a recognized standards body, or, in the case of interfaces specified for a particular programming language, one that is widely used among developers working in that language.</p> <p>The \"System Libraries\" of an executable work include anything, other than the work as a whole, that (a) is included in the normal form of packaging a Major Component, but which is not part of that Major Component, and (b) serves only to enable use of the work with that Major Component, or to implement a Standard Interface for which an implementation is available to the public in source code form. A \"Major Component\", in this context, means a major essential component (kernel, window system, and so on) of the specific operating system (if any) on which the executable work runs, or a compiler used to produce the work, or an object code interpreter used to run it.</p> <p>The \"Corresponding Source\" for a work in object code form means all the source code needed to generate, install, and (for an executable work) run the object code and to modify the work, including scripts to control those activities. However, it does not include the work's System Libraries, or general-purpose tools or generally available free programs which are used unmodified in performing those activities but which are not part of the work. For example, Corresponding Source includes interface definition files associated with source files for the work, and the source code for shared libraries and dynamically linked subprograms that the work is specifically designed to require, such as by intimate data communication or control flow between those subprograms and other parts of the work.</p> <p>The Corresponding Source need not include anything that users can regenerate automatically from other parts of the Corresponding Source.</p> <p>The Corresponding Source for a work in source code form is that same work.</p>"},{"location":"LICENSE/#2-basic-permissions","title":"2. Basic Permissions.","text":"<p>All rights granted under this License are granted for the term of copyright on the Program, and are irrevocable provided the stated conditions are met. This License explicitly affirms your unlimited permission to run the unmodified Program. The output from running a covered work is covered by this License only if the output, given its content, constitutes a covered work. This License acknowledges your rights of fair use or other equivalent, as provided by copyright law.</p> <p>You may make, run and propagate covered works that you do not convey, without conditions so long as your license otherwise remains in force. You may convey covered works to others for the sole purpose of having them make modifications exclusively for you, or provide you with facilities for running those works, provided that you comply with the terms of this License in conveying all material for which you do not control copyright. Those thus making or running the covered works for you must do so exclusively on your behalf, under your direction and control, on terms that prohibit them from making any copies of your copyrighted material outside their relationship with you.</p> <p>Conveying under any other circumstances is permitted solely under the conditions stated below. Sublicensing is not allowed; section 10 makes it unnecessary.</p>"},{"location":"LICENSE/#3-protecting-users-legal-rights-from-anti-circumvention-law","title":"3. Protecting Users' Legal Rights From Anti-Circumvention Law.","text":"<p>No covered work shall be deemed part of an effective technological measure under any applicable law fulfilling obligations under article 11 of the WIPO copyright treaty adopted on 20 December 1996, or similar laws prohibiting or restricting circumvention of such measures.</p> <p>When you convey a covered work, you waive any legal power to forbid circumvention of technological measures to the extent such circumvention is effected by exercising rights under this License with respect to the covered work, and you disclaim any intention to limit operation or modification of the work as a means of enforcing, against the work's users, your or third parties' legal rights to forbid circumvention of technological measures.</p>"},{"location":"LICENSE/#4-conveying-verbatim-copies","title":"4. Conveying Verbatim Copies.","text":"<p>You may convey verbatim copies of the Program's source code as you receive it, in any medium, provided that you conspicuously and appropriately publish on each copy an appropriate copyright notice; keep intact all notices stating that this License and any non-permissive terms added in accord with section 7 apply to the code; keep intact all notices of the absence of any warranty; and give all recipients a copy of this License along with the Program.</p> <p>You may charge any price or no price for each copy that you convey, and you may offer support or warranty protection for a fee.</p>"},{"location":"LICENSE/#5-conveying-modified-source-versions","title":"5. Conveying Modified Source Versions.","text":"<p>You may convey a work based on the Program, or the modifications to produce it from the Program, in the form of source code under the terms of section 4, provided that you also meet all of these conditions:</p> <ul> <li>a) The work must carry prominent notices stating that you modified     it, and giving a relevant date.</li> <li>b) The work must carry prominent notices stating that it is     released under this License and any conditions added under     section 7. This requirement modifies the requirement in section 4     to \"keep intact all notices\".</li> <li>c) You must license the entire work, as a whole, under this     License to anyone who comes into possession of a copy. This     License will therefore apply, along with any applicable section 7     additional terms, to the whole of the work, and all its parts,     regardless of how they are packaged. This License gives no     permission to license the work in any other way, but it does not     invalidate such permission if you have separately received it.</li> <li>d) If the work has interactive user interfaces, each must display     Appropriate Legal Notices; however, if the Program has interactive     interfaces that do not display Appropriate Legal Notices, your     work need not make them do so.</li> </ul> <p>A compilation of a covered work with other separate and independent works, which are not by their nature extensions of the covered work, and which are not combined with it such as to form a larger program, in or on a volume of a storage or distribution medium, is called an \"aggregate\" if the compilation and its resulting copyright are not used to limit the access or legal rights of the compilation's users beyond what the individual works permit. Inclusion of a covered work in an aggregate does not cause this License to apply to the other parts of the aggregate.</p>"},{"location":"LICENSE/#6-conveying-non-source-forms","title":"6. Conveying Non-Source Forms.","text":"<p>You may convey a covered work in object code form under the terms of sections 4 and 5, provided that you also convey the machine-readable Corresponding Source under the terms of this License, in one of these ways:</p> <ul> <li>a) Convey the object code in, or embodied in, a physical product     (including a physical distribution medium), accompanied by the     Corresponding Source fixed on a durable physical medium     customarily used for software interchange.</li> <li>b) Convey the object code in, or embodied in, a physical product     (including a physical distribution medium), accompanied by a     written offer, valid for at least three years and valid for as     long as you offer spare parts or customer support for that product     model, to give anyone who possesses the object code either (1) a     copy of the Corresponding Source for all the software in the     product that is covered by this License, on a durable physical     medium customarily used for software interchange, for a price no     more than your reasonable cost of physically performing this     conveying of source, or (2) access to copy the Corresponding     Source from a network server at no charge.</li> <li>c) Convey individual copies of the object code with a copy of the     written offer to provide the Corresponding Source. This     alternative is allowed only occasionally and noncommercially, and     only if you received the object code with such an offer, in accord     with subsection 6b.</li> <li>d) Convey the object code by offering access from a designated     place (gratis or for a charge), and offer equivalent access to the     Corresponding Source in the same way through the same place at no     further charge. You need not require recipients to copy the     Corresponding Source along with the object code. If the place to     copy the object code is a network server, the Corresponding Source     may be on a different server (operated by you or a third party)     that supports equivalent copying facilities, provided you maintain     clear directions next to the object code saying where to find the     Corresponding Source. Regardless of what server hosts the     Corresponding Source, you remain obligated to ensure that it is     available for as long as needed to satisfy these requirements.</li> <li>e) Convey the object code using peer-to-peer transmission,     provided you inform other peers where the object code and     Corresponding Source of the work are being offered to the general     public at no charge under subsection 6d.</li> </ul> <p>A separable portion of the object code, whose source code is excluded from the Corresponding Source as a System Library, need not be included in conveying the object code work.</p> <p>A \"User Product\" is either (1) a \"consumer product\", which means any tangible personal property which is normally used for personal, family, or household purposes, or (2) anything designed or sold for incorporation into a dwelling. In determining whether a product is a consumer product, doubtful cases shall be resolved in favor of coverage. For a particular product received by a particular user, \"normally used\" refers to a typical or common use of that class of product, regardless of the status of the particular user or of the way in which the particular user actually uses, or expects or is expected to use, the product. A product is a consumer product regardless of whether the product has substantial commercial, industrial or non-consumer uses, unless such uses represent the only significant mode of use of the product.</p> <p>\"Installation Information\" for a User Product means any methods, procedures, authorization keys, or other information required to install and execute modified versions of a covered work in that User Product from a modified version of its Corresponding Source. The information must suffice to ensure that the continued functioning of the modified object code is in no case prevented or interfered with solely because modification has been made.</p> <p>If you convey an object code work under this section in, or with, or specifically for use in, a User Product, and the conveying occurs as part of a transaction in which the right of possession and use of the User Product is transferred to the recipient in perpetuity or for a fixed term (regardless of how the transaction is characterized), the Corresponding Source conveyed under this section must be accompanied by the Installation Information. But this requirement does not apply if neither you nor any third party retains the ability to install modified object code on the User Product (for example, the work has been installed in ROM).</p> <p>The requirement to provide Installation Information does not include a requirement to continue to provide support service, warranty, or updates for a work that has been modified or installed by the recipient, or for the User Product in which it has been modified or installed. Access to a network may be denied when the modification itself materially and adversely affects the operation of the network or violates the rules and protocols for communication across the network.</p> <p>Corresponding Source conveyed, and Installation Information provided, in accord with this section must be in a format that is publicly documented (and with an implementation available to the public in source code form), and must require no special password or key for unpacking, reading or copying.</p>"},{"location":"LICENSE/#7-additional-terms","title":"7. Additional Terms.","text":"<p>\"Additional permissions\" are terms that supplement the terms of this License by making exceptions from one or more of its conditions. Additional permissions that are applicable to the entire Program shall be treated as though they were included in this License, to the extent that they are valid under applicable law. If additional permissions apply only to part of the Program, that part may be used separately under those permissions, but the entire Program remains governed by this License without regard to the additional permissions.</p> <p>When you convey a copy of a covered work, you may at your option remove any additional permissions from that copy, or from any part of it. (Additional permissions may be written to require their own removal in certain cases when you modify the work.) You may place additional permissions on material, added by you to a covered work, for which you have or can give appropriate copyright permission.</p> <p>Notwithstanding any other provision of this License, for material you add to a covered work, you may (if authorized by the copyright holders of that material) supplement the terms of this License with terms:</p> <ul> <li>a) Disclaiming warranty or limiting liability differently from the     terms of sections 15 and 16 of this License; or</li> <li>b) Requiring preservation of specified reasonable legal notices or     author attributions in that material or in the Appropriate Legal     Notices displayed by works containing it; or</li> <li>c) Prohibiting misrepresentation of the origin of that material,     or requiring that modified versions of such material be marked in     reasonable ways as different from the original version; or</li> <li>d) Limiting the use for publicity purposes of names of licensors     or authors of the material; or</li> <li>e) Declining to grant rights under trademark law for use of some     trade names, trademarks, or service marks; or</li> <li>f) Requiring indemnification of licensors and authors of that     material by anyone who conveys the material (or modified versions     of it) with contractual assumptions of liability to the recipient,     for any liability that these contractual assumptions directly     impose on those licensors and authors.</li> </ul> <p>All other non-permissive additional terms are considered \"further restrictions\" within the meaning of section 10. If the Program as you received it, or any part of it, contains a notice stating that it is governed by this License along with a term that is a further restriction, you may remove that term. If a license document contains a further restriction but permits relicensing or conveying under this License, you may add to a covered work material governed by the terms of that license document, provided that the further restriction does not survive such relicensing or conveying.</p> <p>If you add terms to a covered work in accord with this section, you must place, in the relevant source files, a statement of the additional terms that apply to those files, or a notice indicating where to find the applicable terms.</p> <p>Additional terms, permissive or non-permissive, may be stated in the form of a separately written license, or stated as exceptions; the above requirements apply either way.</p>"},{"location":"LICENSE/#8-termination","title":"8. Termination.","text":"<p>You may not propagate or modify a covered work except as expressly provided under this License. Any attempt otherwise to propagate or modify it is void, and will automatically terminate your rights under this License (including any patent licenses granted under the third paragraph of section 11).</p> <p>However, if you cease all violation of this License, then your license from a particular copyright holder is reinstated (a) provisionally, unless and until the copyright holder explicitly and finally terminates your license, and (b) permanently, if the copyright holder fails to notify you of the violation by some reasonable means prior to 60 days after the cessation.</p> <p>Moreover, your license from a particular copyright holder is reinstated permanently if the copyright holder notifies you of the violation by some reasonable means, this is the first time you have received notice of violation of this License (for any work) from that copyright holder, and you cure the violation prior to 30 days after your receipt of the notice.</p> <p>Termination of your rights under this section does not terminate the licenses of parties who have received copies or rights from you under this License. If your rights have been terminated and not permanently reinstated, you do not qualify to receive new licenses for the same material under section 10.</p>"},{"location":"LICENSE/#9-acceptance-not-required-for-having-copies","title":"9. Acceptance Not Required for Having Copies.","text":"<p>You are not required to accept this License in order to receive or run a copy of the Program. Ancillary propagation of a covered work occurring solely as a consequence of using peer-to-peer transmission to receive a copy likewise does not require acceptance. However, nothing other than this License grants you permission to propagate or modify any covered work. These actions infringe copyright if you do not accept this License. Therefore, by modifying or propagating a covered work, you indicate your acceptance of this License to do so.</p>"},{"location":"LICENSE/#10-automatic-licensing-of-downstream-recipients","title":"10. Automatic Licensing of Downstream Recipients.","text":"<p>Each time you convey a covered work, the recipient automatically receives a license from the original licensors, to run, modify and propagate that work, subject to this License. You are not responsible for enforcing compliance by third parties with this License.</p> <p>An \"entity transaction\" is a transaction transferring control of an organization, or substantially all assets of one, or subdividing an organization, or merging organizations. If propagation of a covered work results from an entity transaction, each party to that transaction who receives a copy of the work also receives whatever licenses to the work the party's predecessor in interest had or could give under the previous paragraph, plus a right to possession of the Corresponding Source of the work from the predecessor in interest, if the predecessor has it or can get it with reasonable efforts.</p> <p>You may not impose any further restrictions on the exercise of the rights granted or affirmed under this License. For example, you may not impose a license fee, royalty, or other charge for exercise of rights granted under this License, and you may not initiate litigation (including a cross-claim or counterclaim in a lawsuit) alleging that any patent claim is infringed by making, using, selling, offering for sale, or importing the Program or any portion of it.</p>"},{"location":"LICENSE/#11-patents","title":"11. Patents.","text":"<p>A \"contributor\" is a copyright holder who authorizes use under this License of the Program or a work on which the Program is based. The work thus licensed is called the contributor's \"contributor version\".</p> <p>A contributor's \"essential patent claims\" are all patent claims owned or controlled by the contributor, whether already acquired or hereafter acquired, that would be infringed by some manner, permitted by this License, of making, using, or selling its contributor version, but do not include claims that would be infringed only as a consequence of further modification of the contributor version. For purposes of this definition, \"control\" includes the right to grant patent sublicenses in a manner consistent with the requirements of this License.</p> <p>Each contributor grants you a non-exclusive, worldwide, royalty-free patent license under the contributor's essential patent claims, to make, use, sell, offer for sale, import and otherwise run, modify and propagate the contents of its contributor version.</p> <p>In the following three paragraphs, a \"patent license\" is any express agreement or commitment, however denominated, not to enforce a patent (such as an express permission to practice a patent or covenant not to sue for patent infringement). To \"grant\" such a patent license to a party means to make such an agreement or commitment not to enforce a patent against the party.</p> <p>If you convey a covered work, knowingly relying on a patent license, and the Corresponding Source of the work is not available for anyone to copy, free of charge and under the terms of this License, through a publicly available network server or other readily accessible means, then you must either (1) cause the Corresponding Source to be so available, or (2) arrange to deprive yourself of the benefit of the patent license for this particular work, or (3) arrange, in a manner consistent with the requirements of this License, to extend the patent license to downstream recipients. \"Knowingly relying\" means you have actual knowledge that, but for the patent license, your conveying the covered work in a country, or your recipient's use of the covered work in a country, would infringe one or more identifiable patents in that country that you have reason to believe are valid.</p> <p>If, pursuant to or in connection with a single transaction or arrangement, you convey, or propagate by procuring conveyance of, a covered work, and grant a patent license to some of the parties receiving the covered work authorizing them to use, propagate, modify or convey a specific copy of the covered work, then the patent license you grant is automatically extended to all recipients of the covered work and works based on it.</p> <p>A patent license is \"discriminatory\" if it does not include within the scope of its coverage, prohibits the exercise of, or is conditioned on the non-exercise of one or more of the rights that are specifically granted under this License. You may not convey a covered work if you are a party to an arrangement with a third party that is in the business of distributing software, under which you make payment to the third party based on the extent of your activity of conveying the work, and under which the third party grants, to any of the parties who would receive the covered work from you, a discriminatory patent license (a) in connection with copies of the covered work conveyed by you (or copies made from those copies), or (b) primarily for and in connection with specific products or compilations that contain the covered work, unless you entered into that arrangement, or that patent license was granted, prior to 28 March 2007.</p> <p>Nothing in this License shall be construed as excluding or limiting any implied license or other defenses to infringement that may otherwise be available to you under applicable patent law.</p>"},{"location":"LICENSE/#12-no-surrender-of-others-freedom","title":"12. No Surrender of Others' Freedom.","text":"<p>If conditions are imposed on you (whether by court order, agreement or otherwise) that contradict the conditions of this License, they do not excuse you from the conditions of this License. If you cannot convey a covered work so as to satisfy simultaneously your obligations under this License and any other pertinent obligations, then as a consequence you may not convey it at all. For example, if you agree to terms that obligate you to collect a royalty for further conveying from those to whom you convey the Program, the only way you could satisfy both those terms and this License would be to refrain entirely from conveying the Program.</p>"},{"location":"LICENSE/#13-remote-network-interaction-use-with-the-gnu-general-public-license","title":"13. Remote Network Interaction; Use with the GNU General Public License.","text":"<p>Notwithstanding any other provision of this License, if you modify the Program, your modified version must prominently offer all users interacting with it remotely through a computer network (if your version supports such interaction) an opportunity to receive the Corresponding Source of your version by providing access to the Corresponding Source from a network server at no charge, through some standard or customary means of facilitating copying of software. This Corresponding Source shall include the Corresponding Source for any work covered by version 3 of the GNU General Public License that is incorporated pursuant to the following paragraph.</p> <p>Notwithstanding any other provision of this License, you have permission to link or combine any covered work with a work licensed under version 3 of the GNU General Public License into a single combined work, and to convey the resulting work. The terms of this License will continue to apply to the part which is the covered work, but the work with which it is combined will remain governed by version 3 of the GNU General Public License.</p>"},{"location":"LICENSE/#14-revised-versions-of-this-license","title":"14. Revised Versions of this License.","text":"<p>The Free Software Foundation may publish revised and/or new versions of the GNU Affero General Public License from time to time. Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns.</p> <p>Each version is given a distinguishing version number. If the Program specifies that a certain numbered version of the GNU Affero General Public License \"or any later version\" applies to it, you have the option of following the terms and conditions either of that numbered version or of any later version published by the Free Software Foundation. If the Program does not specify a version number of the GNU Affero General Public License, you may choose any version ever published by the Free Software Foundation.</p> <p>If the Program specifies that a proxy can decide which future versions of the GNU Affero General Public License can be used, that proxy's public statement of acceptance of a version permanently authorizes you to choose that version for the Program.</p> <p>Later license versions may give you additional or different permissions. However, no additional obligations are imposed on any author or copyright holder as a result of your choosing to follow a later version.</p>"},{"location":"LICENSE/#15-disclaimer-of-warranty","title":"15. Disclaimer of Warranty.","text":"<p>THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY APPLICABLE LAW. EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM \"AS IS\" WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH YOU. SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING, REPAIR OR CORRECTION.</p>"},{"location":"LICENSE/#16-limitation-of-liability","title":"16. Limitation of Liability.","text":"<p>IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.</p>"},{"location":"LICENSE/#17-interpretation-of-sections-15-and-16","title":"17. Interpretation of Sections 15 and 16.","text":"<p>If the disclaimer of warranty and limitation of liability provided above cannot be given local legal effect according to their terms, reviewing courts shall apply local law that most closely approximates an absolute waiver of all civil liability in connection with the Program, unless a warranty or assumption of liability accompanies a copy of the Program in return for a fee.</p> <p>END OF TERMS AND CONDITIONS</p>"},{"location":"LICENSE/#how-to-apply-these-terms-to-your-new-programs","title":"How to Apply These Terms to Your New Programs","text":"<p>If you develop a new program, and you want it to be of the greatest possible use to the public, the best way to achieve this is to make it free software which everyone can redistribute and change under these terms.</p> <p>To do so, attach the following notices to the program. It is safest to attach them to the start of each source file to most effectively state the exclusion of warranty; and each file should have at least the \"copyright\" line and a pointer to where the full notice is found.</p> <pre><code>    &lt;one line to give the program's name and a brief idea of what it does.&gt;\n    Copyright (C) &lt;year&gt;  &lt;name of author&gt;\n\n    This program is free software: you can redistribute it and/or modify\n    it under the terms of the GNU Affero General Public License as\n    published by the Free Software Foundation, either version 3 of the\n    License, or (at your option) any later version.\n\n    This program is distributed in the hope that it will be useful,\n    but WITHOUT ANY WARRANTY; without even the implied warranty of\n    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n    GNU Affero General Public License for more details.\n\n    You should have received a copy of the GNU Affero General Public License\n    along with this program.  If not, see &lt;https://www.gnu.org/licenses/&gt;.\n</code></pre> <p>Also add information on how to contact you by electronic and paper mail.</p> <p>If your software can interact with users remotely through a computer network, you should also make sure that it provides a way for users to get its source. For example, if your program is a web application, its interface could display a \"Source\" link that leads users to an archive of the code. There are many ways you could offer source, and different solutions will be better for different programs; see section 13 for the specific requirements.</p> <p>You should also get your employer (if you work as a programmer) or school, if any, to sign a \"copyright disclaimer\" for the program, if necessary. For more information on this, and how to apply and follow the GNU AGPL, see https://www.gnu.org/licenses/.</p>"},{"location":"about/","title":"Conflator","text":"<p>This is a project for conflating map data, with the ultimate goal of importing it into OpenStreetMap(OSM).</p> <p>It is oriented towards conflating external datasets with existing OSM data. External data is usually polygons (building footprints), or POIs. These days there are multiple publically available building footprint datasets with an appropriate license for OSM. The problem is this data needs to be validated.</p> <p>Due to the flexibility of the OSM data schema, it's impossible to get 100% perfect conflation. But the purely manual conflation is very time consuming and tedious. This project aims to do as much as possible to aid the validator to make their work as efficient as possible.</p>"},{"location":"basemapper/","title":"Basemapper.py","text":"<p>Basemapper is a program that creates basemaps for mobile apps in the mbtiles and sqlitedb formats. These formats are commonly used by mobile mapping applications like Osmand and ODK Collect. There are two primary formats:</p> <ul> <li>mbtiles, supported by many apps.</li> <li>sqlitedb, supported only by Osmand</li> </ul> <p>Both of these use formats use underlying sqlite3, with similar database schemas. The schema are a simple XYZ that stores a png or jpeg image. When the entire planet is chopped into squares, there is a relation between which map tile contains the GPS coordinates you want. Small zoom levels cover a large area, higher zoom levels a smaller area.</p> <p>Basemapper does not store anything in memory, all processing is done as a stream so large areas can be downloaded. Time to go buy a really large hard drive. You can also use this map tile cache for any program that supports a TMS data source like JOSM. Luckily once downloaded, you don't have to update the map tile cache very often, but it's also easy to do so when you need to. When I expect to be working offline, I commonly download a larger area, and then in the field produce the smaller files.</p> <p>Basemapper. downloads map tiles to a cache and uses them to generate the output files. It does not perform data conversion. The resulting output can be used for visualizing geographic data and analyzing survey responses in a spatial context. The script provides various command-line options for customizing the output, such as setting the zoom levels, boundary, tile cache, output file name, and more.</p>"},{"location":"basemapper/#database-schemas","title":"Database Schemas","text":"<p>Mbtiles are used by multiple mobile apps, but our usage is primarly for ODK Collect. Imagery basemaps are very useful for two reasons. One, the map data may be lacking, so the imagery helps one to naviagte. For ODK Collect the other advantage is you can select the location based on where the building is, instead of were you are standing. Mbtiles are pretty straight forward.</p> <p>The sqlitedb schema used by Osmand looks the same at first, but has one big difference. In this schema it tops out at zoom level 16, so instead of incrementing, it decrements the zoom level. This obscure detail took me a while to figure out, it isn't documented anywhere.</p>"},{"location":"basemapper/#mbtiles","title":"mbtiles","text":"<pre><code>CREATE TABLE tiles (zoom_level integer, tile_column integer, tile_row integer, tile_data blob);\nCREATE INDEX tiles_idx on tiles (zoom_level, tile_column, tile_row);\nCREATE TABLE metadata (name text, value text);\nCREATE UNIQUE INDEX metadata_idx  ON metadata (name);\n</code></pre>"},{"location":"basemapper/#sqlitedb","title":"sqlitedb","text":"<pre><code>CREATE TABLE tiles (x int, y int, z int, s int, image blob, PRIMARY KEY (x,y,z,s));\nCREATE INDEX IND on tiles (x,y,z,s);\nCREATE TABLE info (maxzoom Int, minzoom Int);\nCREATE TABLE android_metadata (en_US);\n</code></pre>"},{"location":"basemapper/#usage","title":"Usage","text":"<p>The basemapper.py script is run from the command line when running standalone, or the class can be imported into python programs. The Field Mapping Tasking Manager uses this as part of a (FastAPI])https://fastapi.tiangolo.com/) backend for the website.</p> <p>The first time you run basemapper.py, it'll start downloading map tiles, which may take a long time. Often the upstream source is slow. It is not unusual for downloading tiles, especially at higher zoom levels may tak an entire day. Once tiles are download, producing the outout tiles is quick as then it's just packaging. In areas where I work frequentely, I usually download a large area even if it takes a week or more so it's available when I need it. On my laptop I actually have a map tile cache for the entire state of Colorado, as well as many large areas of Nepal, Turkey, Kenya, Uganda, and Tanzania.</p>"},{"location":"basemapper/#options","title":"Options","text":"<p>The basic syntax is as follows:</p> <ul> <li>-h, --help show this help message and exit</li> <li>-v, --verbose verbose output</li> <li>-b BOUNDARY, --boundary BOUNDARY - The boundary for the area you want, as BBOX string or geojson file.</li> <li>-z ZOOMS, --zooms ZOOMS - The Zoom levels</li> <li>-o OUTFILE, --outfile - OUTFILE Output file name</li> <li>-d OUTDIR, --outdir OUTDIR -Output directory name for tile cache</li> <li>-s {ersi,bing,topo,google,oam}, --source {ersi,bing,topo,google,oam} - Imagery source</li> </ul> <p>The suffix of the output file is either mbtiles or sqlitedb, which is used to select the output format. The boundary file, if specified, must be in GeoJson format. If in BBOX string format, it must be comma separated: \"minX,minY,maxX,maxY\".</p>"},{"location":"basemapper/#imagery-sources","title":"Imagery Sources","text":"<ul> <li>ESRI - Environmental Systems Research Institute</li> <li>Bing - Microsoft Bing imagery</li> <li>Topo - USGS topographical maps (US only)</li> <li>OAM - OpenAerialMap</li> </ul> <p>The default output directory is /var/www/html. The actual subdirectory is the source name with tiles appended, so for example /var/www/html/oamtiles. Putting the map tiles into webroot lets JOSM or QGIS use them when working offline.</p>"},{"location":"basemapper/#examples","title":"Examples","text":""},{"location":"basemapper/#example-1","title":"Example 1:","text":"<p>Generate a basemap for Osmand using ERSI imagery, for an area specified by a geojson bounding box, and supporting zoom levels 12 through 19.</p> <pre><code>[path]/basemapper.py -z 12-19 -b test.geojson -o test.sqlitedb -s esri\n</code></pre>"},{"location":"basemapper/#example-2","title":"Example 2:","text":"<p>As above, but mbtiles format, and Bing imagery source. The <code>-v</code> option enables verbose output, which will show more details about the download and processing progress. Also only download a single zoon level.</p> <pre><code>[path]/basemapper.py -v -z 16 -b test.geojson -o test.mbtiles -s bing\n</code></pre>"},{"location":"boundaries/","title":"Boundaries","text":"<p>Good boundaries are critical to being able to chop the large files into manageble pieces.</p>"},{"location":"boundaries/#boundary-sources","title":"Boundary Sources","text":""},{"location":"boundaries/#openstreetmap","title":"OpenStreetMap","text":"<p>It is possible to pull boundaries out of OpenStreetMap, but a warning, many aren't very good. In OSM, some boundaries are a Way (Polygon) some are relations, some are missing sections. Extracting many boundaries is not for the faint of heart... There is a website attempting to do this. If you only need a few boundaries, OSM works fine, but you might have to do a little manual cleanup.</p>"},{"location":"boundaries/#administrative-sources","title":"Administrative Sources","text":"<p>I use official administrative boundary datasets. While these are all public domain, I have no interest in uploadinmg them to OSM, which would be a whole other project. And I only need them for data chopping anyway. You can get the official boundaries from these two sources.</p> <p>National Forests</p> <p>National Parks</p> <p>These datasets are national wise, so need to be split into each forest or park. I wrote a program called  tm-splitter that reads in the large file, and then splits each forest and park into a standalone file. This standlone file is a MultiPolygon, as many forests have multiple sections that aren't actually connected. But at this point, you have small enough boundaries to start on making data extracts for conflation.</p>"},{"location":"boundaries/#boundary-problems","title":"Boundary Problems","text":"<p>Most of the National Park boundaries are ofte a single polygon, as many national parks are smaller than national forests. National Forest boundaries have an interesting set of problems that need to be cleaned up before they're usable.</p>"},{"location":"boundaries/#small-outholdings","title":"Small Outholdings","text":"<p>National Forests often have multiple small Polygons that are outside of the actual forest boundary. These appear to be administrative buildings. Ranger stations, visitors centers, etc... These are all useles for our goal of improving remote trails and highways.</p> <p>When the tm-splitter utility parses each MultiPolygon into it's indivugal Polygons, these small outholdings are ignored, leaving only the actual forest boundaries.</p>"},{"location":"boundaries/#inner-polygons","title":"Inner Polygons","text":"<p>Since we're only interested in using these boundaries for making data extracts, some have interior Polygons that have other ownership or public land designations. Since we're using boundaries to make data extracts, I may drop all inner Polygons, but so far they don't see to cause any problems.</p>"},{"location":"calculations/","title":"Conflation Calculations","text":"<p>Part of the fun of external datasets, especially some that have been around long time like the MVUM data is the the variety of inconsistencies in the data. While OpenStreetMap itself is a bit overly flexible at time, so is external data. And some of the old data has been converted from other formats several times, with bugs getting introduced each time.</p>"},{"location":"calculations/#geometries","title":"Geometries","text":"<p>OpenStreetMap has relations, which are a collection of references to other features. External data may have LineStrings, MultiLineStrings or a GeometryCollection, all in the same file! For all calculations the MultiLineString and GeometryCollections are taken apart, so the calculations are between OSM data and that segment of the external data. Since this may product multiple values, those need to be evaluated and the most likely one returned.</p> <p>It gets more fun as sometimes the MVUM dataset is missing entire segments. Course sometimes OSM is too. conflation successfully merges the MVUM dataset tags for the segments if they match onto the single OSM way.</p> <p>![Screenshot\\ from\\ 2024-10-19\\ 14-06-00.png])</p>"},{"location":"calculations/#distance","title":"Distance","text":"<p>A simple distance calculation is performed after transforming the coordinate system from global degrees to meters. The result is compared to a threshold distance, and any feature within that threshold is added to a list of possible matches. After a few features are found in the required distance, matching stops and then the next feature to be conflated is started on the same process.</p> <p>If the highway is a GeometryCollection or MultiLineString, then it's split into segments, and each one is checked for distance. The closest one is what is returned.</p>"},{"location":"calculations/#slope-and-angle","title":"Slope and Angle","text":"<p>Distance often will return features that are close to each other, but often they are spur roads off the more major one. So when two highway segments are found close to each other, the angle between them is calculated. This works well to differentiate between the more major highway, and the spur road that splits off from that.</p> <p>If the highway is a GeometryCollection or MultiLineString, then it's split into segments, and each one is checked for the angle. The closest one is what is returned.</p> <p>Sometimes the geometry of the feature in OSM was imported from the same external dataset. At that point it's an exact match, so the distance, the slope, and the angle will all be 0.0.</p>"},{"location":"calculations/#tag-checking","title":"Tag Checking","text":"<p>Once there is at least one candidate within the parameters of distance and angle, then the tags are checked for matches. The tags we are primarily interested in are name(s) and reference number(s) of each MVUM road or trail. Some of the existing features in OpenStreetMap may be inaccurate as to the proper name and reference. And of course each feature may have an alt_name or both a ref and a ref:usfs. Due to the wonders of inconsistent data, a fuzzy string comparison is done. This handles most of the basic issues, like capitalization, one or 2 characters difference, etc... Anything above the threshold is considered a probably match, and increments a counter. This value is included in the conflated results, and is often between 1-3.</p> <p>The reference numbers between the two datasets is also compared. There is often a reference number in OSM already, but no name. The external dataset has the name, so we want to update OSM with that. In addition, the external datasets often have access information. Seasonal access, private land, or different types of vehicles which can be added to OSM.</p>"},{"location":"calculations/#tag-merging","title":"Tag Merging","text":"<p>The conflation process for merging tags uses the concept of primary and secondary datasets. The primary is considered to have the true value for a highway or trail. For example, if the name in the two datasets doesn't match, the secondary will then rename the current value to old_something. The primary's version becomes the same. Some with reference numbers.</p> <p>Other tags from the primary can also be merged, overriding what is currently in OSM. Once again, the old values are renamed, not deleted. When validating in JOSM, you can see both versions and make a final determination as to what is the correct value. Often it's just spelling differences.</p> <p>For all the features in OSM that only have a highway=something as a tag, all the desired tags from the primary dataset are added.</p> <p>For some tags like surface and smoothness, the value in OSM is potentially more recent, so those are not updated. For any highway feature lacking those tags, they get added.</p> <p>Optionally the various access tags for private, atv, horse, motorcycle, etc... are set in the post conflation dataset if they have a value in the external dataset. </p>"},{"location":"calculations/#highway-segments","title":"Highway Segments","text":"<p>This turns out to be the challenging part of highway conflation. In any highway dataset a longer highway may be broken into segments. These changing segments may be due to the surface changing, speed limit changing. etc... and may be organized into a group of some kind.</p> <p>The fun starts when the segments used for the OSM highway aren't the same as the segments in the external dataset's geometry. Often in the external dataset there are no segments at all, just a long LineString. In OSM the same highway may be broken into multiple segments.</p> <p>The algorithm for the geometry calculations tries to compare each segment from the primary dataset (if there are any), with any highway segment in the secondary dataset (probably OSM). Later when processing all the possible segments of any highway close to the primary segment, the name and reference numbers are checked, and if they match, the new tags from the external dataset are applied to all the matching segments in the secondary.</p>"},{"location":"calculations/#issues","title":"Issues","text":"<p>Conflation is never 100% accurate due to the wonderful um... \"flexibility\" of the datasets. Minor tweaks to the steering parameters for the distance, angle, and fuzzy string matching can produce slightly different results. I often run the same datasets with different parameters looking for the best results.</p>"},{"location":"calculations/#evaluating-the-results","title":"Evaluating The Results","text":""},{"location":"calculations/#debug-tags","title":"Debug Tags","text":"<p>Currently a few tags are added to each feature to aid in validating and debugging the conflation process. These should obviously be removed before uploading to OSM. They'll be removed at a future date after more validation. These are:</p> <ul> <li>hits - The number of matching tags in a feature</li> <li>name_ratio - The ratio for name matching</li> <li>ref_ratio - The ratio for USFS reference number matching</li> <li>dist -  The distance between features</li> <li>angle - The angle between two features</li> <li>slope - The slope between two features</li> </ul>"},{"location":"calculations/#decision-matrix","title":"Decision Matrix","text":"<p>Initially each feature in the primary data source is compared to each feature in the secondary data source. The first step is the distance calculation, eliminating everything outside the threshold distance. Each feature from the secondary dataset within range is put on a list of possible matches. The angle and slope between the external feature and OSM is also checked.</p> <p>After searching through all of the secondary data, any features are found within the desired distance, slope and angle between the primary and secondary datasets. They are added to a list of possible matches. The slopes and angle is checked to identify branches off the highway that often have a similar name and reference number.</p> <p>If there are any features left in the secondary dataset, after the spatial calculations, then the tags are checked for matches.</p>"},{"location":"calculations/#name-matching","title":"Name Matching","text":"<p>Names are compared with fuzzy logic. This handles minor spelling differences. All names are converted to lowercase when being compared so there are no problems with capitalization. The ratio from the fuzzy matching is returned as a float, and is the name_ratio for name matching, and  ref_ratios for  reference matching in the debug logs. There is a lot of variety in names in most datasets.</p>"},{"location":"calculations/#reference-matching","title":"Reference Matching","text":"<p>Reference matching is similar other than the prefix of the reference number has to be taken into consideration. A ref might be a local county road, so starts with a __CR __ then the number. State and federal highways use a similar scheme. Since we're focused on remote highways, those are left unchanged by conflation. For the OSM Merge use case, the only prefix we care about are __FR __, or __FS __. The ratio from the fuzzy matching is returned as a float, and is the  ref_ratio in the debug tags.</p>"},{"location":"calculations/#changing-tags","title":"Changing Tags","text":"<p>When cleaning up OSM features, sometimes the USFS reference number is in the name tag. Often this uses various prefixes, but these can be identified, and moved to the correct ref:usfs tag from the name tag. Or the reference is in a ref tag, and the same change is made, it's moved to the ref:usfs tag. This change helps the conflation process when trying to match tags since this process is done by the MVUM conversion script, instead of in the conflation algorithm.</p>"},{"location":"calculations/#issues_1","title":"Issues","text":"<p>Conflation is rarely perfect. The biggest issue is due to large differences in the length of the highway or multiple segments. This fails to match by geometry within a reasonable distance. In this case the primary dataset feature is considered a new feature, so winds up in the new feature output file, instead of the conflated highways output file.</p>"},{"location":"calculations/#evaluating-possible-matches","title":"Evaluating Possible Matches","text":"<p>This is the key to conflation, evaluating the debug values to determine the most likely match. Each highway segment within the desired distance is put on a list, along with the results of comparing the tags. This is just a list of possible matches, which then needs to have the statistics evaluated to determine the best match.</p> <p>The primary values for conflation after geometry matching is comparing the tags between the external dataset and OSM. Many of these remote highways in OSM only have highway=track, whereas the external dataset may have a name and reference number. If the highway segments are reasonably similar, this generates a feature in the output data with the tags from the external dataset. When there are tags in both datasets for the same features, this is where things get interesting.</p> <p>There are three tests made to each feature. The first test is to compare the name tag between the primary and secondary features. This is a fuzzy string match, usually any value over 85 is a good match with some variation. The variation is usually spelling differences, or trail vs road for the name of a highway segment. A match ratio of 100 is of course a solid match.</p> <p>The other two tests are for the USFS reference number. The reference numbers have two parts, the prefix and the number. The number is an alphanumeric, and may also include a period. In OSM data, the prefix is usually FS or FR, but often other wild variations like Nsfr. The prefix is compared, and the number is compared without the prefix. If the number matches, it's considered a probable hit even if the prefix differs. a hit is a rough guess as to the confidence the two features match.</p> <p>If there is a match of the name or reference number, the hits value gets set to 1. To determine what was matched, it's possible to look at the additional values of name_ratio and ref_ratio, which are returned from tag checking. Since there is likely additional metadata in the external dataset, those tags are then merged with the OSM ones.</p> <p>If both the tags and the reference number match, the hits gets set to 2. The tags name_ratio and ref_ratio are still set, so can be used to evaluate whether to trust the name or the reference match the possible match more, since it's a fuzzy string match. Often the reference number match is slightly off as in OSM some features use ref=FS 123, where ref=FR 123 is preferred. The reference number itself, without the prefix is also checked. This then goes in the output file to change to the preferred prefix for USFS highways.</p> <p>If hits is set to 3, then the name, the reference number prefix and the reference prefix all match 100%, so this feature is not put in the output file as no updates need to be made. Often when the number match isn't 100% (hits == 2), it's because OSM has FR 123, and the external dataset has FR 123.1 or FR 123A, or FS 123. OSM Merge changes the prefix from FS to FR* when it's used to make searching the data more consistent when using mobile apps.</p> <p>Since each highway be a segment of the entire feature, if there is a good match with the name, then any other of the possible features with that name gets the same metadata.</p>"},{"location":"conflation/","title":"Conflating External Datasets","text":"<p>This project is the merging of several programs for conflating external datasets with OpenStreetMap data developed at HOT. These were originally developed for large scale building imports using MS Footprints in East Africa, and to also work with conflating data collected with OpenDataKit for the Field Mapping Tasking Manager project.</p>"},{"location":"conflation/#the-data-files","title":"The Data Files","text":"<p>While any name can be used for the OSM database, I usually default to naming the OpenStreetMap database the country name as used in the data file. Other datasets have their own schema, and can be imported with ogr2ogr, or using python to write a custom importer. In that case I name the database after the dataset source. Past versions of this program could conflate between multiple datasets, so it's good to keep things clear.</p>"},{"location":"conflation/#overture-data","title":"Overture Data","text":"<p>The Overture Foundation (https://www.overturemaps.org) has been recently formed to build a competitor to Google Maps. The plan is to use OpenStreetMap (OSM) data as a baselayer, and layer other datasets on top. The currently available data (July 2023) has 13 different datasets in addition to the OSM data. It is available here. It also includes a snapshot  of OSM data from the same time frame. Other than the OSM data and MS Footprints, all the current additional data is primarily US specific, and often contains multiple copies of the same dataset, but from different organization.</p> <p>The osm-rawdata python module has a utility that'll import the Parquet data files into the postgress database schema used by multiple projects at HOT. That schema is designed for data analysis, unlike the standard OSM database schema. There is more detail in these notes I've written about importing Overture Data into postgres.</p>"},{"location":"conflation/#duplicate-buildings","title":"Duplicate Buildings","text":"<p>This is the primary conflation task. Because of offsets in the satellite imagery used for the original buildings, there is rarely an exact duplicate, only similar. The only times when you see an exact duplicate, it's because the same source data is in multiple other datasets. The orientation may be different even if the same rough size, or it'll be roughly in the same position, but differing sizes. Several checks are made to determine duplicates. First is to check for any intersection of the two polygons. If the two polygons intersection it's an overlapping building or possibly duplicate. Any building in the footprint data that is found to be a duplicate is removed from the output data file.</p>"},{"location":"conflation/#overlapping-buildings","title":"Overlapping Buildings","text":"<p>It is entirely possible that a new building in the footprints data may overlap with an existing building in OSM. It wouldn't be overlapping in the footprints data. Since this requires human intervention to fix, these buildings are left in the output data, but flagged with a debugging tag of overlapping=yes. There is also many occurances where the building being imported has a better building geometry than OSM, so the best one should be selected.</p> <p>Using the HOT Underpass project, it is possible to scan the building geometries and either delete the bad geometry one, or flag it in the result data files for a human to validate the results.</p>"},{"location":"conflation/#known-problems","title":"Known Problems","text":"<p>There are two main issues with ML/AI derived building footprints, Buildings that are very close together, like the business section in many areas of the world, do not get marked as separate buildings. Instead the entire block of buildings is a single polygon. This will eventually get fixed by drone mapping, where there can be more of a street view of the buildings that you can't get using existing satellite imagery.</p> <p>The other problem is that as processing satellite imagery is that buildings are recognized by shading differences, so often features are flagged as buildings that don't actually exist. For example, big rocks in the desert, or haystacks in a field both get marked as a building. Any building in the footprints data that has no other buildings nearby, nor a highway or path of some kind, is flagged with a debugging tag of false=yes. Usually this is easy to determine looking at satellite imagery, since these are often remote buildings. The tags can be searched for when editing the data to visually determine whether it's a real building or not.</p>"},{"location":"conflation/#conflating-other-than-buildings","title":"Conflating Other Than Buildings","text":""},{"location":"conflation/#opendatakit","title":"OpenDataKit","text":"<p>Data collected in the field using ODK Collect is a specific case. If using using data extracts from OpenStreetMap, the data extract has the OSM ID, so it's much simpler to conflate the new tags with either the existing building polygon or POI. For this workflow, any tag in the feature from ODK will overwrite any existing values in the existing feature. This allows for updating the tags &amp; values when ground-truthing. When the OSM XML file is loaded into JOSM, it has the modified attribute set, and the version has been incremented. In JOSM under the File menu, select the Update Modified menu item. This will sync the modified feature with current OSM. At that point all that needs to be done is validate the modified features, and upload to OSM.</p> <p>When ODK Collect is used but has no data extract, conflation is more complicated. For this use case, a more brute force algorythm is used. Initially any building polygon or POI within 7 meters is found by querying the database. Most smartphone GPS chipsets, even on high-end phones, are between 4-9m off from your actual location. That value was derived by looking at lots of data, and can be changed when invoking the conflation software in this project. Once nearby buildings are identified, then the tags are compared to see if there is a match.</p> <p>For example, if collecting data on a restaurant, it may have a new name, but if the nearby building is the only one with an amenity=restaurant** (or cafe, pub, etc...) it's considered a probable match. If there are multiple restaurants this doesn't work very well unless the name hasn't changed. If there are multiple possible features, a *fixme= tag is added to the POI, and it has to be later validated manually. Every tag in the ODK data has to be compares with the nearby buildings. Often it's the name tag that is used for many amenities.</p> <p>If a satellite imagery basemap is used in Collect, conflation is somewhat simpler. If the mapper has selected the center of the building using the basemap, conflation starts by checking for the building polygon in OSM that contains this location. If no building is found, the POI is added to the output file with a fixme=new building tag so the buildings can traced by the validator. Any tags from the POI are added to the new building polygon.</p>"},{"location":"conflation/#points-of-interest-poi","title":"Points Of Interest (POI)","text":"<p>It is common when collecting datasets from non-OSM sources each feature may only be single node. This may be a list of schools, businesses, etc... with additional information with each POI that can be added to the OSM building polygon (if it exists). Obviously any imported data must have a license acceptable for importing into OSM.</p> <p>Similar to how conflating ODK data when not using a data extract, the tags &amp; values are compared with any nearby building. Since often these imports are features already in OSM with limited metadata, this adds more details.</p>"},{"location":"conflation/#highways","title":"Highways","text":"<p>Highways are more complex because it uses relations. A relation is a groups of highway segments into a single entity. Some times the tags are on the relation, other times each highway segment. The segments change when the highway condition changes, but the name and reference number doesn't change. External datasets don't use relations, they are OSM specific.</p>"},{"location":"conflation/#mvum-highways","title":"MVUM Highways","text":"<p>The USDA publishes a dataset of Motor Vehicle Use Maps (MVUM) highways in the National Forest. Some of this data has already been imported into OSM, although the metadata may be lacking, but the LineString is there. MVUM roads are primarily compacted dirt roads. While some can be driven in a passenger vehicle, most are varying degrees of bad to horrible to impassable. These highways are often used for recreational traffic by off-road vehicles, or for emergency access for a wildland fire or backcountry rescue.</p> <p>Another key detail of MVUM highways is each one may have 4 names! There is of course the primary name, for example \"Cedar Lake Road\". But it may also have a locals name, common in remote areas. And then there is the reference number. A MVUM highway may have two reference numbers, the country designated one, and the USDA one. Luckily OSM supports this. Many of these tags effect both how the highway is displayed, as well as routing for navigation. </p> <pre><code>\"name\": \"Platte Lake Road\",\n\"alt_name\": \"Bar-K Ranch Road\",\n\"surface\": \"dirt\",\n\"smoothness\": \"bad\",\n\"highway\": \"track\",\n\"ref\": \"CO 112\",\n\"ref:usfs\": \"FR 521.1A\"\n\"tracktype\": \"grade3\"\n</code></pre> <p>A bad highway is something I'd be comfortable driving in a 4x4 high-clearance vehicle. Smoothness values can be a bit misleading, as often what is in OSM may be years out of date. And most MVUM roads get zero maintainance, so get eroded, pot-holed, and or exposed rocks. And people's perception of road conditions is subjective based on one's experience driving these highways.</p> <p>All of this metadata makes conflation interesting. Since existing OSM features were added by more than one person, the tagging may not be consistent. For example, the existing data may have Forest Service Road 123, which should really be ref:usfs=FR 123. And the real highway name Piney Pass Road is in the MVUM dataset. The goal of highway conflation is to merge the new metadata into the existing OSM feature where possible. This then needs to be validated by a human being. There is still much tedious work to process post conflation data before it can be uploaded to OSM.</p> <p>But sometimes conflation works well, especially when the LineString in OSM was imported from older versions of the MVUM data. But often highways in OSM were traced off satellite imagery, and may have wildly different geometry.</p> <p>If you ignore conflating the tags other than name or ref, the process is somewhat less messy. And tags like surface and smoothness really should be ground-truthed anyway. So I do ignore those for now and stick to validating the name and the two reference numbers which are usually lacking in OSM. That and addding consistency to the data to make it easier to make data extracts.</p> <p>To conflate OSM highways with external data, initially each entry in the external dataset does a distance comparison with the existing OSM data. There is an optional threshold to set the distance limit. Since currently this is focused on conflating files without a database, this is computationally intensive, so slow. For data that was imported in the past from MVUM datasets, a distance of zero means it's probably the same segment. The external dataset needs to have the tagging converted to the syntax OSM uses. Tagging can be adjusted using a conversion program, but as conversion is usually a one-off task, it can also be done using JOSM or QGIS. Usually it's deleting most of the tags in the external dataset that aren't appropriate for OSM. Primarily the only tags that are needed are the name and any reference numbers. Since the MVUM data also classified the types of road surface, this can also be converted. Although as mentioned, may be drastically out of data, and OSM is more recent and ground-truthed.</p> <p>Then there is a comparison of the road names. It's assumed the one from the MVUM dataset is the correct one. And since typos and weird abbreviations may exist in the datasets, fuzzy string matching is performed. This way names like FS 123.1 can match FR 123.1A. In this case the current name value in OSM becomes alt_name, and the MVUM name becomes the official name. This way when validating you can make decisions where there is confusion on what is correct. For an exact name match no other tags are checked to save a little time.</p> <p>Any other processing is going to be MVUM highway specific, so there will be an additional step to work through the reference numbers not supported by this program.</p>"},{"location":"conflation/#output-files","title":"Output Files","text":"<p>If the data files are huge, it's necessary to conflate with a subset of all the data. For projects using the Tasking Manager or the Field Mapping Tasking Manager you can download the project boundary file and use that. For other projects you can extract administrative bondaries from OpenStreetMap, or use external sources. Usually county administrative boundaries are a good size. These can be extracted from OSM itself, or an external data file of boundaries.</p> <p>After conflation, an output file is created with the new buildings that are not duplicates of existing OSM data. This is much smaller than the original data, but still too large for anyone having bandwidth issues. This output file is in GeoJson format, so can be edited with JOSM or QGIS</p> <p>Since this software is under development, rather than automatically deleting features, it adds tags to the features. Then when editing the data, it's possible to see the flagged data and validate the conflation. It also makes it possible to delete manually the results of the conflation from the output file once satisfied about the validation of the results.</p>"},{"location":"conflation/#validating-the-conflation","title":"Validating The Conflation","text":"<p>The conflated data file can't be uploaded to OSM until it is validated. While QGIS can be used for this purpose, JOSM is preferred because it does validation checks, and uploads directly to OpenStreetMap. I start by loading the conflation data file, and then enabling the OpenStreetMap imagery for the basemap. Existing buildings in OSM are grey polygons, so it's possible to see existing buildings with the conflated new buildings as a layer on top.</p> <p>Once the buildings are loaded, you can then download the OSM data for that view. Then use the SelectDuplicateBuilding script to find any buildings that have been added since the initial data file for conflation was used. Once selected, those can be deleted in a single operation.</p> <p>The next step is validating what is left that is considered to be a new building. This is done using satellite imagery. Most commercial satellite imagery available for public use comes from Maxar. But the different providers (Bing, ESRI, Google, etc...) have different update cycles, so I often double check with ESRI imagery.</p> <p>If there is drone imagery available from Open Aerial Map, that's also a good surce of imagery, but often doesn't cover a large area.</p>"},{"location":"dataflow/","title":"Mapper Data flow","text":"<p>Much of the process of conflation is preparing the datasets since we're dealing with huge files with inconsistent metadata. The primary goal is to process the data so validating the post conflation is as efficient as possible. Conflating large datasets can be very time consuming, so working with smaller files generates results quicker for the area you are focused on mapping.</p> <p>It is also possible to use the pre or post conflation data files in JOSM without the Tasking Manager if the density of the data per task isn't too large. 100 Highways or less seems to be the most efficient density per task. Included in this project is a utility, TM Splitter that can be used to generated a grid of tasks that are smaller if you have the project boundary (AOI).</p>"},{"location":"dataflow/#tasking-manager","title":"Tasking Manager","text":"<p>The other goal is to prepare the data for Tasking Manager (TM). TM has a project size limit of 5000km sq, which is also a good size when not using the TM. Each national forest or park needs to be split into a grid of TM sized tasks. Each of these is used when creating the TM project.</p> <p>When you select a task in the TM project, it'll download an OSM extract and satellite imagery for that task. We don't really need those, as we're dealing with disk files, not remote mapping. While it's entirely possible to use the TM project sized data extracts, I also create a custom task boundaries files for TM, and make small task sized extracts that are relatively quick to conflate and validate.</p>"},{"location":"dataflow/#getting-the-data","title":"Getting The Data","text":"<p>To make any progress. obviously data is needed. Often getting the data and processing it can be the hard part for a mapper wanting to improve the metadata in OSM for remote highways and trails in one area. While the nationwide data sets are available. it can be tedious and time consuming to manage all the data.</p>"},{"location":"dataflow/#data-extracts","title":"Data Extracts","text":"<p>This project hosts data extracts broken down by national forest, park, or wilderness areas on the OSM Merge project website. These are the output files of the conversion and data cleaning process for MVUM, RoadCore. or Trails data. The conversion utilities are documented here. This process does several things. For external datasets, it drops unnecessary data fields from the original data schemas, and converts the data fields suitable for OpenStreetMap into an OSM schema for conflation. For OSM data extracts, it also correct multiple data quality issues, like expanding abbreviations, more USFS reference numbers from the name field into the appropriate ref:usfs* for OSM.</p> <p>There are other projects for converting the original datasets to OSM syntax, that do a similar function. Those are usfs-to-osm and nps-to-osm, and generate files that can also be used for conflation.</p>"},{"location":"dataflow/#original-sources","title":"Original Sources","text":"<p>All the datasets are of course publicly available. The primary source of the Motor Vehicle Use Map (MVUM) and RoadCore data is available from the FSGeodata Clearinghouse, which is maintained by the USDA. The Topographical map vector tiles are available from here., which is maintained by the National Forest Service. OpenStreetMap data for a country can be downloaded from Geofabrik. National Park trail data is available from the NFS Publish site.</p>"},{"location":"dataflow/#initial-setup","title":"Initial Setup","text":""},{"location":"dataflow/#small-scale","title":"Small Scale","text":"<p>Many mappers are focused on a few (potentially large) areas that interest them. They are often out ground-truthing the map data (or lack thereof) in OSM. For the mappers that may want to add metadata for their favorite areas, this project makes the smaller datasets available so they can focus on mapping, instead of data manipulation.</p> <p>These data extracts are available from the project website, and as legacy datasets, the MVUM, RoadCore, and Trails dataset extracts only get updated when there are improvements made to the conversion utilities.</p> <ul> <li>National Forests</li> <li>National Parks</li> <li>National Wilderness</li> </ul> <p>There are also extracts from OpenStreetMap for each area, as well. Note that these can be out of date since OSM is constantly changing. These extracts have been converted as well, so the guidelines on automated edits apply. Each feature must be validated manually, the goal is to make this validation process as efficient as possible.</p> <p>The conversion does two primary tasks. One is it drops all the ancient tiger: tags that the community has decided are meaningless to OSM. Also non OSM tags from old partial imports like SOURCEID are also deleted. It also looks for a possible forest service reference number in the name or ref field, and moves it to the proper ref:usfs, with a \"FR\" prefix. This saves much cut &amp; paste later when validating the results.</p> <p>For more detail on validating the converted and/or conflated files, there is a detailed document on validating with JOSM. Since reviewing and updating all the remote trails and highways for the entire US is a huge task, this project is very focused on supporting that dataflow for other mappers so they can concentrate on validating and mapping their favorite areas.</p>"},{"location":"dataflow/#large-scale","title":"Large Scale","text":"<p>Maintaining data extracts for multiple areas requires more manual work than just mapping. For one thing, it'll consume a lot of disk space and cpu cycles. Setting up a large scale data extract is covered in detail in that document.</p>"},{"location":"dataflow/#processing-the-data","title":"Processing The Data","text":"<p>To support conflation, all the datasets need to be filtered to fix known issues, and to standardize the data. The OpenStreetMap tagging schema is used for all data. You can skip this section if using data extracts from the OSM Merge as the conversion is already done. Or if you are interested in the details of the conversion process between data formats.</p> <p>Each of the external datasets has it's own conversion process, which is documented in detail here:</p> <ul> <li>MVUM</li> <li>Trails</li> <li>OSM</li> </ul> <p>While it's possible to manually convert the tags using an editor, it can be time consuming. There are also many, many weird and inconsistent abbreviations in all the datasets. I extracted all the weird abbreviations by scanning the data for the western United States, and embedding them in the conversion utilities. There are also many fields in the external datasets that aren't for OSM, so they get dropped. The result are files with only the tags and features we want to conflate. These are the files I put in my top level SourceData directory.</p>"},{"location":"dataflow/#conflation","title":"Conflation","text":"<p>Once all the files and infrastructure is ready, then conflating the external datasets with OpenStreetMap can start. Here is a detailed description of conflating highways. Conflating with  OpenDataKit is also documented. The final result of conflation is an OSM XML file for JOSM, and a GeoJson for other editors. The size of this file is determined by task boundaries you've created.</p> <p>If you want to use TM, then create the project with the 5000km sq task boundary, and fill in all the information required. Then select your task from the TM project and get started with validation.</p> <p>Conflation uses primary and secondary datasets. Any data in the primary is considered to be the official name or reference number. Where there is an existing name in OSM, it's changed to an alt_name so it can be manually validated. No data is lost, just renamed, leaving it up to the mapper to decide.</p>"},{"location":"dataflow/#validation","title":"Validation","text":"<p>The real fun starts after all this prep work. The goal is to make this part of the process, validating the data and improving OSM as efficient as possible. If it's not efficient, manual conflation is incredibly time-consuming, tedious, and boring. Which is probably why nobody has managed to fix more than a small area.</p> <p>The conflation results have all the tags from the external datasets that aren't in the OSM feature or have different values. Any existing junk tags have already been deleted. The existing OSM tags are renamed where they don't match the external dataset, so part of validation is choosing the existing value or the external one, and delete the one you don't want. Often this is a minor difference in spelling.</p> <p>If the conflation has been good, you don't have to edit any features, only delete the tags you don't want. This makes validating a feature quick, often in under a minute per feature. Since many remote MVUM roads are only tagged in OSM with highway=track, validating those is very easy as it's just additional tags for surface, smoothness, and various access tags.</p> <p>In the layer in JOSM with the conflated data, I can select all the modified features, and load them into the TODO plugin. Then I just go through them all one at a time to validate the conflation. I also have the original datasets loaded as layers, and also use the USGS Topographical basemaps in JOSM for those features I do need to manually edit. Even good conflation is not 100%.</p>"},{"location":"dbextract/","title":"DB Extract","text":"<p>This is a simple utility to make data extracts from a postgres database containing OpenStreetMap data. Currently it only supports highways, and is focused on creating a data extract that can be used for conflation. While this works fine on a static database, more importantly it can use a database of OSM data updated every minute.</p> <p>This database can be kept up to date using the Underpass programs. This program uses the actual changeset files to update the data, same as Overpass would do. It uses a priority.geojson file to limit the geographical area too keep updated as most people don't need the entire planet.</p>"},{"location":"dbextract/#output-files","title":"Output Files","text":"<p>Dbextract can output a GeoJson or a OSM XML output file. Currently the OSM XML output file only contains ways, so no data is visible. To get the nodes, in JOSM execute File-&gt;update data and they'll be visible. This restriction will be gone after the bug in Underpass is fixed.</p>"},{"location":"dbextract/#examples","title":"Examples","text":"<p>To generate an OSM XML data file for a small AOI, do this:</p> <pre><code>dbextract.py -v -b Dixie_Task_1.geojson -u localhost/uri -o out.osm\n</code></pre> <p>To generate an GeoJson data file from the entire database, do this:</p> <pre><code>dbextract.py -v -u localhost/uri -o out.geojson\n\nOptions:\n-h, --help              Show this help message and exit\n-v, --verbose [VERBOSE] Verbose output\n-b, --boundary BOUNDARY Optional boundary to clip the data\n-o, --outfile OUTFILE   The output file (*.osm or *.geojson)\n-u, --uri URI           Database URI\n</code></pre>"},{"location":"formats/","title":"File Formats","text":"<p>This project support two file formats, GeoJson and OSM XML. </p>"},{"location":"formats/#geojson","title":"GeoJson","text":"<p>GeoJson is widely supported by many tools, and this project uses it as the internal data structure for consistency. At the top level the file starts with a GeometryCollection, which is just a container for the list of features.</p>"},{"location":"formats/#geometry","title":"Geometry","text":"<p>Each GeoJson feature contains a geometry object that has two fields, the coordinates, and the type. Shapely or GDAL can be used to convert between string representations and geometry objects.</p>"},{"location":"formats/#properties","title":"Properties","text":"<p>The properties is the array of keyword=value pairs, similar to the tags in OSM. There is no definition of a schema, and pair works. For conflation though, standardizing on the OSM schema for tagging pairs is critical to keep things simple.</p> <pre><code>\"properties\": {\n    \"ref:usfs\": \"FR 965.2\",\n    \"name\": \"  Road\",\n    \"4wd_only\": \"yes\",\n    \"seasonal\": \"yes\"\n},\n</code></pre>"},{"location":"formats/#osm-xml","title":"OSM XML","text":"<p>An OSM XML file is read and converted to GeoJson, and then later it can get converted to OSM XML for the output file. In addition to the tags and geometry, each feature also has attributes.</p>"},{"location":"formats/#attributes","title":"Attributes","text":"<p>The OSM XML format has attributes, which are used to control editing a feature. Since this project wants to generate an OSM XML file for JOSM that allows for tag merging, these attributes are important. In the post conflation data file, the version of the existing OSM feature has been incremented, and the action is set to modify. This enable JOSM to see this as an edited feature so it can be uploaded.</p> <ul> <li>id - the OSM ID of the feature</li> <li>version - the current version of the feature</li> <li>action - the action to apply when uploading to OSM<ul> <li>create</li> <li>modify</li> <li>delete</li> </ul> </li> <li>timestamp - the timestamp of the feature's last change</li> </ul> <p>With action=modify set, in JOSM you can update modified and sync with current OSM.</p>"},{"location":"formats/#data-types","title":"Data Types","text":"<p>There are two data types in the OSM XML files used for conflation. These are nodes and ways.</p>"},{"location":"formats/#nodes","title":"Nodes","text":"<p>A node is a single coordinate. This is often used as a POI, and will have tags. A node that is referenced in a way won't have any tags, just the coordinates. The version and timestamp get updated if there is a change to the node location.</p> <pre><code>&lt;node id=\"83276871\" version=\"3\"\n    timestamp=\"2021-06-12T16:25:43Z\" lat=\"37.6064731\" lon=\"-114.00674\"/&gt;\n</code></pre>"},{"location":"formats/#ways","title":"Ways","text":"<p>A way can be a linestring, polygon, any geometry that includes more than one node. This makes it difficult to do spatial comparisons, so when an OSM XML file is loaded, in addition to the refs, they are also converted to an actual geometry. All the calculations use the geometry, and the refs are used to construct the OSM XML output file for JOSM. OSM has no concept of a LineString or Polygon, the shape is determined by the tags, for example highway=track, or building=yes.</p> <pre><code>&lt;way id=\"10109556\" version=\"4\" timestamp=\"2021-06-12T15:42:25Z\"&gt;\n&lt;nd ref=\"83305252\"/&gt;\n&lt;nd ref=\"8118009676\"/&gt;\n&lt;nd ref=\"8118009677\"/&gt;\n&lt;nd ref=\"83277113\"/&gt;\n&lt;nd ref=\"83277114\"/&gt;\n&lt;nd ref=\"83277116\"/&gt;\n&lt;nd ref=\"83277117\"/&gt;\n&lt;tag k=\"highway\" v=\"unclassified\"/&gt;\n&lt;tag k=\"surface\" v=\"dirt\"/&gt;\n</code></pre> <p></p>"},{"location":"formats/#converting-between-formats","title":"Converting Between Formats","text":"<p>To support reading and writing OSM XML files, this project has it's own code that builds on top of the OsmFile() class in the OSM Fieldwork. This parses the OSM XML file into GeoJson format for internal use. All of the attributes in the OSM XML file being read are convert to tags in the GeoJson properties section, and then later converted from the properties back to OSM XML attributes when writing the output file.</p>"},{"location":"highways/","title":"Conflating Highway and Trail Data","text":"<p>This is focused only on highway and trail data in the US, but should be useful for other countries. In particular, this is focused on the primary goal of improving OpenStreetMap data in remote areas as these are used for emergency response. Most of these roads and trails are in OSM already, some from past imports, some traced off of satellite imagery. </p> <p>I did a talk at SOTM-US in Tucson about this project called OSM For Fire Fighting. This conflation software was developed to improve the quality of the remote highway data in OpenStreetMap. This is not an import of new data, only updating existing features with a focus on improved navigation. Importing new features from these datasets uses a different process, so it's better to not mix the two.</p> <p></p> <p>While there are details in the the datasets that would be useful, the initial set is the name, the reference number, and the vehicle class appropriate for this highway. Not this can change over time, so if the smoothness tag is in the OSM feature, it's assumed that value is more accurate.</p> <p>The primary purpose is to clean up the TIGER import mess, which is often inaccurate. This leads to navigation problems as sometimes what is in OSM is not what the street sign says. Since there are multiple datasets supplied by government agencies with a good license for OSM, we data mine these through conflation to get the best name and reference number.</p> <p>Although most of the fields in these datasets aren't useful for OSM, some are like is it a seasonal road, various off road vehicle access permissions, etc... since this is also useful for navigation. Any tags added or edited will follow the OSM Tagging Guidelines for forest roads.</p>"},{"location":"highways/#the-datasets","title":"The Datasets","text":"<p>The primary source of these datasets is available from the  FSGeodata Clearinghouse, which is maintained by the USDA.</p> <p>The Topographical map vector tiles are available from here., which is maintained by the National Forest Service.</p> <p>These have been partially imported in some areas in the past, complete with the bugs in the original datasets. One big advantage though is that the geometry in OSM was from the same USDA datasets at some point in the past, so it's relatively easy to match the geometries. Conflation then is mostly working through the name and reference fields between multiple files, which sometimes don't agree on the proper name.</p> <p>And OpenStreetMap of course.</p>"},{"location":"highways/#processing-the-datasets","title":"Processing The Datasets","text":"<p>Since the files are very large with different schema, a critical part of the conflation process is preparing the data. Some of these files are so large neither QGIS or JOSM can load them without crashing. I use two primary tools for splitting up the files. ogr2ogr for the GeoJson files, and osmium for the OSM XML files. The OSM XML format is required if you want the conflation process to merge the tags into an existing feature. If conflating with OSM data using the GeoJson format, you need to manually cut &amp; paste the new tags onto the existing feature.</p> <p>As you further reduce large datasets to smaller more manageable pieces, this can generate many files. The top level choice is the largest category. I use National Forests boundaries as they can cross state lines.</p> <p>All of the datasets have issues with some features lacking a geometry. These appear to be duplicates of a Feature that does have a good geometry. They are also in \"NAD 83 - EPSG:4269\" for the CRS, so need to convert and fix the geometries. I use ogr2ogr to convert the GDB files to GeoJson like this:</p> <pre><code>ogr2ogr Road_MVUM.geojson S_USA_Road_MVUM.gdb.zip -makevalid -s_srs EPSG:4269 -t_srs EPSG:4326 -sql 'SELECT * FROM Road_MVUM WHERE SHAPE IS NOT NULL'\n\nogr2ogr Trails_MVUM.geojson S_USA_Trail_MVUM.gdb.zip -makevalid -s_srs EPSG:4269 -t_srs EPSG:4326 -sql 'SELECT * FROM Trail_MVUM WHERE SHAPE IS NOT NULL'\n</code></pre> <p>This generates a clean GeoJson file. It has many fields we don't want, so I run a simple conversion program that parses the fields are defined in the original file, and converts the few fields we want for conflation into the OSM equivalent tag/value. For conflation to work really well, all the datasets must use the same schema for the tags and values.</p> <p>Since the MVUM and RoadCore datasets covers the entire country, I build a directory tree in which the deeper you go, the smaller the datasets are. I have the National Forest Service Administrative boundaries unpacked into a top level directory. From there I chop the national dataset into just the data for a forest. This is still a large file, but manageable to edit. Sometimes with rural highway mapping, a large area works better. If there are plans to use the Tasking Manager, The files are still too large, as TM has a 5000sq km limit.</p> <p>Next is generating the task boundaries for each national forest that'll be under the 5000km limit. I used the tm-splitter.py program in this project to use the national forest boundary and break it into squares, and clipped properly at the boundary. These task boundary polygons can then be used to create the project in the Tasking Manager, which will further split that into the size you want for mapping.</p> <p>Something to be conscious of is these external datasets are also full of obscure bugs. Some of the data I think hasn't been updated since the government discovered digital mapping a few decades ago. The conversion utilities will handle all of these problems in these datasets.</p>"},{"location":"highways/#the-openstreetmap-extract","title":"The OpenStreetMap Extract","text":"<p>This step is unnecessary if you plan to manually conflate with a GeoJson file, so jump ahead to the next section.</p> <p>To conflate against OSM data with the goal of automatically merging the tags into the feature you have to prepare the dataset. Each feature needs to be validated anyway, merging tags is more efficient than cut &amp; paste. Since this project is processing data from multiple US states, it exceeds the Overpass data size.</p> <p>I download the states I want to conflate from Geofabrik, and then use osmium merge to turn it into one big file. I have to do this because most of the national forest cross state lines. You'll get duplicate ID errors if you download these files on different days, so grab all the ones you plan to merge at the same time. Geofabrik updates every 24 hours.</p> <p>When dealing with files too large for JOSM or QGIS, osmium is the tool to use. There is also osmfilter and osmconvert which can be used as well. Ogr2ogr can't be used as it can't write the OSM XML format. To merge multiple files with osmium, do this:</p> <pre><code>osmium merge --overwrite -o outdata.osm *.osm.pbf\n</code></pre> <p>The next step is to delete everything but highways from the OSM XML file. When conflating highways, we don't care about amenities or waterways.</p> <p>The preferred data extraction program for conflation is the osmhighways.py program, which has much more fine-grained control, and also replaces the older fixname.py program and fixes the issues when the name field is actually a reference. It also deletes the extraneous tiger:* tags to reduce bloat.</p> <p>You can do something similar with osmium tool, but you wind up with extra features and tags which impacts conflation performance.</p> <pre><code>osmium tags-filter --overwrite --remove-tags -o outdata.osm indata.osm w/highway=track,service,unclassified,primary,tertiary,secondary,path,residential,abandoned,footway,motorway,trunk\n</code></pre> <p>Finally I clip this large file into separate datasets, one for each national forest.</p> <pre><code>osmium extract --overwrite --polygon boundary.geojson -o outdata-roads.osm\n</code></pre> <p>Then the real fun starts after the drudgery of getting ready to do conflation.</p> <p></p>"},{"location":"highways/#forest-road-names","title":"Forest Road Names","text":"<p>The names and reference number in OSM now have a wide variety of incorrect tagging when it comes to names. \"Forest Service Road 123.4A\" is not a name, it is a reference number. Same for \"County Road 43\".  The fixname.py utility scan the OSM extract and when it see incorrect tagging, correct it to the OSM standard. Since the external datasets already follow the same guidelines, this increases the chance of a good match when conflating, since comparing names is part of the process.</p>"},{"location":"highways/#forest-road-reference-numbers","title":"Forest Road Reference Numbers","text":"<p>I'm a huge believer that the name and reference number in OSM should match the street sign, since that's often what is used for navigation. Unfortunately the MVUM data has many highways with a .1 suffix, which some street signs don't display. Also, depending on the age of the paper maps or digital files, older maps lack the .1 suffix, but newer datasets so have the .1 suffix. Since a .1 suffix may be a spur road of questionable quality, it's an important detail, so included when updating the reference numbers.</p> <p>A minor note, the USGS Topographical basemap for JOSM also sometimes lacks the .1 suffix, so can't be used to validate it.</p>"},{"location":"highways/#tiger-tag-deletion","title":"TIGER Tag Deletion","text":"<p>Since there is community consensus that the tiger: tags added back in 2008 when the TIGER data was imported are meaningless, so should be deleted as bloat. The fixnames.py utility used for correct the name also deletes these from each feature so you don't have to manually do it.</p>"},{"location":"highways/#mvum-roads","title":"MVUM Roads","text":"<p>This is all the highways in National Forests. The data contains several fields that would be useful in OSM. This dataset has a grading of 1-5 for the type of vehicle that can drive the road, as well as a field for high clearance vehicles only. This is roughly equivalent to the smoothness tag in OSM. The surface type is also included, which is the same as the OSM surface tag. There are other fields for seasonal access, and seasonal road closures. Roads tagged as needing a high clearance vehicle generate a 4wd_only tag for OSM.</p> <p>The reference numbers often have a typo, an additional number (often 5 or 7) prefixed to the actual number in the original dataset, and were imported this way. Since the reference number needs to match what the map or street sign says, these all need to be fixed. And there are thousands of these...  </p> <p>The type of vehicle that can be driven on a particular road is a bit subjective based on ones off-road driving experience. These are typically jeep trails of varying quality, but very useful for back-country rescues or wildland fires.</p>"},{"location":"highways/#mvum-trails","title":"MVUM Trails","text":"<p>These are Multi Vehicle Use Maps (MVUM), which define the class of vehicle appropriate to drive a road. The trails dataset contains additional highways, as some hiking trails are also forest service roads. These are primarily for hiking, but allow vehicle use, primarily specialized off-road vehicles like an ATV or UTV. They suffer from the same bad data as the MVUM roads.</p>"},{"location":"highways/#national-forest-trails","title":"National Forest Trails","text":"<p>This dataset is hiking trails that don't allow any vehicle usage at all. Many of these trails are in OSM, but lack the trail name and reference number. These also get used for emergency response as well. If there is a name and reference number for the trail, this makes it easier to refer a location to somebody over a radio instead of GPS coordinates.</p>"},{"location":"highways/#usgs-topographical-maps","title":"USGS Topographical maps","text":"<p>It's possible to download the vector datasets used to produce topographical maps. Each file covers a single 7.5 map quad, which is 49 miles or 78.85 km square. There are two variants for each quad, a GDB formatted file, and a Shapefile formatted file. The GDB file contains all the data as layers, whereas the Shapefiles have separate files for each feature type. I find the smaller feature based files easier to deal with. The two primary features we want to extract are Trans_RoadSegment and Trans_TrailSegment. Because of the volume of data, I only have a few states downloaded.</p> <p>I then used ogrmerge to produce a single file for each feature from all the smaller files. This file covers an entire state. This file has also has many fields we don't need, so only want the same set used for all the datasets. The usgs.py contained in this project is then run to filter the input data file into GeoJson with OSM tagging schema. The topographical data is especially useful for conflation, since the name and reference number match the paper or GeoPDF maps many people use.</p> <p>I found a few problems processing the ShapeFiles due to font encoding issues, and also with converting directly to GeoJson. I do this as a two step process, first make a unified ShapeFile from all the other ShapeFiles, and then convert it to GeoJson, which seems to work best.</p> <pre><code>ogrmerge.py -nln highways -single -o highways.shp VECTOR_*/Shape/Trans_Road*.shp -lco ENCODING=\"\"\nogr2ogr highways.geojson highways.shp\n</code></pre>"},{"location":"highways/#conflation","title":"Conflation","text":"<p>Once all the datasets are broken into manageable pieces, and everything is using the OSM tagging schema conflation can start. There are two datasets specified, one is the primary, and the other is the secondary. The tag values in the primary will override the values in the secondary file. To be paranoid about the details, when a tag value is overwritten by the primary data source, the current value becomes old_, ie... name becomes old_name, and then name is updated to the current value. Sometimes when editing the difference in the names is due to abbreviations being used, spelling mistakes, etc... so the old_name can be deleted. This way the final validation is done by the mapper.</p> <p>When conflating multiple datasets, those need to be conflated against each other before conflating with OSM. Since the topographical dataset is what matches a paper map, or GeoPDF, I consider that the primary dataset. The MVUM and trail data are particularly full of mistakes. Sometimes one dataset has a name, and the other doesn't, so conflation here produces that value.</p> <p>There are also many, many highways in these areas that in OSM only have highway=something. These are easy to conflate as you are only adding new tags. While in TIGER there are many highway=residential, that should really be highway=unclassified or highway=track, it is entirely possible it is a residential road. There's a lot of nice cabins way out in most national forests. But this is the type of thing you'd really need to ground-truth, and luckily doesn't effect navigation when you are out in a network of unmaintained dirt roads.</p> <p></p> <p>The conflation algorithm is relatively simple at the high level, just find all other highways within a short distance, and then check the slope to eliminate a side road that may be touching. At the lower level, there is a lot of support for dealing with the bugs in the external datasets.</p> <p>The conflation algorithm is relatively simple at the high level, just find all other highways within a short distance, and then check the slope to eliminate a side road that may be touching. At the lower level, there is a lot of support for dealing with the bugs in the external datasets.</p>"},{"location":"highways/#highway-segments","title":"Highway Segments","text":"<p>In all highway datasets a longer highway may be broken into segments. These changing segments may be due to the surface changing, speed limit changing. etc... and may be organized into a group of some kind.</p> <p>The fun starts when the segments used for the OSM highway aren't the same as the segments in the external datasets MultiLineString. Often in the external dataset there are no segments at all, just a long LineString. In OSM the same highway may be broken into multiple segments since often the data is more detailed.</p> <p>The algorithm for the geometry calculations tries to compare each segment from the primary dataset (if there are any), with any highway segment in the secondary dataset (probably OSM). Later when processing all the possible segments of any highway close to the primary segment, the name and reference numbers are checked, and if they match, the new tags from the external dataset are applied to all the matching segments in the secondary.</p>"},{"location":"highways/#editing-in-josm","title":"Editing in JOSM","text":"<p>Unfortunately manually validating the data is very time consuming, but it's important to get it right. I use the TODO plugin and also a data filter so I just select highways. With the TODO plugin, I add the selected features, ideally the entire task. Then I just go through all the features one at a time. When the OSM XML dataset is loaded, nothing will appear in JOSM. This is because the OSM XML file produced by conflation has the refs for the way, but lack the nodes. All it takes is selecting the update modified menu item under the File menu and all the nodes get downloaded, and the highways appear.</p> <p>I often have the original datasets loaded as layers, since sometimes it's useful to refer back to when you find issues with the conflation. Much of the existing data in OSM has many unused tags added during the TIGER import. These also get deleted as meaningless bloat. Some were imported with all the tags from the original dataset which also get deleted. This is life as a data janitor...</p> <p>Once you've validated all the features in the task, it can be run through the JOSM validator, and if all is good, uploaded to OSM. Often the JOSM validator finds many existing issues. I fix anything that is an error, and mostly ignore all the warning as that's a whole other project.</p> <p>If you are editing with the OSM XML file produced by conflation, when the file is opened, there will be some conflicts. This is usually due to things like the incorrect forest road name getting deleted, since now it's a proper ref:usfs reference number. And the tiger tags are gone as well if the fixnames.py utility is used.</p> <p>To fix the conflicts, I just select them all, and click on resolve to my version. Since all the new tags and old tags are preserved, you can edit them directly in the tags window in JOSM. Then I load all the ways into the TODO plugin. You can also use the conflict dialog box to edit the merged tags, but I find the other way more efficient.</p> <p>Using the plugin to validate a feature all I have to do is click on the entry. Sometimes there will be issues that need to be manually fixed. If conflation has changed the name, the old one is still in the</p> <p>feature so a manual comparison can be done. Often validating a feature is just deleting a few tags. But this is the important detail for machine editing. Somebody (not AI) must manually validate each changed feature. This is why the efficiency of mapping is important if you want to update a large area, like an entire national forest.</p> <p>Sometimes there are weird typos that have slipped through the process. This is where the time goes since you have to manually edit the values. But many times for these remote highways you can just mark it as done, and go on to the next one. Many of these highways in OSM have no tags beyond highway=track, so mo conflicts.This lets you validate a large number of features relatively quickly without sacrificing quality.</p>"},{"location":"highways/#editing-osm-xml","title":"Editing OSM XML","text":"<p>The conflation process produces an output file in OSM XML format. This file has incremented the version number and added action=modify to the attributes for the feature. When loaded into OSM, no data is initially visible. If you go to the File menu, go down and execute update modified. This will download all the nodes for the ways, and all the highways will become visible. Highways that have multiple tags already in OSM will become a conflict. These can be resolved easier in JOSM using the conflict dialog box. No geometries have changed, just tags, so you have to manually select the tags to be merged. Features without tags beyond highway=something merge automatically. which makes validating these features quick and easy. Note that every feature needs to be validated individually.</p>"},{"location":"highways/#editing-geojson","title":"Editing GeoJson","text":"<p>While JOSM can load and edit GeoJson data, not being in a native OSM format it can't be automatically merge. Instead load the GeoJson file and then create a new OSM layer. I select all the highways in the task, and load them into the TODO plugin. Sometimes there are so few highways, I don't use the TODO plugin. I then cut the tags and values for a feature from the GeoJson file, then switch to the OSM layer, and paste the tags into the feature. This is often the way this data has been updated in OSM in the past. Fine for smaller datasets, tedious for large datasets.</p>"},{"location":"highways/#validating","title":"Validating","text":"<p>Here's an example of the results of a 3 way conflation. This was between the MVUM data, the topographical data, and OSM data.</p> <ul> <li>highway=unclassified</li> <li>lanes=2</li> <li>name=Whisky Park Road</li> <li>operator=US Forest Service</li> <li>ref:usfs=FR 503</li> <li>smoothness=good</li> <li>surface=gravel</li> </ul> <p>Note that the name is spelled wrong. Here is a document on how to validating in JOSM for much more detail.</p>"},{"location":"highways/#splitting-highways","title":"Splitting Highways","text":"<p>In national forest lands, the reference number changes at every major intersection. Side roads that branch off have an additional modifier added. or example, the main road may be called ref:usfs=\"FR 505\", with a change to ref:usfs=\"FR 505.1\" when it crosses a state line. Spur roads (often to campsites) get a letter attached, so the spur road is *ref:usfs=\"FR 505.1A\". Understanding how the reference numbers are assigned makes it easy to transmit your location over a radio or phone, and have somebody looking on a map find that location. Much easier than using GPS coordinates.</p> <p>For the highways that were traced off of satellite imagery, there is often a problem with forks in the road. Often tree cover or poor resolution imagery makes it hard to see the highway. And a lot of the highways go through an area with an entire network of other dirt roads, so the reference number may just group a bunch of highway segments. Often the most visible highway branch in the imagery at a fork is not the actual road. In this case the highway has to be split at the fork, and the new segment tagged for it's actual value, and the actual highway segment gets tagged correctly. This is critical if you want navigation to work.</p>"},{"location":"highways/#ground-truthing","title":"Ground-truthing","text":"<p>If you really want detailed and accurate maps, ground-truthing is an important part of the process. Road conditions change, especially the unmaintained dirt roads. Years of erosion, off-road vehicle abuse, etc... all change. For this reason the surface, smoothness and tracktype tags are not merged, as what is in the external datasets is likely out of date. Also sometimes parts of a dirt road get paved, or access is closed off completely.</p> <p>This is a good excuse to go there for some hiking and camping fun. You can load data into StreetComplete when online, and then use that in the field since will likely be no cell phone connection. Depending on the software used to collect the data, that may need conflation before uploading, for example OpenDataKit data. Some detail on that process is in this Highway Mapping blog post about a field mapping trip.</p>"},{"location":"mvum/","title":"MVUM &amp; RoadCore Conversion","text":"<p>The MVUM and RoadCore datasets are all of the motor vehicle roads in a national forest. These are primarily remote dirt roads, often just a jeep track. These are heavily used for back country access for wildland fires and rescues. Currently much of this data has been imported in the past, complete with all the bugs in the dataset. The RoadCore dataset is a superset of the MVUM data, but otherwise the same. Since OpenStreetMap contains more than jeep tracks, the RoadCore dataset is usually used for conflation.</p> <p>This utility program normalizes the data, correcting or flagging bugs as an aid for better conflation. It can process both the MVUM and RoadCore datasets.. The schema between these two is the same other than a subtle difference in field names.</p> <p>The original datasets can be found here on the USDA  FSGeodata Clearinghouse website.</p>"},{"location":"mvum/#dataset-bugs","title":"Dataset Bugs","text":"<p>No dataset is bug free. </p>"},{"location":"mvum/#bad-geometry","title":"Bad Geometry","text":"<p>There are many instances where a highway in the MVUM or RoadCore data is a MultiLineString instead of just a LineString. The problem with these are sometimes the segments are far apart, with long sections with no data. These are all the same highway, just bad data. To me it looks like somebodies's GPS had a dropped signal in places when they were recording a track.</p>"},{"location":"mvum/#bad-reference-numbers","title":"Bad Reference Numbers","text":"<p>Every national foreset has it's own schema for reference numbers. There is some similarity between them all, but sometimes it's hard to tell the difference between a bad reference number, and a local numbering scheme. Having stared at much data, my guess is sometimes a prefix represents something like a trail vs a road. As I work my way through each forest, I'll try to document what appears to be a local numbering scheme.</p> <p>In some areas the MVUM and RoadCore data has extract numerals prefixed to the actual reference number. These are all usually in the same area, so I assume whomever was doing data entry had a sticky keyboard, it got messed up when converting from paper maps to digital, who really knows. But it makes that tag worthless. Utah datasets in particular suffer greatly from this problem.</p> <p>Another common problem in the reference nummbers is in some areas the major maintained roads have a .1 appended. All minor part of the number should always have a letter appended. So FR 432.1\" is actually *FR 432\", whereas \"432.1A is correct. This was confirmed by reviewing multiple other map sources, as the paper and PDF version of the dataset has the correct version without the .1 appended. Obviously this dataset is not used to produce the maps you can get from the Forest Service.</p> <p>Cleaning up all the wrong reference numbers will make OSM the best map for road and trail navigation on public lands.</p>"},{"location":"mvum/#dixie-national-forest","title":"Dixie National Forest","text":"<p>In the current MVUM and RoadCore datasets for this national forest, for some reason a 30, 31, 32, 33, 34 has been prefixed to many of the IDs, making the reference numbers wrong. After staring at the original data file, I noticed these were all 5 characters long, and lacked a letter or a minor number suffix. Limiting the trigger to just that case seems to fix the problem. A note is added to any feature where the ref:usfs is changed as an aid towards validation.</p>"},{"location":"mvum/#manti-lasal-national-forest","title":"Manti-LaSal National Forest","text":"<p>In the current MVUM dataset for this national forest, for some reason a 5 or 7 has been prefixed to many of the IDs, making the reference numbers wrong.</p>"},{"location":"mvum/#fishlake-national-forest","title":"Fishlake National Forest","text":"<p>In the current MVUM dataset for this national forest, for some reason a 4 or 40 has been prefixed to some of the IDs, making the reference numbers wrong.</p>"},{"location":"mvum/#mount-hood-national-forest","title":"Mount Hood National Forest","text":"<p>For some reason, some of the reference numbers have a 000 appended, making the reference numbers wrong. This applies to paved roads, not just remote jeep tracks.</p>"},{"location":"mvum/#doesnt-match-the-sign","title":"Doesn't Match The Sign","text":"<p>There is an issue with the USFS reference numbers not matching the sign. This is luckily limited to whether there is a .1 appended to the reference number without an letter at the end. Usually a reference without a .1 is a primary road, and the .1 gets appended for a major branch off that road. While out ground-truthing MVUM roads recently I saw multiple examples where the reference numnber in the MVUM data (and often in OSM) has the .1, so I use that value regardless of what the sign says. It's still quite obviously what the reference number is since the only difference is the .1 suffix.</p> <p>This gets more interesting when you compare with other data sources, ie... paper and digital maps. Older data source seem to drop the .1, whereas the same road in a newer version of the dataset has the .1 suffix. So I figure anyone navigating remote roads that checks their other maps would figure out which way to go. So anyway, when way out on remote very_bad or horrible MVUM roads, you should have multiple maps if you don't want to get confused.</p>"},{"location":"mvum/#missing-geometry","title":"Missing Geometry","text":"<p>There are features with no geometry at all, but the tags all match an existing feature that does have a geometry. These appear to be accidental duplicates, so they get removed.</p>"},{"location":"mvum/#dropped-fields","title":"Dropped Fields","text":"<p>These fields are dropped as they aren't useful for OpenStreetMap.</p> <ul> <li>TE_CN</li> <li>BMP</li> <li>EMP</li> <li>SYMBOL_CODE</li> <li>SEG_LENGTH</li> <li>JURISDICTION</li> <li>SYSTEM</li> <li>ROUTE_STATUS</li> <li>OBJECTIVE_MAINT_LEVEL</li> <li>FUNCTIONAL_CLASS</li> <li>LANES</li> <li>COUNTY</li> <li>CONGRESSIONAL_DISTRICT</li> <li>ADMIN_ORG</li> <li>SERVICE_LIFE</li> <li>LEVEL_OF_SERVICE</li> <li>PFSR_CLASSIFICATION</li> <li>MANAGING_ORG</li> <li>LOC_ERROR</li> <li>GIS_MILES</li> <li>SECURITY_ID</li> <li>OPENFORUSETO</li> <li>IVM_SYMBOL</li> <li>GLOBALID</li> <li>SHAPE_Length</li> </ul>"},{"location":"mvum/#preserved-fields","title":"Preserved Fields","text":"<p>The field names are a bit truncated in the dataset, but these are the ones that are converted. The MVU and RoaCore datasets uses the same columns names, the only difference is whether an underbar is used.</p> <ul> <li>ID is id</li> <li>NAME is name</li> <li>OPER_MAINT_LEVEL is smoothness</li> <li>SYMBOL_NAME smoothness</li> <li>SURFACE_TYPE is surface</li> <li>SEASONAL is seasonal</li> <li>PRIMARY_MAINTAINER is operator</li> </ul>"},{"location":"mvum/#abbreviations","title":"Abbreviations","text":"<p>There are multiple and somewhat inconsistent abbreviations in the MVUM dataset highway names. OpenStreetMap should be using the full value. These were all found by the conflation software when trying to match names between two features. Since much of the MVUM data is of varying quality, there's probably a few not captured here that will have to be fixed when editing the data. This however improves the conflation results to limit manual editing.</p> <ul> <li>\" Cr \" is \" Creek \"</li> <li>\" Cr. \" is \" Creek \"</li> <li>\" Crk \" is \" Creek \"</li> <li>\" Cg \" is \" Campground \"</li> <li>\" Rd. \" is \" Road\"</li> <li>\" Mt \" is \" Mountain\"</li> <li>\" Mtn \" is \" Mountain\"</li> <li>\" Disp \" is \" Dispersed\"</li> <li>\" Rd. \" is \" Road\"</li> <li>\" Mtn. \" is \" Mountain\"</li> <li>\" Mtn \" is \" Mountain\"</li> <li>\" Lk \" is \" Lake\"</li> <li>\" Resvr \" is \" Reservoir\"</li> <li>\" Spg \" is \" Spring\"</li> <li>\" Br \" is \" Bridge\"</li> <li>\" N \" is \" North\"</li> <li>\" W \" is \" West\"</li> <li>\" E \" is \" East\"</li> <li>\" S \" is \" South\"</li> <li>\" So \" is \" South\"</li> </ul>"},{"location":"mvum/#tag-values","title":"Tag values","text":""},{"location":"mvum/#oper_maint_level","title":"OPER_MAINT_LEVEL","text":"<p>This field is used to determine the smoothness of the highway. Using the official forest service guidelines for this field, convienently they publish a Road Maintaince Guidelines, complete with muiltiple pictures and detaild technical information on each level. The coorelate these values, I did some ground-truthing on MVUM and I'd agree that level 2 is definetely high clearance vehicle only, and that it fits the definition here for very_bad, although some sections were more horrible, deeply rutted, big rocks, lots of erosion.</p> <ul> <li> <p>5 -HIGH DEGREE OF USER COMFORT:  Assigned to roads that provide a high degree of user comfort and convenience. This becomes smoothness=excellent.</p> </li> <li> <p>4 -MODERATE DEGREE OF USER COMFORT:  Assigned to roads that provide a moderate degree of user comfort and convenience at moderate travel speeds. This becomes smoothness=bad.</p> </li> <li> <p>3 -SUITABLE FOR PASSENGER CARS:  Assigned to roads open for and maintained for travel by a prudent driver in a standard passenger car. This becomes smnoothness=good.</p> </li> <li> <p>2 -HIGH CLEARANCE VEHICLES:  Assigned to roads open for use by high clearance vehicles. This adds 4wd_only=yes and becomes smoothness=vary_bad.</p> </li> <li> <p>1 -BASIC CUSTODIAL CARE (CLOSED):  Assigned to roads that have been placed in storage (&gt; one year) between intermittent uses. Basic custodial maintenance is performed. Road is closed to vehicular traffic. This becomes access=no</p> </li> </ul>"},{"location":"mvum/#symbol_name","title":"SYMBOL_NAME","text":"<p>Sometimes OPER_MAINT_LEVEL doesn't have a value, so this is used as a backup. These values are not used to update the existing values in OSM, they are only used for route planning ground-truthing trips.</p> <ul> <li>Gravel Road, Suitable for Passenger Car becomes surface=gravel</li> <li>Dirt Road, Suitable for Passenger Car becomes surface=dirt</li> <li>Road, Not Maintained for Passenger Car becomes smoothness=very_bad</li> <li>Paved Road becomes surface=paved</li> </ul>"},{"location":"mvum/#surface_type","title":"SURFACE_TYPE","text":"<p>This is another field that is converted, but not used when editing the existing OSM feature. This can only really be determined by ground-truthing, but it converted as another aid for route planning.</p> <ul> <li>AGG -CRUSHED AGGREGATE OR GRAVEL becomes surface=gravel</li> <li>AC -ASPHALT becomes surface=asphalt</li> <li>IMP -IMPROVED NATIVE MATERIAL becomes surface=compacted</li> <li>CSOIL -COMPACTED SOIL becomes surface=compacted</li> <li>NAT -NATIVE MATERIAL becomes surface=dirt</li> <li>P - PAVED becomes surface=paved</li> </ul>"},{"location":"mvum/#name","title":"Name","text":"<p>The name is always in all capitol letters, so this is converted to a standard first letter of every word is upper case, the rest is lower case.</p>"},{"location":"mvum/#options","title":"Options","text":"<pre><code>-h, --help            show this help message and exit\n-v, --verbose         verbose output\n-i INFILE, --infile INFILE MVUM data file\n-c, --convert         Convert MVUM feature to OSM feature\n-o OUTFILE, --outfile OUTFILE Output GeoJson file\n</code></pre>"},{"location":"odk2osm/","title":"odk2osm","text":"<p>This programs reads the various data formats used with OpenDataKit, and converts them to an OSM XML file, and a GeoJson file. The two formats loaded from ODK Central are a CSV file downloaded from the submissions page, or a JSON file downloaded using ODATA</p> <p>In addition for working offline, this can also parse the ODK XML format used for the instances files in ODK Collect. When working in the field, I use adb to pull files off my smartphone using a USB cable.</p> <p>The options for this program are:</p> <p>Convert ODK XML instance file to OSM XML format</p> <p>options:    -h, --help            show this help message and exit    -v [VERBOSE], --verbose [VERBOSE]        verbose output    -y YAML, --yaml YAML  Alternate YAML file    -x XLSFILE, --xlsfile XLSFILE        Source XLSFile    -i INFILE, --infile INFILE        The input file By default, the xforms.yaml file is used when converting to OSM XML format. Using the --yaml option allows you to have a custom conversion of the data collected by your XLSForm. This only applies to the OSM XML output, when processing the GeoJson file, no conversion is done.</p> <p>The input file is the CSV or JSON file downloaded from ODK Central. ODK Collect stores the instance files in a collection of sub-directories that are timestamped and have a unique instance number as part of the file name. The primary part of the filename is the same as the title of the XLSForm.</p> <p>For example:</p> <p>instances/Buildings_3_2024-05-28_18-34-38/Buildings_3_2024-05-28_18-34-38.xml  instances/Buildings_2_2024-01-24_13-36-20/Buildings_2_2024-01-24_13-36-20.xml  instances/Buildings_3_2024-05-31_11-08-22/Buildings_3_2024-05-31_11-08-22.xml  instances/Buildings_2_2024-01-26_15-16-53/Buildings_2_2024-01-26_15-16-53.xml  instances/Buildings_2_2024-01-26_15-07-17/Buildings_2_2024-01-26_15-07-17.xml  instances/Buildings_3_2024-05-29_11-46-53/Buildings_3_2024-05-29_11-46-53.xml  instances/Buildings_3_2024-06-03_11-14-02/Buildings_3_2024-06-03_11-14-02.xml  instances/Buildings_3_2024-06-03_10-33-27/Buildings_3_2024-06-03_10-33-27.xml  instances/Buildings_2_2024-01-26_11-42-38/Buildings_2_2024-01-26_11-42-38.xml  instances/Buildings_3_2024-05-29_12-13-37/Buildings_3_2024-05-29_12-13-37.xml  ...</p> <p>In this case the parameter passed to odk2osm can contain a regular expression to process multiple files, as each time you open Collect, it creates a new directory and file. The output is a single file. So in this case, run odk2osm like this:</p> <p>odk2osm -v -i Buildings_3* -x Buildings.xls</p> <p>The --xlsfile is used to specigy the XLSForm that was used for this mapping session. This is used to supply the correct data type of each entry collected.</p>"},{"location":"odkconflation/","title":"Conflating OpenDataKit with OpenStreetMap","text":"<p>Typically conflation is done when doing data imports, but not always. Data collected in the field can be considered an import. Conflating buildings or POIs from external data is relatively easy as it's already been cleaned up and validated. When you are doing field mapping, then you have to cleanup and validate the data during conflation. This is a time consuming process even with good conflation software.</p> <p>I've worked with multiple conflation software over the years. Hootenanny, OpenJump (later forked into RoadMatcher), etc...  which currently are now dead projects. Conflation is a hard technical challenge and often the results are poor and unstatisfing result. For smalller datasets often it's easier to do do manual conflation using JOSM or Qgis. This project tries to simply the problem by focusing on OpenStreetMap data.</p>"},{"location":"odkconflation/#smartphone-data-collection","title":"Smartphone Data Collection","text":"<p>While commercial organizations may use expensive GPS devices, most of us that do data collection as a volunteer or for an NGO use their smartphone. Their is a variety of smartphone apps for data collection that fall ihnto two categories. The first category are the apps like Vespucci, StreetComplete, and Organic Maps. These directly upload to OpenStreetMap. These are great for the casual mapper who only adds data occasionally and is limited to a POI. For example, a casual mapper may want to add the restaurant they are currrently eating in when they notices it's not in OpenStreetMap. In addition, they probably have a cell phone connection, so the data gets added right away.</p> <p>The other category are apps like ODK Collect, QField ArcGIS Field Maps which are oriented to larger scale mapping projects, often offline without any cellular connection. These collect a lot of data that then needs to get processed later. And conflation is part of this process.</p> <p>All of these smartphone based data collection apps suffer from poor GPS location accuracy. Modern smartphones (2024) are often 5-9 meters off the actual location, sometimes worse. In addition when field data collecting, you can't always record the actual location you want, you can only record where you are standing.</p> <p>You can improve the location data somewhat if you have a good quality basemap, for example you see a building within a courthouse wall when you are standing in the street. If you have a basemap, typically satellite imagery, you can touch the location on the basemap, and use that instead of where you are standing. Then later when conflating, you have a much higher chance the process will be less painful.</p>"},{"location":"odkconflation/#opendatakit","title":"OpenDataKit","text":"<p>OpenDataKit is a format for data import forms used to collect custom data. The source file is a spreadsheet, called an XLSForm. This gets used by the mobile app for the quesion and answer process defined by the XLSForm. There are multiple apps and projects using XLSForms, so it's well supported and maintained.</p> <p>The XLS source file syntax is a bit wierd at first, being a spreadsheet, so the osm-fieldwork project contains tested XLSForm templates for a variety of mapping project goals. These can be used to create efficient XForms that are easy to convert to OSM. The primary task when manually converting ODK collected data into OSM format is converting the tags. If the XLSForm is created with a focus towards OSM the XLSForm can make this a much simpler process. This is detailed more in this document. Simply stated, what is in the name colum in the XLSForm becomes the name of the tag in OSM, and the response from the choices sheet becomes the value.</p>"},{"location":"odkconflation/#odk-collect-central","title":"ODK Collect &amp; Central","text":"<p>ODK Collect is a mobile app for data collection using XLSForms. It's server side is ODK Central, which replaces the  older ODK Aggregate. ODK Central manages the XLSForms downloaded to your phone, as wall as the submissions uploaded from your phone when back online.</p> <p>A related project for processing ODK data and working remotely with Central is osm-fieldwork. This Python project handles conversion of the various data files from Collect or Central, into OSM XML and GeoJson for future processing via editing or conflation. This is heavily used in the FMTM backend.</p>"},{"location":"odkconflation/#field-data-collection","title":"Field Data Collection","text":"<p>Collecting data in the field is to best way to add data to OpenStreetMap. Whether done by casual mappers adding POIs, to more dedicated mappers, what is reality at that moment is the key to keeping OSM fresh and updated. When it comes to improving the metadata for buildings, many have been imported with building=yes from remote mapping using the HOT Tasking Manager to trace buildings from satellite imagery. </p> <p>But ground-truthing what kind of building it is improvers the map. It may be a medical clinic, restaurant, residence, etc.. who know until somebody stands in front of the building to collect more informsation about it. This may be idenifying it as a clinic or reseidence, adding the building material, what is the roof made of, is it's power non-existance, or are there solar panels or a generator ? Some humanitarian mapping is collecting data on public toilets, and community water sources for future improvements. </p> <p>Knowing there is a building on the map is useful, but better yet is what is the building used for ? What is it made of ? Does it have AC or DC power ? Water available ? All of these details improve the map to make it more useful to others.</p>"},{"location":"odkconflation/#field-mapping-tasking-manager","title":"Field Mapping Tasking Manager","text":"<p>The Field Mapping Tasking Manager (FMTM) is a project to oprganize large scale data collection using ODK Collect and ODK Central. It uses the osm-fieldwork project for much of the backend processing of the ODK data,  but is designed for large scale field mapping involving many people. It uses ODK Collect and ODK Central as the primary tools. One of the final steps in processing ODK data to import into OSM is conflating it with existing data. This can be done manually of course, but with a large number of data submissions this becomes tedious and time consuming. FMTM aggrgates all the data for an entire project, and may have thousands of submissions. This is where conflation is critical.</p>"},{"location":"odkconflation/#the-algorythm","title":"The Algorythm","text":"<p>Currently conflation is focused on ODK with OSM. This uses the conflator.py program which can conflate between the ODK data and an OSM data extract. There are other conflation programs in this project for other external datasets, but uses a postgres database instead of two files.</p>"},{"location":"odkconflation/#the-conflator-class","title":"The Conflator() Class","text":"<p>This is the primary interface for conflating files. It has two primary endpoint. This top level endpoint is Conflator.conflateFiles(), which is used when the conflator program is run standalone. It opens the two disk files, parses the various formats, and generates a data structure used for conflation. This class uses the Parsers() class from osm-fieldwork that can parse the JSON or CSV files downloaded from ODK Central, or the ODK XML \"instance\" files when working offline. OPSM XML or GeoJson files are also supported. Each entry in the files is turned into list of python dicts to make it easier to compaert the data.</p> <p>Once the two files are read, the Conflator.conflateFeatures() endpoint takes the two lists of data and does the actual conflation. There is an additional parameter passed to this endpoint that is the threshold distance. This is used to find all features in the OSM data extract within that distance. Note that this is a unit of the earth's circumforance, not meters, so distance calulations are a bit fuzzy.</p> <p>This is a brute force conflation algorythm, not fast but it tries to be complete. it is comprised of two loops. The top level loops through the ODK data. For each ODK data entry, it finds all the OSM features within that threshold distance. The inner loop then uses the closest feature and compares the tags. This is where things get interesting.... If there is a name tag in the ODK data, this is string compared with the name in the closest OSM feature. Fuzzy string matching is used to handle minor spelling differences. Sometimes the mis-spelling is in the OSM data, but often when entering names of features on your smartphone, mis-typing occurs. If there is a 100% match in the name tags, then chances are the feature exists in OSM already.</p> <p>If there is no name tag in the ODK data, then the other tags are compared to try to find a possible duplicate feature. For example, a public toilet at a trailhead has no name, but if both ODK and OSM have amenity=toilet, then it's very likey a duplicate. If no tags match, then the ODK data is proably a new feature.</p> <p>Any time a possible duplicate is found, it is not automatically merged. Instead a fixme tag is added to the feature in the output file with a statement that it is potentially a duplicate. When the output file is loaded into JOSM, you can search for this tag to manually decide if it is a duplicate.</p>"},{"location":"odkconflation/#xlsform-design","title":"XLSForm Design","text":"<p>Part of the key detail to improve conflation requires a carefully created XLSForm. There is much more detailed information on XLSForm design, but briefly whatever is in the name column in the survey sheet becomes the name of the tags, and whatever is in the name column in the choices sheet becomes the value. If you want a relatively smooth conflation, make sure your XLSForm uses OSM tagging schemas.</p> <p>If you don't follow OSM tagging, then conflation will assumme all your ODK data is a new feature, and you'll have to manually conflate the results using JOSM. That's OK for small datasets, but quickly becomes very tedious for the larger datasets that FMTM collects.</p>"},{"location":"odkconflation/#the-output-file","title":"The Output File","text":"<p>The output file must be in OSM XML to enable updating the ways. If the OSM data is a POI, viewing it in JOSM is easy. If the OSM data is a polygon, when loaded into JOSM, they won't appear at first. Since the OSM way created by conflation has preserved the refs used by OSM XML to reference the nodes, doing update modified in JOSM then pulls down the nodes and all the polygons will appear.</p>"},{"location":"odkconflation/#conflicts","title":"Conflicts","text":"<p>There are some interesting issues to fix post conflation. ODK data is usually a single POI, whereas in OSM it may be a polygon. Sometimes though the POI is already in OSM. Remote mapping or building footprint imports often have a polygon with a single building=yes tag. If the POI we collected in ODK has more data, for example this building is a restaurant serving pizza, and is made of brick.</p> <p>In OSM sometimes there is a POI for an amenity, as well as a building polygon that were added at different times by different people.  The key detail for conflation is do any of the tags and values from the new data match existing data ?</p> <p>FMTM downloads a data extract from OSM using osm-rawdata, and then filters the data extract based on what is on the choices sheet of the XLSForm. Otherwise Collect won't launch. Because this data extract does not contain all the tags that are in OSM, it creates conflicts. This problem is FMTM specific, and can be improved by making more complete data extract from OSM.</p> <p>When the only tag in the OSM data is building=, any tags from ODK are merged with the building polygon when possible. If the OSM feature has other tags, JOSM will flag this as a conflict. Then you have to manually merge the tags in JOSM.</p>"},{"location":"osm-merge/","title":"Conflator Program","text":"<p>osm-merge is a program that conflates building footprint data with OpenStreetMap data to remove duplicates. The result of the conflation process is buildings that only exist in the footprints data file.</p> <p>This program can process data from either a postgres database, or data files in geojson, shapefile format. One of the core concepts is using a data file of polygons to filter the larger datasets, since a database may contain multiple countries.</p> <p>The process of setting up for large scale conflation is in this document.</p>"},{"location":"osm-merge/#command-line-options","title":"Command Line Options","text":""},{"location":"osm-merge/#common-options","title":"Common Options","text":"<p>These are the nost commonly used options.</p> <pre><code>--help(-h)       Get command line options\n--verbose(-v)    Enable verbose output\n--boundary(-b)   Specify a multipolygon for boundaries, one file for each polygon\n--project(-p)    Tasking Manager project ID to get boundaries from database\n--osmdata(-x)    OSM XML/PBF or OSM database to get boundaries (prefix with pg: if database)\n--outdir(-o)     Output file prefix for output files (default \"/tmp/tmproject-\")\n--footprints(-f) File or building footprints Database URL (prefix with pg: if database)\n--dbhost(-d)     Database host, defaults to \"localhost\"\n--dbuser(-u)     Database user, defaults to current user\n--dbpass(-w)     Database user, defaults to no password needed\n</code></pre>"},{"location":"osm-merge/#tasking-manager-options","title":"Tasking Manager Options","text":"<p>These options are used to dynamically extract a project boundary from a Tasking Manager database. A more common usage is to use the splitter.py program to download the project boundary from the Tasking Manager itself.</p> <pre><code>--splittasks     When using the Tasking Manager database, split into tasks\n--schema         OSM database schema (pgsnapshot, ogr2ogr, osm2pgsql) defaults to \"pgsnapshot\"\n--tmdata(-t)     Tasking Manager database to get boundaries if no boundary file prefix with pg: for database usage, http for REST API\n</code></pre>"},{"location":"osm-merge/#osm-options","title":"OSM Options","text":"<p>When extracting administrative boundaries from an OpenStreetMap database, the default admin levl is 4, which is commonly used for couty boundaries. This lets the user select what level of administrative boundaries they want.</p> <pre><code>--admin(-a)      When querying the OSM database, this is the admin_level, (defaults to 4)\n</code></pre>"},{"location":"osm-merge/#examples","title":"Examples","text":"<p>PATH/conflator.py -v -x 12057-osm.geojson -f 12057-ms.geojson -o 12057</p> <p>This takes two disk files, which have already been filtered to only contain data for the area to conflate.</p> <p>PATH/conflator.py -v -x pg:kenya -b 12007-project.geojson -f 12057-ms.geojson -o 12057</p> <p>This uses a database that contains all of Kenya, but we only want to process a single project, so that's supplied as the boundary. The foorptin data was already filtered using ogr2ogr, and the project ID is used as the prefix for the output files.</p> <p>PATH/conflator.py -v -x pg:kenya -b 12007-project.geojson -f pg:kenya_footprints -o 12057 -d mapdb -u me</p> <p>This is the same except the database is on a remote machine called mapdb and the user needs to be me.</p> <p>PATH/conflator.py -t tmsnap -p 8345 -b pg:kenya_foot -o pg:Kenya</p> <p>Reads from 3 data sources. The first one is a snapshot of the Tasking Manager database, and we want to use project 8345 as the boundary. The two data sources are prefixed with \"pg\", which defines them as a database URL instead of a file. The database needs to be running locally in this case.</p>"},{"location":"osm2favorites/","title":"osm2favorities.py","text":"<p>This is a simple utility that generates a GPX file from OSM data or a GeoJson file for Osmand. This makes the data a available under My Places in the Osmand menu.This is useful for a field mapping project that covers a large area, but with a few small areas of interest. This makes them all readily available for navigation. For some features this program also adds Osmand styling to change the displayed icons and colors.</p>"},{"location":"osm2favorites/#options","title":"options","text":"<pre><code>-h, --help            show this help message and exit\n-v, --verbose         verbose output\n-i INFILE, --infile INFILE\n                    The data extract\n</code></pre>"},{"location":"osmhighways/","title":"OpenStreetMap Data","text":"<p>Being crowd sourced and open to all who want to contribute, OpenStreetMap (OSM) has infinite flexibility in the various tag/values used for metadata. Many of the tags not in common use are ignored by the renderers and routing engines, but still live in the database and data files. You'd really only notice if you're deep in the data, which is the key to good conflation.</p> <p>The features in OSM come from a wide variety of sources. Mobile apps, imports, satellite imagery. Often features traced from imagery are lacking any tags beyond building=yes or highway=track, which we hope to improve on by conflating with other datasets.</p>"},{"location":"osmhighways/#data-janitor","title":"Data Janitor","text":"<p>Being a data janitor is important, if rather boring and tedious task. Bugs in the data can lead to navigation problems at the very least. An accurate and detailed map is a thing of beauty, and often OSM gets really close.</p> <p>Unfortunately to conflate OSM data with external data sources, it needs to be cleaned up. Normally it gets cleaned up by the mapper, who has to manually review and edit the tags. Since the highway name is an important item used to confirm a near match in geometry, too much variety can make this a slow process.</p> <p>This project has an osmhighways.py program that is used to cleanup some of the problems, like deleting unnecessary tags, and fixing the name vs reference number problem. Deleting all bogus tags reduces the data size, which is a benefit. This project also extracts only highway linestrings, so a clean dataset for conflating geometries.</p>"},{"location":"osmhighways/#old-imports","title":"Old Imports","text":"<p>OpenStreetMap (OSM) has a past history of imports, often done way back when OSM had little highway data. This was a way to bootstrap navigation, and it mostly worked. </p>"},{"location":"osmhighways/#tiger","title":"TIGER","text":"<p>Since it was publically available, the data used by the US Census Department was imported around 2007. The data is of varying quality, but was better than nothing. The OSM community has been cleaning up the mess ever since. More information on the TIGER fixup can be found  here.</p> <p>An small example of the tags added from TIGER, all of which can be deleted.</p> <pre><code>    &lt;tag k=\"tiger:name_base\" v=\"75th\"/&gt;\n    &lt;tag k=\"tiger:name_base_1\" v=\"75th\"/&gt;\n    &lt;tag k=\"tiger:name_direction_prefix\" v=\"N\"/&gt;\n    &lt;tag k=\"tiger:name_type\" v=\"St\"/&gt;\n    &lt;tag k=\"tiger:name_type_1\" v=\"St\"/&gt;\n    &lt;tag k=\"tiger:cfcc\" v=\"A41\"/&gt;\n    &lt;tag k=\"tiger:reviewed\" v=\"no\"/&gt;\n</code></pre> <p>I don't think I've ever seen a tiger:reviewed=yes tag.</p>"},{"location":"osmhighways/#motor-vehicle-use-map-mvum","title":"Motor Vehicle Use Map (MVUM)","text":"<p>The MVUM data is highways in national forests, so useful in remote area not always in TIGER. Or in TIGER but completely wrong. I've seen roads in TIGER that don't actually exist. All the MVUM data is better quality as much of the data was mapped by ground-truthing. It has useful data fields, like is a high clearance vehicle needed, what is the surface, and other access data like are ATVs allowed ?</p> <p>[MVUM)](https://data.fs.usda.gov/geodata/edw/edw_resources/shp/S_USA.Road_MVUM.zip</p>"},{"location":"osmhighways/#clipping","title":"Clipping","text":"<p>To support conflation, even OSM data needs to be chopped into smaller pieces. While osmium and osmfilter could so this, I've had problmes with the other tools when the task polygon is small. The osmhighways.py program also clips files. Since it's in an OSM data format, we can't really use shapely, or geopandas, just osmium. It's a bit slow, being pure python. If it's a continuing problem I'll refactor it into C++.</p> <p>There is a question as to whether it's better to clip a highway at the bundary, or include ther part of it's geometry tha's outside the boundary. I'm experimenting with both, and seeing how it effects conflation.</p>"},{"location":"osmhighways/#options","title":"Options","text":"<pre><code>-h, --help                     show this help message and exit\n-v, --verbose                  verbose output\n-i INFILE, --infile INFILE     Input data file\n-o OUTFILE, --outfile OUTFILE  Output filename\n-c CLIP, --clip CLIP           Clip data extract by polygon\n-s SMALL, --small SMALL        Small dataset\n</code></pre> <p>This program extracts all the highways from an OSM file, and correct as many of the bugs with names that are actually a reference number.</p> <pre><code>For Example:\n    osmhighways.py -v -i colorado-latest.osm.pbf -o co-highways.osm\n</code></pre>"},{"location":"setup/","title":"Setup Infrastructure","text":"<p>This is more focused on the backend of the website that makes all the processed data available. Or other crazy people who have a need to work on much larger areas. The output datasets this will generate a lot of files if you plan to work with multiple national forests or parks. I use a tree structure. At the top is the directory with all the source files. You also need a directory with the national forest or park boundaries which get used for data clipping.</p> <p>Once I have the source files ready, I start the splitting up process to make data extracts for each forest or park. If you are only working on one forest or park, you can do this manually. Since I'm working with data for multiple states, I wrote a shell script to automate the process. For OSM, you need to either merge multiple state level data into a single file, or use the very large US dataset.</p> <p>Since this project is focused on remote highways and trails, for the source OSM dataset I use a subset of the entire US, a data extract of only highways. This will get clipped by the boundary of each area when updating source datasets.</p>"},{"location":"setup/#boundaries","title":"Boundaries","text":"<p>You need boundaries with a good geometry. These can be extracted from OpenStreetMap, they're usually relations. The official boundaries are also available from the same site as the datasets as a MultiPolygon. This file contains boundaries for the entire US, so to use other tools like ogr2ogr or osmium, it needs to be split into separate files.</p> <p>I use the TM Splitter utility included in this project to split the MultiPolygon into these separate files, one for each public land area. Each of these files are also a MultiPolygon, since often a national forest or park has several areas that aren't connected.</p>"},{"location":"setup/#updatesh","title":"update.sh","text":"<p>Most of the process is executing other external programs like osmium or ogr2ogr, so I wrote a bourne shell script to handle all the repetitious tasks. This also lets me easily regenerate all the files if I make a change to any of the utilities or the process. This uses a modern shell syntax with functions and data structures to reduce cut &amp; paste.</p> <p>The command line options this program supports are:</p> <pre><code>--tasks (-t): Split tasks boundaries into files for ogr2ogr\n--datasets (-d): Build only this dataset for all boundaries\n--split (-s): Split the AOI into tasks, also very slow\n--extract (-e): Make a data extract from OSM\n--only (-o): Only process one state\n--dry run (-n): Don't actually write any datafiles\n--base (-b): build all base datasets, which is slow\n</code></pre> <p>The locations of the files is configurable, so it can easily be extended for other forests or parks. This script is in the cont rib directory of this project.</p> <p>This also assumes you want to build a tree of output directories.</p> <p>For example I use this layout:</p> <pre><code>SourceData\n    -&gt; Tasks\n        -&gt; Colorado\n            -&gt; Medicine_Bow_Rout_National_Forest_Tasks\n                -&gt; Medicine_Bow_Rout_Task_[task number]\n            -&gt; Rocky_Mountain_National_Park_Task\n                -&gt; Rocky_Mountain_National_Park_Task_[task number]\n        -&gt; Utah\n            -&gt; Bryce_Canyon_National_Park_Tasks\n        etc...\n</code></pre> <p>All my source datasets are in SourceData.   In the Tasks directory I have all the Multi Polygon files for each forest or park. I create these files by running update.sh --split. These are the large files that have the AOI split into 5000km sq polygons.</p> <p>Since I'm working with multiple states, that's the next level, and only contains the sub directories for all the forests or parks in that state. Note that some national forests cross state boundaries. This does not effect task splitting, as that's done using the boundary of the entire forest. It does effect making the OSM data extract if you are using data downloaded from GeoFabrik, since they are usually done on a per state basis. The source data extract needs to cover multiple states, or the entire US. Multiple state data files can be merged using ogrmerge.</p> <p>While there is support for all the states, right now it's focused on Colorado, Utah, and Wyoming. I use these for debugging the data splitting and conversion processes, and to prepare the data extracts for conflation. The other states can be reactivated by uncommenting them in the shell script.</p>"},{"location":"tm-splitter/","title":"TM Splitter Utility","text":"<p>This is a simple utility for task splitting to reduce large datasets into more manageble sizes. It is oriented towards generating a MultiPolyon or Polygons which can then be be used by ogr2ogr or osmium for making data extracts.</p>"},{"location":"tm-splitter/#administrative-boundaries","title":"Administrative Boundaries","text":"<p>The administrative boundary files are a MultiPolygon of every national forest, park, or wilderness aea in the United States. Using the --split option creates a MultiPolygon of each region that can be used to make data extracts using ogr2ogr or osmium. Each output file has the name of the region as the filename.</p> <p>This is only useful if you are just getting started on a large mappoing campaign. For OSM Merge, this splits all the public lands into a file that can then be furthur split.</p>"},{"location":"tm-splitter/#grid-creation","title":"Grid Creation","text":"<p>Once the files for each region have been generated, they are still large. Next a grid can be generated from each region as a MultiPolygon. Each task in the grid is the maximum size supported to create a Tasking Manager project, which is 5000km square.</p>"},{"location":"tm-splitter/#task-creation","title":"Task Creation","text":"<p>To create a file for making data extracts, the grid can be further split into indivigual files for to use with ogr2ogr or osmium.</p>"},{"location":"tm-splitter/#options","title":"Options","text":"<pre><code>Usage: tm-splitter [-h] [-v] [-g] [-m] [-s] -i INFILE [-o OUTFILE]\n\noptions:\n-h, --help                             show this help message and exit\n-v, --verbose                          verbose output\n-s, --split                            Split Multipolygon\n-g, --grid                             Create a grid from an AOI\n-m, --meters                           Square area in meters\n-i INFILE, --infile INFILE             The input dataset\n-o OUTFILE, --outfile OUTFILE          Output filename\n</code></pre>"},{"location":"trails/","title":"National Park Service Trails","text":"<p>This processes both the National Park Service trails dataset, and the National Forest Service trail datasets. The schema of the two datasets is very similar. One of the differences is for Park Service Trails has two default tags in the output file which are bicycle=no and motor_vehicle=no. These default tags are documented here.</p> <p>This dataset is available in a variety of formats from the ArcGIS Hub.</p>"},{"location":"trails/#processed-fields","title":"Processed Fields","text":"<p>These are the fields extracted from the data that are converted to OpenStreetMap syntax so they can be conflated.</p> <ul> <li>OBJECTID becomes id</li> <li>TRLNAME becomes name</li> <li>TRLCLASS becomes sac_scale</li> <li>TRLUSE becomes yes for horse, bicycle, atv, etc...</li> <li>TRLALTNAME becomes alt_name</li> <li>SEASONAL becomes seasonal</li> <li>MAINTAINER becomas operator</li> <li>TRLSURFACE becomes surface</li> </ul>"},{"location":"trails/#dropped-fields","title":"Dropped Fields","text":"<p>These fields are all ignored, and are dropped from the output file.</p> <ul> <li>MAPLABEL</li> <li>TRLSTATUS</li> <li>TRLTYPE</li> <li>PUBLICDISP</li> <li>DATAACCESS</li> <li>ACCESSNOTE</li> <li>ORIGINATOR</li> <li>UNITCODE</li> <li>UNITNAME</li> <li>UNITTYPE</li> <li>GROUPCODE</li> <li>GROUPNAME</li> <li>REGIONCODE</li> <li>CREATEDATE</li> <li>EDITDATE</li> <li>LINETYPE</li> <li>MAPMETHOD</li> <li>MAPSOURCE</li> <li>SOURCEDATE</li> <li>XYACCURACY</li> <li>GEOMETRYID</li> <li>FEATUREID</li> <li>FACLOCID</li> <li>FACASSETID</li> <li>IMLOCID</li> <li>OBSERVABLE</li> <li>ISEXTANT</li> <li>OPENTOPUBL</li> <li>ALTLANGNAM</li> <li>ALTLANG</li> <li>NOTES</li> </ul>"},{"location":"trails/#national-forest-service-trails","title":"National Forest Service Trails","text":"<p>The US Forest Service makes much of their data publically accessible, so it's been a source for imports for a long time. There is a nice detailed wiki page on the Forest Service Data. The conversion process handles most of the implementation details.</p>"},{"location":"trails/#keep-fields","title":"Keep Fields","text":"<p>The two primary fields are TRAIL_NO, which is used for the ref:usfs tags, and TRAIL_NAME, which is the name of the trail. In addition to these</p>"},{"location":"trails/#the-5-variations","title":"The 5 Variations","text":"<p>For many of the features classes, there are 5 variations on each one which is used for access.</p> <ul> <li>Managed: Usage allowed and managed by the forest service</li> <li>Accepted: Usage is accepted year round</li> <li>Accepted/Discouraged: Usage is accepted, but discouraged</li> <li>Restricted: Usage is restricted</li> <li>Discouraged: Usage is discouraged</li> </ul> <p>These are converted to the apppropriate value.</p> <ul> <li>Managed* sets the keyword to designated</li> <li>Accepted* sets the keyword to yes</li> <li>Restricted* sets the keyword to no</li> <li>Discouraged* sets the keyword to discouraged</li> <li>Accepted/Discouraged* sets the keyword to permissive</li> </ul> <p>Many of the values for these are NULL, so ignored when generating the output file. If the value exists, it's either a Y or a N, which is used to set the values. For example: \"SNOWMOBILE\": \"Y\" becomes snowmobile=yes in the output file.</p> <ul> <li>PACK_SADDLE_ becomes horse=</li> <li>BICYCLE_ becomes bicycle=</li> <li>MOTORCYCLE_ becomes motorcycle=</li> <li>ATV_ becoms atv=</li> <li>FOURWD_ becomes 4wd_only</li> <li>SNOWMOBILE_ becomes snowmobile=</li> <li>SNOWSHOE_ becomes snowwhoe=</li> <li>XCOUNTRY_SKI_ becomes ski</li> </ul> <p>Currently these fields appear to be empty, but that may change in the future.</p> <ul> <li>SNOWCOACH_SNOWCAT_</li> <li>SNOWCOACH_SNOWCAT_</li> <li>E_BIKE_CLASS1_</li> <li>E_BIKE_CLASS2_</li> <li>E_BIKE_CLASS3_</li> </ul> <p>This field is ignored as it's assumed the trail is accessible by hikers.</p> <ul> <li>HIKER_PEDESTRIAN_</li> </ul>"},{"location":"trails/#dropped-fields_1","title":"Dropped Fields","text":"<p>These fields are dropped as unnecessary for OSM. Manye only have a NULL value anyway, so useless.</p> <ul> <li>MOTOR_WATERCRAFT_</li> <li>NONMOTOR_WATERCRAFT_</li> <li>GIS_MILES</li> <li>Geometry Column</li> <li>TRAIL_TYPE</li> <li>TRAIL_CN</li> <li>BMP</li> <li>EMP</li> <li>SEGMENT_LENGTH</li> <li>ADMIN_ORG</li> <li>MANAGING_ORG</li> <li>SECURITY_ID</li> <li>ATTRIBUTESUBSET</li> <li>NATIONAL_TRAIL_DESIGNATION</li> <li>TRAIL_CLASS</li> <li>ACCESSIBILITY_STATUS</li> <li>TRAIL_SURFACE</li> <li>SURFACE_FIRMNESS</li> <li>TYPICAL_TRAIL_GRADE</li> <li>TYPICAL_TREAD_WIDTH</li> <li>MINIMUM_TRAIL_WIDTH</li> <li>TYPICAL_TREAD_CROSS_SLOPE</li> <li>SPECIAL_MGMT_AREA</li> <li>TERRA_BASE_SYMBOLOGY</li> <li>MVUM_SYMBOL</li> <li>TERRA_MOTORIZED</li> <li>SNOW_MOTORIZED</li> <li>WATER_MOTORIZED</li> <li>ALLOWED_TERRA_USE</li> <li>ALLOWED_SNOW_USE</li> </ul>"},{"location":"trails/#options","title":"Options","text":"<pre><code>-h, --help            show this help message and exit\n-v, --verbose         verbose output\n-i INFILE, --infile INFILE input data file\n-c, --convert         Convert feature to OSM feature\n-o OUTFILE, --outfile OUTFILE Output GeoJson file\n</code></pre>"},{"location":"usgs/","title":"US Topographical Data","text":""},{"location":"usgs/#us-topographical-trails","title":"US Topographical Trails","text":"<ul> <li>OBJECTID</li> <li>permanenti</li> <li>name</li> <li>namealtern</li> <li>trailnumbe</li> <li>trailnum_1</li> <li>sourcefeat</li> <li>sourcedata</li> <li>sourceda_1</li> <li>sourceorig</li> <li>loaddate</li> <li>trailtype</li> <li>hikerpedes</li> <li>bicycle</li> <li>packsaddle</li> <li>atv</li> <li>motorcycle</li> <li>ohvover50i</li> <li>snowshoe</li> <li>crosscount</li> <li>dogsled</li> <li>snowmobile</li> <li>nonmotoriz</li> <li>motorizedw</li> <li>primarytra</li> <li>nationaltr</li> <li>lengthmile</li> <li>networklen</li> <li>SHAPE_Leng</li> </ul>"},{"location":"usgs/#us-topographical-highways","title":"US Topographical Highways","text":"<ul> <li>OBJECTID</li> <li>permanent_</li> <li>source_fea</li> <li>source_dat</li> <li>source_d_1</li> <li>source_ori</li> <li>loaddate</li> <li>interstate</li> <li>us_route</li> <li>state_rout</li> <li>county_rou</li> <li>federal_la</li> <li>stco_fipsc</li> <li>tnmfrc</li> <li>name</li> <li>mtfcc_code</li> <li>intersta_1</li> <li>intersta_2</li> <li>intersta_3</li> <li>us_route_a</li> <li>us_route_b</li> <li>us_route_c</li> <li>state_ro_1</li> <li>state_ro_2</li> <li>state_ro_3</li> <li>SHAPE_Leng</li> </ul>"},{"location":"utilities/","title":"Utility Programs","text":"<p>To conflate external datasets with OSM, the external data needs to be converted to the OSM tagging schema. Otherwise comparing tags gets very convoluted. Since every dataset uses a different schema, included are a few utility programs for converting external datasets. Currently the only datatsets are for highways. These datasets are available from the USDA, and have an appropriate license to use with OpenStreetMap. Indeed, some of this data has already been imported. The files are available from the  FSGeodata Clearinghouse</p> <p>Most of the fields in the dataset aren't needed for OSM, only the reference number if it has one, and the name. Most of these highways are already in OSM, but it's a bit of a mess, and mostly invalidated. Most of the problems are related to the TIGER import in 2007. So the goal of these utilities is to add in the TIGER fixup work by updating or adding the name and a reference number. These utilities prepare the dataset for conflation.</p> <p>There are other fields in the datasets we might want, like surface type, is it 4wd only, etc... but often the OSM data is more up to date. And to really get that right, you need to ground truth it.</p>"},{"location":"utilities/#mvumpy","title":"mvum.py","text":"<p>This converts the Motor Vehicle Use Map(MVUM) dataset that contains data on highways more suitable for offroad vehicles. Some require specialized offroad vehicles like a UTV or ATV. The data in OSM for these roads is really poor. Often the reference number is wrong, or lacks the suffix. We assume the USDA data is correct when it comes to name and reference number, and this will get handled later by conflation.</p>"},{"location":"utilities/#roadcorepy","title":"roadcore.py","text":"<p>This converts the Road Core vehicle map. This contains data on all highways in a national forest. It's similar to the MVUM dataset.</p>"},{"location":"utilities/#trailspy","title":"trails.py","text":"<p>This converts the NPSPublish Trail dataset. These are hiking trails not open to motor vehicles. Currently much of this dataset has empty fields, but the trail name and reference number is useful. This utility is to support the OpenStreetMap US Trails Initiative.</p>"},{"location":"utilities/#usgspy","title":"usgs.py","text":"<p>This converts the raw data used to print Topographical maps in the US. This obviously is a direct source when it comes to names if you want to be accurate. Although things do change over time, so you still have to validate it all. The files are available from the National Map. I use the Shapefiles, as the different categories are in separate files inside the zip. Each one covers a 7.5 quad square on a topo map. These have to be merged together into a single file to be practical.</p>"},{"location":"utilities/#osmhighwayspy","title":"osmhighways.py","text":"<p>On the OSM wiki, there is a list of incorrect tagging for forest highway names. Basically the name shouldn't be something like \"Forest Service Road 123.4A\". That's actually a reference number, not a name. This is primarily a problem with existing OSM data. These would all have to get manually fixed when validating in JOSM, so this program automates the process so you only have to validate, and not edit the feature. This also extracts only highway linestrings, so is used to create the OSM dataset for conflation. Since the other external datasets also correctly use name, ref, and ref:usfs, this simplifys conflation. Otherwise the algorithm would get very complicated and hard to maintain.</p>"},{"location":"utilities/#geojson2polypy","title":"geojson2poly.py","text":"<p>This is a very simple utility to convert a GeoJson boundary Multipolygon into an Osmosis poly file. This can be used with osmium, or osmconvert to make data extracts.</p>"},{"location":"validating/","title":"Validating The Conflation","text":"<p>Every feature must be validated before it can be uploaded to OpenStreetMap. OSM discourages automated edits  without human intervention. This software doesn't make any geometry changes, just tags, but still needs to be reviewed carefully. </p> <p>At it's core, conflation is just merging tags between datasets to avoid tedious cut &amp; paste. But this still needs to be validated as bugs and inconsistencies in the datasets can sneak in.</p>"},{"location":"validating/#my-process","title":"My Process","text":"<p>I have the two datasets that I was conflating loaded as layers in JOSM so I can check the original sources easily if needed. Since these are remote highways and trails, I use the USGS topographical basemap in JOSM. Course that can also lead to confusion as sometimes the reference number in the basemap has been truncated. When in doubt, I reference the latest dataset from the national forest service, which is the most correct. That dataset is in vtpk format, so doesn't work in JOSM, so I run it in QGIS.</p>"},{"location":"validating/#debug-tags","title":"Debug Tags","text":"<p>Currently a few tags are added to each feature to aid in validating and debugging the conflation algorythm. These should obviously be removed before uploading to OSM. They'll be removed at a future date after more validation of the software. These are:</p> <ul> <li>hits - The number of matching tags in a feature</li> <li>ratio - The ratio for name matching if not 100%</li> <li>dist -  The distance between features</li> <li>angle - The angle between two features</li> <li>slope - The slope between two features</li> </ul> <p>If a feature has 3 hits, that's probably an exact match. One hit is sufficient if there are no other nearby highways or trails. Two hits usually means the name and reference number matched. If a highway only contains highway=path, but the distance is very small, and there are no other nearby features, it's assumed to be a match. Modifying the current thresholds for distance, angle, and slope changes the results, so having these as temporary tags in the feature is useful when validating conflation results.</p>"},{"location":"validating/#missed-highway","title":"Missed Highway","text":"<p>Sometimes you see an external trail or highway that is not in the conflated data. This is because that highway is not currently in OSM, so we ignore it since we're focused on existing features.</p> <p></p> <p>When youhave the external dataset and OSM loaded in JOSM as layers, this screenshot shows the other layers under the conflated data and show as black lines. It's easy to see which layer it came from by toggling the layers on and off.</p> <p>The conflation software can also produce a data file of highways in the MVUM dataset that aren't in OSM. Importing those is a different process, so not discussed here. The conflation software can also produce a data file of all the trails in OSM that are not in any official sources.</p>"},{"location":"validating/#missing-segments","title":"Missing Segments","text":"<p>Sometimes the external datset has missing segments, where OSM has the entire highway. You can see in this screenshot of an MVUM highway on top of an OSM basemap. The MVUM highway is missing the middle segment.</p> <p></p> <p>The conflation software sucessfully merges the tags from the external dataset to the complete OSM highway feature.</p> <p>Currently any features that are an exact match between the external dataset and OSM are not in the conflated output to reduce the data that needs to be validated. If you have OSM loaded into a layer in JOSM, the matched segments will be black lines.</p>"},{"location":"validating/#reference-numbers-dont-match","title":"Reference Numbers Don't Match","text":"<p>If you are using the USGS topographical basemap, you can't depend on it for the official reference number. Often the reference numbers in the basemap are truncated, so you may think there is a problem. The reference number in the MVUM dataset is the correct one.</p> <p></p> <p>The other issue with reference numbers is also related to them being truncated. Older versions of the external datasets are often missing the .1 suffix. All the newer versions of the MVUM dataset and the USGS topographical maps do have the full reference number with the .1 appended. Depending on when the data was imported into OSM, it may be lacking the .1 suffix.</p>"},{"location":"validating/#proper-abbreviation","title":"Proper Abbreviation","text":"<p>The community accepted abbreviation for Forest Service Road is FR. The MVUM dataset of course lacks this abbreviation, but it's added when converting the MVUM dataset to OSM. In OSM there is a wide variety of variations on this, some examples are usfs *, *usfsr, FD, FS, etc... The conversion process also looks for these and makes them consistent with OSM by using only FR. If they are converteds, the existing value in OSM is renamed to old_ref, so when validating the conflated data, you'll see both, and the old value can be deleted.</p>"},{"location":"validating/#proper-tag","title":"Proper Tag","text":"<p>One of the wonderful things about the flexibility of thw data schema to support multiple values for the same feature. An MVUM highway will of course have a reference number, but it may also have a county reference number.</p> <p>Based on a long discussion on the OSM Tagging email list a few years ago, the consensus was to use ref for the county reference number, and ref:usfs for the forest service number. The OSM carto will display anything with a ref* tag. The conflation process also looks for a forest service reference number under a ref_ tag, and changes it to use ref:usfs. Both versions are displayed in the conflated data, you can delete the older one under ref.</p>"},{"location":"validating/#geometry-matching","title":"Geometry Matching","text":"<p>If the distance, the slope, and the angle are all zero, that's an exact match of geometries. Usually these highways segments were imported from the same external dataset I'm conflating, so identical other than lacking metadata. These an easy to validate since we have high confidence the external feature matches the OSM feature.</p> <p></p> <p>Sometimes though you get a similar geometry, but they are parallel. This happens for highways that were traced off of satelloite imagery as the offsets vary. You can see in this screenshot that the conflation software sucessfully made the match, and the merged the tags into the OSM feature, which currently is only highway=track. At some point it'd be good to go back through and fix the geometry, but for now we're just focused on improving the tags.</p> <p>When highways are traced from satellite imagery, sometimes they don't match the geometry in the external dataset. While too much differences in geometry can lead to false positives, we don't want to only identify an exact match. There are steering paramaters on how much difference is acceptable.</p> <p></p>"},{"location":"validating/#splitting-forks","title":"Splitting Forks","text":"<p>While this project is focused on conflating metadata, and not making geometry changes, sometimes when validating the conflation results I come across problems. Usually in OSM these are just tagged with highway=track, and were obviously traced from satellite imagery. Unfortunately without another dataset to reference, sometimes the highway continues on the wrong branch of the fork. This obviously gets flagged by the conflation process, but needs to be fixed manually.</p> <p></p> <p>I try to fix these as I come across them as I'm validating the conflation results. Since conflation is not fast, I have time between conflation runs as I improve the algorythm. Fixing these takes a few minutes per feature, and you have to be careful you don't break navigation. I work on these in JOSM while waiting for other tasks to finish. Since I can regenerate all the OSM data extracts for conflation, this will improve the results in the future. If I add the tags too, when conflating later it'll be a perfect match, so won't be in the results.</p> <p>To fix this I select the node at the fork, and split the highway that is wrong. Then I have to manually cut &amp; paste the tags to the correct branches of the fork. Sometimes I'll combine the ways for the newly fixed highways, but that is optional.</p>"},{"location":"wiki_redirect/","title":"OSM Merge","text":"<p>Please see the docs page at: https://osm-merge.github.io/osm-merge/</p>"},{"location":"zion/","title":"Analyzing Zion National Park Trails","text":"<p>As an aid to debugging my conflation software, I decided to use Zion National Park trail data. This involved two external datasets, USGS vector topographical maps and the National Park Service trails dataset.The Topographical maps are in ShapeFile format, the NPS trails is in GeoJson.</p> <p>The topographical dataset has many more attributes than the NPS dataset. For example, the topo dataset contains access information, which is one of the goals of the Trail Access Project. One of the details I noticed was having a value of designated instead of yes if the trail is in an official source. There are multiple access types, horse, bicycles, etc... having them be no might be useless data as it could be assumed if the access is allowed.</p> <pre><code>\"properties\": {\n    \"highway\": \"path\",\n    \"source\": \"National Park Service\",\n    \"bicycle\": \"no\",\n    \"atv\": \"no\",\n    \"horse\": \"designated\",\n    \"motorcycle\": \"no\",\n    \"snowmobile\": \"no\"\n    },\n</code></pre>"},{"location":"zion/#conflating-with-openstreetmap","title":"Conflating with OpenStreetMap","text":"<p>One big difference is that the OpenStreetMap dataset has many more features tagged with highway than the other datasets. OSM has mucn more detail, campground loop roads, service roads, </p> <p>Topo Trails  Coalpits Wash Trail (official) Dalton Wash Trail (BLM ?) Huber Wash Trail (not sure) Left Fork North Creek Trail aka Subway (official)</p> <p>The Subway (Bottom) in Topo and Left Fork North Creek Trail in OSM</p> <p>Pa'rus Trail is same in topo and nps, not in OSM.</p> <p>Deertrap Mountain Trail, or Cable Mountain.</p> <p>nps:COMMENT=062904-GPSed for cultural projects coverage nps:EDIT_DATE=082004 nps:ED_COMMENT=063004-removed spikes from arc nps:MILES=0.182262</p>"},{"location":"api/basemapper/","title":"basemapper.py","text":"<p>Thread to handle downloads for Queue.</p> <p>Parameters:</p> Name Type Description Default <code>dest</code> <code>str</code> <p>The filespec of the tile cache.</p> required <code>mirrors</code> <code>list</code> <p>The list of mirrors to get imagery.</p> required <code>tiles</code> <code>list</code> <p>The list of tiles to download.</p> required Source code in <code>osm_merge/fieldwork/basemapper.py</code> <pre><code>def dlthread(dest: str, mirrors: list[dict], tiles: list[tuple]) -&gt; None:\n    \"\"\"Thread to handle downloads for Queue.\n\n    Args:\n        dest (str): The filespec of the tile cache.\n        mirrors (list): The list of mirrors to get imagery.\n        tiles (list): The list of tiles to download.\n    \"\"\"\n    if len(tiles) == 0:\n        # epdb.st()\n        return\n\n    # Create the subdirectories as pySmartDL doesn't do it for us\n    Path(dest).mkdir(parents=True, exist_ok=True)\n\n    log.info(f\"Downloading {len(tiles)} tiles in thread {threading.get_ident()} to {dest}\")\n\n    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n        futures = [executor.submit(download_tile, dest, tile, mirrors) for tile in tiles]\n        concurrent.futures.wait(futures)\n</code></pre> <p>options: show_source: false heading_level: 3</p> <p>               Bases: <code>object</code></p> <p>Basemapper parent class.</p> <p>Parameters:</p> Name Type Description Default <code>boundary</code> <code>Union[str, BytesIO]</code> <p>A BBOX string or GeoJSON provided as BytesIO object of the AOI. The GeoJSON can contain multiple geometries.</p> required <code>base</code> <code>str</code> <p>The base directory to cache map tile in</p> required <code>source</code> <code>str</code> <p>The upstream data source for map tiles</p> required <p>Returns:</p> Type Description <code>BaseMapper</code> <p>An instance of this class</p> Source code in <code>osm_merge/fieldwork/basemapper.py</code> <pre><code>def __init__(\n    self,\n    boundary: Union[str, BytesIO],\n    base: str,\n    source: str,\n):\n    \"\"\"Create an tile basemap for ODK Collect.\n\n    Args:\n        boundary (Union[str, BytesIO]): A BBOX string or GeoJSON provided as BytesIO object of the AOI.\n            The GeoJSON can contain multiple geometries.\n        base (str): The base directory to cache map tile in\n        source (str): The upstream data source for map tiles\n\n    Returns:\n        (BaseMapper): An instance of this class\n    \"\"\"\n    bbox_factory = BoundaryHandlerFactory(boundary)\n    self.bbox = bbox_factory.get_bounding_box()\n    self.tiles = list()\n    self.base = base\n    # sources for imagery\n    self.source = source\n    self.sources = dict()\n\n    path = rootdir + \"/fieldwork/imagery.yaml\"\n    self.yaml = YamlFile(path)\n\n    for entry in self.yaml.yaml[\"sources\"]:\n        for k, v in entry.items():\n            src = dict()\n            for item in v:\n                src[\"source\"] = k\n                for k1, v1 in item.items():\n                    # print(f\"\\tFIXME2: {k1} - {v1}\")\n                    src[k1] = v1\n            self.sources[k] = src\n</code></pre> <p>options: show_source: false heading_level: 3</p> <p>Create a basemap with given parameters.</p> <p>Parameters:</p> Name Type Description Default <code>boundary</code> <code>str | BytesIO</code> <p>The boundary for the area you want.</p> <code>None</code> <code>tms</code> <code>str</code> <p>Custom TMS URL.</p> <code>None</code> <code>xy</code> <code>bool</code> <p>Swap the X &amp; Y coordinates when using a custom TMS if True.</p> <code>False</code> <code>outfile</code> <code>str</code> <p>Output file name for the basemap.</p> <code>None</code> <code>zooms</code> <code>str</code> <p>The Zoom levels, specified as a range (e.g., \"12-17\") or comma-separated levels (e.g., \"12,13,14\").</p> <code>'12-17'</code> <code>outdir</code> <code>str</code> <p>Output directory name for tile cache.</p> <code>None</code> <code>source</code> <code>str</code> <p>Imagery source, one of [\"esri\", \"bing\", \"topo\", \"google\", \"oam\", \"custom\"] (default is \"esri\").</p> <code>'esri'</code> <code>append</code> <code>bool</code> <p>Whether to append to an existing file</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>osm_merge/fieldwork/basemapper.py</code> <pre><code>def create_basemap_file(\n    boundary=None,\n    tms=None,\n    xy=False,\n    outfile=None,\n    zooms=\"12-17\",\n    outdir=None,\n    source=\"esri\",\n    append: bool = False,\n) -&gt; None:\n    \"\"\"Create a basemap with given parameters.\n\n    Args:\n        boundary (str | BytesIO, optional): The boundary for the area you want.\n        tms (str, optional): Custom TMS URL.\n        xy (bool, optional): Swap the X &amp; Y coordinates when using a\n            custom TMS if True.\n        outfile (str, optional): Output file name for the basemap.\n        zooms (str, optional): The Zoom levels, specified as a range\n            (e.g., \"12-17\") or comma-separated levels (e.g., \"12,13,14\").\n        outdir (str, optional): Output directory name for tile cache.\n        source (str, optional): Imagery source, one of\n            [\"esri\", \"bing\", \"topo\", \"google\", \"oam\", \"custom\"] (default is \"esri\").\n        append (bool, optional): Whether to append to an existing file\n\n    Returns:\n        None\n    \"\"\"\n    log.debug(\n        \"Creating basemap with params: \"\n        f\"boundary={boundary} | \"\n        f\"outfile={outfile} | \"\n        f\"zooms={zooms} | \"\n        f\"outdir={outdir} | \"\n        f\"source={source} | \"\n        f\"tms={tms}\"\n    )\n\n    # Validation\n    if not boundary:\n        err = \"You need to specify a boundary! (in-memory object or bbox)\"\n        log.error(err)\n        raise ValueError(err)\n\n    # Get all the zoom levels we want\n    zoom_levels = list()\n    if zooms:\n        if zooms.find(\"-\") &gt; 0:\n            start = int(zooms.split(\"-\")[0])\n            end = int(zooms.split(\"-\")[1]) + 1\n            x = range(start, end)\n            for i in x:\n                zoom_levels.append(i)\n        elif zooms.find(\",\") &gt; 0:\n            levels = zooms.split(\",\")\n            for level in levels:\n                zoom_levels.append(int(level))\n        else:\n            zoom_levels.append(int(zooms))\n\n    if not outdir:\n        base = Path.cwd().absolute()\n    else:\n        base = Path(outdir).absolute()\n\n    # Source / TMS validation\n    if not source and not tms:\n        err = \"You need to specify a source!\"\n        log.error(err)\n        raise ValueError(err)\n    if source == \"oam\" and not tms:\n        err = \"A TMS URL must be provided for OpenAerialMap!\"\n        log.error(err)\n        raise ValueError(err)\n    # A custom TMS provider\n    if source != \"oam\" and tms:\n        source = \"custom\"\n\n    tiledir = base / f\"{source}tiles\"\n    # Make tile download directory\n    tiledir.mkdir(parents=True, exist_ok=True)\n    # Convert to string for other methods\n    tiledir = str(tiledir)\n\n    basemap = BaseMapper(boundary, tiledir, source)\n\n    if tms:\n        # Add TMS URL to sources for download\n        basemap.customTMS(tms, True if source == \"oam\" else False, xy)\n\n    # Args parsed, main code:\n    tiles = list()\n    for zoom_level in zoom_levels:\n        # Download the tile directory\n        basemap.getTiles(zoom_level)\n        tiles += basemap.tiles\n\n    if not outfile:\n        log.info(f\"No outfile specified, tile download finished: {tiledir}\")\n        return\n\n    suffix = Path(outfile).suffix.lower()\n    image_format = basemap.sources[source].get(\"suffix\", \"jpg\")\n    log.debug(f\"Basemap output format: {suffix} | Image format: {image_format}\")\n\n    if any(substring in suffix for substring in [\"sqlite\", \"mbtiles\"]):\n        outf = DataFile(outfile, basemap.getFormat(), append)\n        if suffix == \".mbtiles\":\n            outf.addBounds(basemap.bbox)\n            outf.addZoomLevels(zoom_levels)\n        # Create output database and specify image format, png, jpg, or tif\n        outf.writeTiles(tiles, tiledir, image_format)\n\n    elif suffix == \".pmtiles\":\n        tile_dir_to_pmtiles(outfile, tiledir, basemap.bbox, image_format, zoom_levels, source)\n\n    else:\n        msg = f\"Format {suffix} not supported\"\n        log.error(msg)\n        raise ValueError(msg) from None\n    log.info(f\"Wrote {outfile}\")\n</code></pre> <p>options: show_source: false heading_level: 3</p> <p>Helper function to get the tile id from a tile in xyz (zyx) directory structure.</p> <p>TMS typically has structure z/y/x.png If the --xy flag was used previously, the TMS was downloaed into directories of z/y/x structure from their z/x/y URL.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>Union[Path, str]</code> <p>The path to tile image within the xyz directory.</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The globally defined tile id from the xyz definition.</p> Source code in <code>osm_merge/fieldwork/basemapper.py</code> <pre><code>def tileid_from_zyx_dir_path(filepath: Union[Path, str]) -&gt; int:\n    \"\"\"Helper function to get the tile id from a tile in xyz (zyx) directory structure.\n\n    TMS typically has structure z/y/x.png\n    If the --xy flag was used previously, the TMS was downloaed into\n    directories of z/y/x structure from their z/x/y URL.\n\n    Args:\n        filepath (Union[Path, str]): The path to tile image within the xyz directory.\n\n    Returns:\n        int: The globally defined tile id from the xyz definition.\n    \"\"\"\n    # Extract the final 3 parts from the TMS file path\n    tile_image_path = Path(filepath).parts[-3:]\n\n    try:\n        final_tile = int(Path(tile_image_path[-1]).stem)\n    except ValueError as e:\n        msg = f\"Invalid tile path (cannot parse as int): {str(tile_image_path)}\"\n        log.error(msg)\n        raise ValueError(msg) from e\n\n    x = final_tile\n    z, y = map(int, tile_image_path[:-1])\n\n    return zxy_to_tileid(z, x, y)\n</code></pre> <p>options: show_source: false heading_level: 3</p>"},{"location":"api/basemapper/#osm_merge.fieldwork.basemapper.BaseMapper.customTMS","title":"customTMS","text":"<pre><code>customTMS(url, is_oam=False, is_xy=False)\n</code></pre> <p>Add a custom TMS URL to the list of sources.</p> <p>The url must end in %s to be replaced with the tile xyz values.</p> <p>Format examples: https://basemap.nationalmap.gov/ArcGIS/rest/services/USGSTopo/MapServer/tile/{z}/{y}/{x} https://maps.nyc.gov/xyz/1.0.0/carto/basemap/%s https://maps.nyc.gov/xyz/1.0.0/carto/basemap/{z}/{x}/{y}.jpg</p> <p>The method will replace {z}/{x}/{y}.jpg with %s</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL string</p> required <code>is_oam</code> <code>bool</code> <p>Is a URL for OAM</p> <code>False</code> <code>is_xy</code> <code>bool</code> <p>Swap the x and y for the provider --&gt; 'zxy'</p> <code>False</code> Source code in <code>osm_merge/fieldwork/basemapper.py</code> <pre><code>def customTMS(self,\n              url: str,\n              is_oam: bool = False,\n              is_xy: bool = False):\n    \"\"\"Add a custom TMS URL to the list of sources.\n\n    The url must end in %s to be replaced with the tile xyz values.\n\n    Format examples:\n    https://basemap.nationalmap.gov/ArcGIS/rest/services/USGSTopo/MapServer/tile/{z}/{y}/{x}\n    https://maps.nyc.gov/xyz/1.0.0/carto/basemap/%s\n    https://maps.nyc.gov/xyz/1.0.0/carto/basemap/{z}/{x}/{y}.jpg\n\n    The method will replace {z}/{x}/{y}.jpg with %s\n\n    Args:\n        url (str): The URL string\n        is_oam (bool): Is a URL for OAM\n        is_xy (bool): Swap the x and y for the provider --&gt; 'zxy'\n    \"\"\"\n    # Remove any file extensions if present and update the 'suffix' parameter\n    # NOTE the file extension gets added again later for the download URL\n    if url.endswith(\".jpg\"):\n        suffix = \"jpg\"\n        url = url[:-4]  # Remove the last 4 characters (\".jpg\")\n    elif url.endswith(\".png\"):\n        suffix = \"png\"\n        url = url[:-4]  # Remove the last 4 characters (\".png\")\n    else:\n        # FIXME handle other formats for custom TMS\n        suffix = \"jpg\"\n\n    # If placeholders present, validate they have no additional spaces\n    if \"{\" in url and \"}\" in url:\n        pattern = r\".*/\\{[zxy]\\}/\\{[zxy]\\}/\\{[zxy]\\}(?:/|/?)\"\n        if not bool(re.search(pattern, url)):\n            msg = \"Invalid TMS URL format. Please check the URL placeholders {z}/{x}/{y}.\"\n            log.error(msg)\n            raise ValueError(msg)\n\n    # Remove \"{z}/{x}/{y}\" placeholders if they are present\n    url = re.sub(r\"/{[xyz]+\\}\", \"\", url)\n    # Append \"%s\" to the end of the URL to later add the tile path\n    url = url + r\"/%s\"\n\n    if is_oam:\n        # Override dummy OAM URL\n        source = \"oam\"\n        self.sources[source][\"url\"] = url\n    else:\n        source = \"custom\"\n        tms_params = {\"name\": source, \"url\": url, \"suffix\": suffix, \"source\": source, \"xy\": is_xy}\n        log.debug(f\"Setting custom TMS with params: {tms_params}\")\n        self.sources[source] = tms_params\n\n    # Select the source\n    self.source = source\n</code></pre>"},{"location":"api/basemapper/#osm_merge.fieldwork.basemapper.BaseMapper.getFormat","title":"getFormat","text":"<pre><code>getFormat()\n</code></pre> <p>Get the image format of the map tiles.</p> <p>Returns:</p> Type Description <code>str</code> <p>the upstream source for map tiles.</p> Source code in <code>osm_merge/fieldwork/basemapper.py</code> <pre><code>def getFormat(self):\n    \"\"\"Get the image format of the map tiles.\n\n    Returns:\n        (str): the upstream source for map tiles.\n    \"\"\"\n    return self.sources[self.source][\"suffix\"]\n</code></pre>"},{"location":"api/basemapper/#osm_merge.fieldwork.basemapper.BaseMapper.getTiles","title":"getTiles","text":"<pre><code>getTiles(zoom)\n</code></pre> <p>Get a list of tiles for the specified zoom level.</p> <p>Parameters:</p> Name Type Description Default <code>zoom</code> <code>int</code> <p>The Zoom level of the desired map tiles.</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The total number of map tiles downloaded.</p> Source code in <code>osm_merge/fieldwork/basemapper.py</code> <pre><code>def getTiles(self, zoom: int) -&gt; int:\n    \"\"\"Get a list of tiles for the specified zoom level.\n\n    Args:\n        zoom (int): The Zoom level of the desired map tiles.\n\n    Returns:\n        int: The total number of map tiles downloaded.\n    \"\"\"\n    info = get_cpu_info()\n    cores = info[\"count\"]\n\n    self.tiles = list(mercantile.tiles(self.bbox[0], self.bbox[1], self.bbox[2], self.bbox[3], zoom))\n    total = len(self.tiles)\n    log.info(f\"{total} tiles for zoom level {zoom}\")\n\n    mirrors = [self.sources[self.source]]\n    chunk_size = max(1, round(total / cores))\n\n    if total &lt; chunk_size or chunk_size == 0:\n        dlthread(self.base, mirrors, self.tiles)\n    else:\n        with concurrent.futures.ThreadPoolExecutor(max_workers=cores) as executor:\n            futures = []\n            for i in range(0, total, chunk_size):\n                chunk = self.tiles[i : i + chunk_size]\n                futures.append(executor.submit(dlthread, self.base, mirrors, chunk))\n                log.debug(f\"Dispatching Block {i}:{i + chunk_size}\")\n            concurrent.futures.wait(futures)\n\n    return total\n</code></pre>"},{"location":"api/basemapper/#osm_merge.fieldwork.basemapper.BaseMapper.tileExists","title":"tileExists","text":"<pre><code>tileExists(tile)\n</code></pre> <p>See if a map tile already exists.</p> <p>Parameters:</p> Name Type Description Default <code>tile</code> <code>MapTile</code> <p>The map tile to check for the existence of</p> required <p>Returns:</p> Type Description <code>bool</code> <p>Whether the tile exists in the map tile cache</p> Source code in <code>osm_merge/fieldwork/basemapper.py</code> <pre><code>def tileExists(\n    self,\n    tile: MapTile,\n):\n    \"\"\"See if a map tile already exists.\n\n    Args:\n        tile (MapTile): The map tile to check for the existence of\n\n    Returns:\n        (bool): Whether the tile exists in the map tile cache\n    \"\"\"\n    filespec = f\"{self.base}{tile[2]}/{tile[1]}/{tile[0]}.{self.sources[{self.source}]['suffix']}\"\n    if Path(filespec).exists():\n        log.debug(\"%s exists\" % filespec)\n        return True\n    else:\n        log.debug(\"%s doesn't exists\" % filespec)\n        return False\n</code></pre>"},{"location":"api/conflator/","title":"Conflator","text":"<p>               Bases: <code>object</code></p> <p>Parameters:</p> Name Type Description Default <code>uri</code> <code>str</code> <p>URI for the primary database</p> <code>None</code> <code>boundary</code> <code>str</code> <p>Boundary to limit SQL queries</p> <code>None</code> <p>Returns:</p> Type Description <code>Conflator</code> <p>An instance of this object</p> Source code in <code>osm_merge/conflator.py</code> <pre><code>def __init__(self,\n             uri: str = None,\n             boundary: str = None\n             ):\n    \"\"\"\n    Initialize Input data sources.\n\n    Args:\n        uri (str): URI for the primary database\n        boundary (str, optional): Boundary to limit SQL queries\n\n    Returns:\n        (Conflator): An instance of this object\n    \"\"\"\n    self.postgres = list()\n    self.tags = dict()\n    self.boundary = boundary\n    self.dburi = uri\n    self.primary = None\n    if boundary:\n        infile = open(boundary, 'r')\n        self.boundary = geojson.load(infile)\n        infile.close()\n    # Distance in meters for conflating with postgis\n    self.tolerance = 7\n    self.data = dict()\n    self.analyze = (\"building\", \"name\", \"amenity\", \"landuse\", \"cuisine\", \"tourism\", \"leisure\")\n</code></pre> <p>options: show_source: false heading_level: 3</p>"},{"location":"api/conflator/#osm_merge.conflator.Conflator.getDistance","title":"getDistance","text":"<pre><code>getDistance(newdata, olddata)\n</code></pre> <p>Compute the distance between two features in meters</p> <p>Parameters:</p> Name Type Description Default <code>newdata</code> <code>Feature</code> <p>A feature from the external dataset</p> required <code>olddata</code> <code>Feature</code> <p>A feature from the existing OSM dataset</p> required <p>Returns:</p> Type Description <code>float</code> <p>The distance between the two features</p> Source code in <code>osm_merge/conflator.py</code> <pre><code>def getDistance(self,\n        newdata: Feature,\n        olddata: Feature,\n        ) -&gt; float:\n    \"\"\"\n    Compute the distance between two features in meters\n\n    Args:\n        newdata (Feature): A feature from the external dataset\n        olddata (Feature): A feature from the existing OSM dataset\n\n    Returns:\n        (float): The distance between the two features\n    \"\"\"\n    # timer = Timer(text=\"getDistance() took {seconds:.0f}s\")\n    # timer.start()\n    # dist = shapely.hausdorff_distance(center, wkt)\n    dist = float()\n\n    # Transform so the results are in meters instead of degress of the\n    # earth's radius.\n    project = pyproj.Transformer.from_proj(\n        pyproj.Proj(init='epsg:4326'),\n        pyproj.Proj(init='epsg:3857')\n        )\n    newobj = transform(project.transform, shape(newdata[\"geometry\"]))\n    oldobj = transform(project.transform, shape(olddata[\"geometry\"]))\n\n    # FIXME: we shouldn't ever get here...\n    if oldobj.type == \"MultiLineString\":\n        log.error(f\"MultiLineString unsupported!\")\n        # FIXME: this returns a MultiLineString, so nee to track down why\n        newline = linemerge(oldobj)\n\n    if newobj.type == \"MultiLineString\":\n        lines = newobj.geoms\n    elif newobj.type == \"GeometryCollection\":\n        lines = newobj.geoms\n    else:\n        lines = MultiLineString([newobj]).geoms\n\n    # dists = list()\n    best = None\n    diff = newobj.length - oldobj.length\n    if diff &gt; 10000:\n        log.error(f\"Large difference in highway lengths! {abs(diff)}\")\n        return 0.0\n    for segment in lines:\n        if oldobj.geom_type == \"LineString\" and segment.geom_type == \"LineString\":\n            # Compare two highways\n            # if oldobj.within(segment):\n            #    log.debug(f\"CONTAINS\")\n            dist = segment.distance(oldobj)\n        elif oldobj.geom_type == \"Point\" and segment.geom_type == \"LineString\":\n            # We only want to compare LineStrings, so force the distance check\n            # to be False\n            log.error(f\"Unimplemented\")\n            dist = 12345678.9\n        elif oldobj.geom_type == \"Point\" and segment.geom_type == \"Point\":\n            dist = segment.distance(oldobj)\n        elif oldobj.geom_type == \"Polygon\" and segment.geom_type == \"Polygon\":\n            log.error(f\"Unimplemented\")\n            # compare two buildings\n            pass\n        elif oldobj.geom_type == \"Polygon\" and segment.geom_type == \"Point\":\n            # Compare a point with a building, used for ODK Collect data\n            center = shapely.centroid(oldobj)\n            dist = segment.distance(center)\n        elif oldobj.geom_type == \"Point\" and segment.geom_type == \"LineString\":\n            dist = segment.distance(oldobj)\n        elif oldobj.geom_type == \"LineString\" and segment.geom_type == \"Point\":\n            dist = segment.distance(oldobj)\n\n        # Find the closest segment\n        if best is None:\n            best = dist\n        elif dist &lt; best:\n            # log.debug(f\"BEST: {best} &lt; {dist}\")\n            best = dist\n\n    # timer.stop()\n    return best # dist # best\n</code></pre>"},{"location":"api/conflator/#osm_merge.conflator.Conflator.checkTags","title":"checkTags","text":"<pre><code>checkTags(extfeat, osm)\n</code></pre> <p>Check tags between 2 features.</p> <p>Parameters:</p> Name Type Description Default <code>extfeat</code> <code>Feature</code> <p>The feature from the external dataset</p> required <code>osm</code> <code>Feature</code> <p>The result</p> required <p>Returns:</p> Type Description <code>int</code> <p>The number of tag matches</p> <code>dict</code> <p>The updated tags</p> Source code in <code>osm_merge/conflator.py</code> <pre><code>def checkTags(self,\n              extfeat: Feature,\n              osm: Feature,\n               ):\n    \"\"\"\n    Check tags between 2 features.\n\n    Args:\n        extfeat (Feature): The feature from the external dataset\n        osm (Feature): The result\n\n    Returns:\n        (int): The number of tag matches\n        (dict): The updated tags\n    \"\"\"\n    match_threshold = 80\n    match = [\"name\", \"ref\", \"ref:usfs\"]\n    # skip = [\"UT\", \"CR\", \"WY\", \"CO\"]\n    hits = 0\n    props = dict()\n    id = 0\n    version = 0\n    props = extfeat['properties'] | osm['properties']\n    props[\"name_ratio\"] = 0\n    props[\"ref_ratio\"] = 0\n    # ODK Collect adds these two tags we don't need\n    if \"title\" in props:\n        del props[\"title\"]\n    if \"label\" in props:\n        del props[\"label\"]\n\n    if \"id\" in props:\n        # External data not from an OSM source always has\n        # negative IDs to distinguish it from current OSM data.\n        id = int(props[\"id\"])\n    else:\n        id -= 1\n        props[\"id\"] = id\n\n    if \"version\" in props:\n        # Always use the OSM version if it exists, since it gets\n        # incremented so JOSM see it's been modified.\n        props[\"version\"] = int(version)\n        # Name may also be name:en, name:np, etc... There may also be\n        # multiple name:* values in the tags.\n    else:\n        props[\"version\"] = 1\n\n    for key in match:\n        if \"highway\" in osm[\"properties\"]:\n            # Always use the value in the secondary, which is\n            # likely OSM.\n            props[\"highway\"] = osm[\"properties\"][\"highway\"]\n        if key not in props:\n            continue\n\n        # Usually it's the name field that has the most variety in\n        # in trying to match strings. This often is differences in\n        # capitalization, singular vs plural, and typos from using\n        # your phone to enter the name. Course names also change\n        # too so if it isn't a match, use the new name from the\n        # external dataset.\n        if key in osm[\"properties\"] and key in extfeat[\"properties\"]:\n            # Sometimes there will be a word match, which returns a\n            # ratio in the low 80s. In that case they should be\n            # a similar length.\n            length = len(extfeat[\"properties\"][key]) - len(osm[\"properties\"][key])\n            ratio = fuzz.ratio(extfeat[\"properties\"][key].lower(), osm[\"properties\"][key].lower())\n            print(f\"\\tChecking ({key}:{ratio}): \\'{extfeat[\"properties\"][key].lower()}\\', \\'{osm[\"properties\"][key].lower()}\\'\")\n            if key == \"name\":\n                props[\"name_ratio\"] = ratio\n            else:\n                props[\"ref_ratio\"] = ratio\n            if ratio &gt; match_threshold and length &lt;= 3:\n                hits += 1\n                props[key] = extfeat[\"properties\"][key]\n                if ratio != 100:\n                    # Often the only difference is using FR or FS as the\n                    # prefix. In that case, see if the ref matches.\n                    if key[:3] == \"ref\":\n                        # This assume all the data has been converted\n                        # by one of the utility programs, which enfore\n                        # using the ref:usfs tag.\n                        tmp = extfeat[\"properties\"][\"ref:usfs\"].split(' ')\n                        exttype = tmp[0].upper()\n                        extref = tmp[1].upper()\n                        tmp = osm[\"properties\"][\"ref:usfs\"].split(' ')\n                        newtype = tmp[0]\n                        newref = tmp[1].upper()\n                        print(f\"\\tREFS: {newtype} - {extref} vs {newref}: {extref == newref}\")\n                        if key == \"ref\" and (newtype != \"FS\" or newtype != \"FR\"):\n                            props[key] = newtype\n                            continue\n                        if extref == newref:\n                            hits += 1 \n                            # Many minor changes of FS to FR don't require\n                            # caching the exising value as it's only the\n                            # prefix that changed. It always stays in this\n                            # range.\n                            if osm[\"properties\"][\"ref:usfs\"][:3] == \"FS \" and ratio &gt; 80 and ratio &lt; 90:\n                                # log.debug(f\"Ignoring old ref {osm[\"properties\"][\"ref:usfs\"]}\")\n                                continue\n                    # For a fuzzy match, cache the value from the\n                    # secondary dataset and use the value in the\n                    # primary dataset.\n                if ratio != 100:\n                    props[f\"old_{key}\"] = osm[\"properties\"][key]\n\n    # print(props)\n    return hits, props\n</code></pre>"},{"location":"api/conflator/#osm_merge.conflator.Conflator.conflateData","title":"conflateData","text":"<pre><code>conflateData(\n    primaryspec,\n    secondaryspec,\n    threshold=10.0,\n    informal=False,\n)\n</code></pre> <p>Open the two source files and contlate them.</p> <p>Parameters:</p> Name Type Description Default <code>primaryspec</code> <code>str</code> <p>The primary dataset filespec</p> required <code>secondaryspec</code> <code>str</code> <p>The secondary dataset filespec</p> required <code>threshold</code> <code>float</code> <p>Threshold for distance calculations in meters</p> <code>10.0</code> <code>informal</code> <code>bool</code> <p>Whether to dump features in OSM not in external data</p> <code>False</code> <p>Returns:</p> Type Description <code>list</code> <p>The conflated output</p> Source code in <code>osm_merge/conflator.py</code> <pre><code>def conflateData(self,\n                primaryspec: str,\n                secondaryspec: str,\n                threshold: float = 10.0,\n                informal: bool = False,\n                ) -&gt; list:\n    \"\"\"\n    Open the two source files and contlate them.\n\n    Args:\n        primaryspec (str): The primary dataset filespec\n        secondaryspec (str): The secondary dataset filespec\n        threshold (float): Threshold for distance calculations in meters\n        informal (bool): Whether to dump features in OSM not in external data\n\n    Returns:\n        (list):  The conflated output\n    \"\"\"\n    timer = Timer(text=\"conflateData() took {seconds:.0f}s\")\n    timer.start()\n    odkdata = list()\n    osmdata = list()\n\n    result = list()\n    # if odkspec[:3].lower() == \"pg:\":\n    #     db = GeoSupport(odkspec[3:])\n    #     result = await db.queryDB()\n    # else:\n    primarydata = self.parseFile(primaryspec)\n\n    # if osmspec[:3].lower() == \"pg:\":\n    #     db = GeoSupport(osmspec[3:])\n    #     result = await db.queryDB()\n    # else:\n    secondarydata = self.parseFile(secondaryspec)\n\n    entries = len(primarydata)\n    chunk = round(entries / cores)\n\n    alldata = list()\n    newdata = list()\n    tasks = list()\n\n    # log.info(f\"The primary dataset has {len(primarydata)} entries\")\n    # log.info(f\"The secondary dataset has {len(secondarydata)} entries\")\n    print(f\"The primary dataset has {len(primarydata)} entries\")\n    print(f\"The secondary dataset has {len(secondarydata)} entries\")\n\n    # Make threading optional for easier debugging\n    if chunk == 0:\n        single = True\n    else:\n        single = False\n\n    # single = True          # FIXME: debug\n    if single:\n        alldata = conflateThread(primarydata, secondarydata)\n    else:\n        futures = list()\n        with concurrent.futures.ProcessPoolExecutor(max_workers=cores) as executor:\n            for block in range(0, entries, chunk):\n                data = list()\n                future = executor.submit(conflateThread,\n                        primarydata[block:block + chunk - 1],\n                        secondarydata,\n                        informal\n                        )\n                futures.append(future)\n            #for thread in concurrent.futures.wait(futures, return_when='ALL_COMPLETED'):\n            for future in concurrent.futures.as_completed(futures):\n                res = future.result()\n                # log.debug(f\"Waiting for thread to complete..,\")\n                data.extend(res[0])\n                newdata.extend(res[1])\n            alldata = [data, newdata]\n\n        executor.shutdown()\n\n    timer.stop()\n\n    return alldata\n</code></pre>"},{"location":"api/conflator/#osm_merge.conflator.Conflator.dump","title":"dump","text":"<pre><code>dump()\n</code></pre> <p>Dump internal data for debugging.</p> Source code in <code>osm_merge/conflator.py</code> <pre><code>def dump(self):\n    \"\"\"\n    Dump internal data for debugging.\n    \"\"\"\n    print(f\"Data source is: {self.dburi}\")\n    print(f\"There are {len(self.data)} existing features\")\n</code></pre>"},{"location":"api/conflator/#osm_merge.conflator.Conflator.parseFile","title":"parseFile","text":"<pre><code>parseFile(filespec)\n</code></pre> <p>Parse the input file based on it's format.</p> <p>Parameters:</p> Name Type Description Default <code>filespec</code> <code>str</code> <p>The file to parse</p> required <p>Returns:</p> Type Description <code>list</code> <p>The parsed data from the file</p> Source code in <code>osm_merge/conflator.py</code> <pre><code>def parseFile(self,\n            filespec: str,\n            ) -&gt;list:\n    \"\"\"\n    Parse the input file based on it's format.\n\n    Args:\n        filespec (str): The file to parse\n\n    Returns:\n        (list): The parsed data from the file\n    \"\"\"\n    path = Path(filespec)\n    data = list()\n    if path.suffix == '.geojson':\n        # FIXME: This should also work for any GeoJson file, not\n        # only  ones, but this has yet to be tested.\n        log.debug(f\"Parsing GeoJson files {path}\")\n        file = open(path, 'r')\n        features = geojson.load(file)\n        data = features['features']\n    elif path.suffix == '.osm':\n        log.debug(f\"Parsing OSM XML files {path}\")\n        osmfile = OsmFile()\n        data = osmfile.loadFile(path)\n    elif path.suffix == \".csv\":\n        log.debug(f\"Parsing csv files {path}\")\n        odk = ODKParsers()\n        for entry in odk.CSVparser(path):\n            data.append(odk.createEntry(entry))\n    elif path.suffix == \".json\":\n        log.debug(f\"Parsing json files {path}\")\n        odk  = ODKParsers()\n        for entry in odk.JSONparser(path):\n            data.append(odk.createEntry(entry))\n    return data\n</code></pre>"},{"location":"api/conflator/#osm_merge.conflator.Conflator.conflateDB","title":"conflateDB","text":"<pre><code>conflateDB(source)\n</code></pre> <p>Conflate all the data. This the primary interfacte for conflation.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>The source file to conflate</p> required <p>Returns:</p> Type Description <code>dict</code> <p>The conflated features</p> Source code in <code>osm_merge/conflator.py</code> <pre><code>def conflateDB(self,\n                 source: str,\n                 ) -&gt; dict:\n    \"\"\"\n    Conflate all the data. This the primary interfacte for conflation.\n\n    Args:\n        source (str): The source file to conflate\n\n    Returns:\n        (dict):  The conflated features\n    \"\"\"\n    timer = Timer(text=\"conflateData() took {seconds:.0f}s\")\n    timer.start()\n\n    log.info(\"Opening data file: %s\" % source)\n    toplevel = Path(source)\n    if toplevel.suffix == \".geosjon\":\n        src = open(source, \"r\")\n        self.data = geojson.load(src)\n    elif toplevel.suffix == \".osm\":\n        src = open(source, \"r\")\n        osmin = OsmFile()\n        self.data = osmin.loadFile(source) # input file\n        if self.boundary:\n            gs = GeoSupport(source)\n            # self.data = gs.clipFile(self.data)\n\n    # Use fuzzy string matching to handle minor issues in the name column,\n    # which is often used to match an amenity.\n    if len(self.data) == 0:\n        self.postgres[0].query(\"CREATE EXTENSION IF NOT EXISTS fuzzystrmatch\")\n    # log.debug(f\"OdkMerge::conflateData() called! {len(odkdata)} features\")\n\n    # A chunk is a group of threads\n    chunk = round(len(self.data) / cores)\n\n    # cycle = range(0, len(odkdata), chunk)\n\n    # Chop the data into a subset for each thread\n    newdata = list()\n    future = None\n    result = None\n    index = 0\n    if True:                # DEBUGGING HACK ALERT!\n        result = conflateThread(self.data, self, index)\n        return dict()\n\n    with concurrent.futures.ThreadPoolExecutor(max_workers=cores) as executor:\n        i = 0\n        subset = dict()\n        futures = list()\n        for key, value in self.data.items():\n            subset[key] = value\n            if i == chunk:\n                i = 0\n                result = executor.submit(conflateThread, subset, self, index)\n                index += 1\n                # result.add_done_callback(callback)\n                futures.append(result)\n                subset = dict()\n            i += 1\n        for future in concurrent.futures.as_completed(futures):\n        # # for future in concurrent.futures.wait(futures, return_when='ALL_COMPLETED'):\n            log.debug(f\"Waiting for thread to complete..\")\n            # print(f\"YYEESS!! {future.result(timeout=10)}\")\n            newdata.append(future.result(timeout=5))\n    timer.stop()\n    return newdata\n</code></pre>"},{"location":"api/conflator/#osm_merge.conflator.Conflator.writeGeoJson","title":"writeGeoJson","text":"<pre><code>writeGeoJson(data, filespec)\n</code></pre> <p>Write the data to a GeoJson file.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict</code> <p>The list of GeoJson features</p> required <code>filespec</code> <code>str</code> <p>The output file name</p> required Source code in <code>osm_merge/conflator.py</code> <pre><code>def writeGeoJson(self,\n             data: dict,\n             filespec: str,\n             ):\n    \"\"\"\n    Write the data to a GeoJson file.\n\n    Args:\n        data (dict): The list of GeoJson features\n        filespec (str): The output file name\n    \"\"\"\n    file = open(filespec, \"w\")\n    fc = FeatureCollection(data)\n    geojson.dump(fc, file, indent=4)\n</code></pre>"},{"location":"api/conflator/#osm_merge.conflator.Conflator.osmToFeature","title":"osmToFeature","text":"<pre><code>osmToFeature(osm)\n</code></pre> <p>Convert an entry from an OSM XML file with attrs and tags into a GeoJson Feature.</p> <p>Parameters:</p> Name Type Description Default <code>osm</code> <code>dict</code> <p>The OSM entry</p> required <p>Returns:</p> Type Description <code>Feature</code> <p>A GeoJson feature</p> Source code in <code>osm_merge/conflator.py</code> <pre><code>def osmToFeature(self,\n                 osm: dict(),\n                 ) -&gt; Feature:\n    \"\"\"\n    Convert an entry from an OSM XML file with attrs and tags into\n    a GeoJson Feature.\n\n    Args:\n        osm (dict): The OSM entry\n\n    Returns:\n        (Feature): A GeoJson feature\n    \"\"\"\n    if \"attrs\" not in osm:\n        return Feature(geometry=shape(osm[\"geometry\"]), properties=osm[\"properties\"])\n\n    if \"osm_id\" in osm[\"attrs\"]:\n        id = osm[\"attrs\"][\"osm_id\"]\n    elif \"id\" in osm[\"attrs\"]:\n        id = osm[\"attrs\"][\"id\"]\n    props = {\"id\": id}\n    if \"version\" in osm[\"attrs\"]:\n        props[\"version\"] = osm[\"attrs\"][\"version\"]\n\n    props.update(osm[\"tags\"])\n    # It's a way, so no coordinate\n    if \"refs\" in osm:\n        return Feature(properties=props)\n    else:\n        geom = Point((float(osm[\"attrs\"][\"lon\"]), float(osm[\"attrs\"][\"lat\"])))\n\n        return Feature(geometry=geom, properties=props)\n</code></pre>"},{"location":"api/convert/","title":"convert.py","text":"<p>               Bases: <code>YamlFile</code></p> <p>A class to apply a YAML config file and convert ODK to OSM.</p> <p>Returns:</p> Type Description <code>Convert</code> <p>An instance of this object</p> Source code in <code>osm_merge/fieldwork/convert.py</code> <pre><code>def __init__(\n    self,\n    xform: str = None,\n):\n    if xform is not None:\n        file = xform\n    else:\n        file = rootdir + \"/fieldwork/xforms.yaml\"\n    self.yaml = YamlFile(file)\n    self.filespec = file\n    # Parse the file contents into a data structure to make it\n    # easier to retrieve values\n    self.convert = dict()\n    self.ignore = list()\n    self.private = list()\n    self.defaults = dict()\n    self.entries = dict()\n    self.types = dict()\n    self.saved = dict()\n    for item in self.yaml.yaml[\"convert\"]:\n        key = list(item.keys())[0]\n        value = item[key]\n        # print(\"ZZZZ: %r, %r\" % (key, value))\n        if type(value) is str:\n            self.convert[key] = value\n        elif type(value) is list:\n            vals = dict()\n            for entry in value:\n                if type(entry) is str:\n                    # epdb.st()\n                    tag = entry\n                else:\n                    tag = list(entry.keys())[0]\n                    vals[tag] = entry[tag]\n            self.convert[key] = vals\n    self.ignore = self.yaml.yaml[\"ignore\"]\n    self.private = self.yaml.yaml[\"private\"]\n    if \"multiple\" in self.yaml.yaml:\n        self.multiple = self.yaml.yaml[\"multiple\"]\n    else:\n        self.multiple = list()\n</code></pre> <p>options: show_source: false heading_level: 3</p>"},{"location":"api/convert/#osm_merge.fieldwork.convert.Convert.privateData","title":"privateData","text":"<pre><code>privateData(keyword)\n</code></pre> <p>Search the private data category for a keyword.</p> <p>Parameters:</p> Name Type Description Default <code>keyword</code> <code>str</code> <p>The keyword to search for</p> required <p>Returns:</p> Type Description <code>bool</code> <p>=If the keyword is in the private data section</p> Source code in <code>osm_merge/fieldwork/convert.py</code> <pre><code>def privateData(\n    self,\n    keyword: str,\n) -&gt; bool:\n    \"\"\"\n    Search the private data category for a keyword.\n\n    Args:\n        keyword (str): The keyword to search for\n\n    Returns:\n        (bool): =If the keyword is in the private data section\n    \"\"\"\n    return keyword.lower() in self.private\n</code></pre>"},{"location":"api/convert/#osm_merge.fieldwork.convert.Convert.convertData","title":"convertData","text":"<pre><code>convertData(keyword)\n</code></pre> <p>Search the convert data category for a keyword.</p> <p>Parameters:</p> Name Type Description Default <code>keyword</code> <code>str</code> <p>The keyword to search for</p> required <p>Returns:</p> Type Description <code>bool</code> <p>Check to see if the keyword is in the convert data section</p> Source code in <code>osm_merge/fieldwork/convert.py</code> <pre><code>def convertData(\n    self,\n    keyword: str,\n) -&gt; bool:\n    \"\"\"\n    Search the convert data category for a keyword.\n\n    Args:\n        keyword (str): The keyword to search for\n\n    Returns:\n        (bool): Check to see if the keyword is in the convert data section\n    \"\"\"\n    return keyword.lower() in self.convert\n</code></pre>"},{"location":"api/convert/#osm_merge.fieldwork.convert.Convert.ignoreData","title":"ignoreData","text":"<pre><code>ignoreData(keyword)\n</code></pre> <p>Search the convert data category for a ketyword.</p> <p>Parameters:</p> Name Type Description Default <code>keyword</code> <code>str</code> <p>The keyword to search for</p> required <p>Returns:</p> Type Description <code>bool</code> <p>Check to see if the keyword is in the ignore data section</p> Source code in <code>osm_merge/fieldwork/convert.py</code> <pre><code>def ignoreData(\n    self,\n    keyword: str,\n) -&gt; bool:\n    \"\"\"\n    Search the convert data category for a ketyword.\n\n    Args:\n        keyword (str): The keyword to search for\n\n    Returns:\n        (bool): Check to see if the keyword is in the ignore data section\n    \"\"\"\n    return keyword.lower() in self.ignore\n</code></pre>"},{"location":"api/convert/#osm_merge.fieldwork.convert.Convert.getKeyword","title":"getKeyword","text":"<pre><code>getKeyword(value)\n</code></pre> <p>Get the keyword for a value from the yaml file.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str</code> <p>The value to find the keyword for</p> required <p>Returns:</p> Type Description <code>str</code> <p>The keyword if found, or None</p> Source code in <code>osm_merge/fieldwork/convert.py</code> <pre><code>def getKeyword(\n    self,\n    value: str,\n) -&gt; str:\n    \"\"\"\n    Get the keyword for a value from the yaml file.\n\n    Args:\n        value (str): The value to find the keyword for\n\n    Returns:\n        (str): The keyword if found, or None\n    \"\"\"\n    key = self.yaml.yaml(value)\n    if type(key) == bool:\n        return value\n    if len(key) == 0:\n        key = self.yaml.getKeyword(value)\n    return key\n</code></pre>"},{"location":"api/convert/#osm_merge.fieldwork.convert.Convert.getValues","title":"getValues","text":"<pre><code>getValues(keyword=None)\n</code></pre> <p>Get the values for a primary key.</p> <p>Parameters:</p> Name Type Description Default <code>keyword</code> <code>str</code> <p>The keyword to get the value of</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>The values or None</p> Source code in <code>osm_merge/fieldwork/convert.py</code> <pre><code>def getValues(\n    self,\n    keyword: str = None,\n) -&gt; str:\n    \"\"\"\n    Get the values for a primary key.\n\n    Args:\n        keyword (str): The keyword to get the value of\n\n    Returns:\n        (str): The values or None\n    \"\"\"\n    if keyword is not None:\n        if keyword in self.convert:\n            return self.convert[keyword]\n    else:\n        return None\n</code></pre>"},{"location":"api/convert/#osm_merge.fieldwork.convert.Convert.convertEntry","title":"convertEntry","text":"<pre><code>convertEntry(tag, value)\n</code></pre> <p>Convert a tag and value from the ODK represention to an OSM one.</p> <p>Parameters:</p> Name Type Description Default <code>tag</code> <code>str</code> <p>The tag from the ODK XML file</p> required <code>value</code> <code>str</code> <p>The value from the ODK XML file</p> required <p>Returns:</p> Type Description <code>list</code> <p>The converted values</p> Source code in <code>osm_merge/fieldwork/convert.py</code> <pre><code>def convertEntry(\n    self,\n    tag: str,\n    value: str,\n) -&gt; list:\n    \"\"\"\n    Convert a tag and value from the ODK represention to an OSM one.\n\n    Args:\n        tag (str): The tag from the ODK XML file\n        value (str): The value from the ODK XML file\n\n    Returns:\n        (list): The converted values\n    \"\"\"\n    all = list()\n\n    # If it's not in any conversion data, pass it through unchanged.\n    if tag.lower() in self.ignore:\n        # logging.debug(f\"FIXME: Ignoring {tag}\")\n        return None\n    low = tag.lower()\n    if value is None:\n        return low\n\n    if low not in self.convert and low not in self.ignore and low not in self.private:\n        return {tag: value}\n\n    newtag = tag.lower()\n    newval = value\n    # If the tag is in the config file, convert it.\n    if self.convertData(newtag):\n        newtag = self.convertTag(newtag)\n        # if newtag != tag:\n        #    logging.debug(f\"Converted Tag for entry {tag} to {newtag}\")\n\n    # Truncate the elevation, as it's really long\n    if newtag == \"ele\":\n        value = value[:7]\n    newval = self.convertValue(newtag, value)\n    # logging.debug(\"Converted Value for entry '%s' to '%s'\" % (value, newval))\n    # there can be multiple new tag/value pairs for some values from ODK\n    if type(newval) == str:\n        all.append({newtag: newval})\n    elif type(newval) == list:\n        for entry in newval:\n            if type(entry) == str:\n                all.append({newtag: newval})\n            elif type(entry) == dict:\n                for k, v in entry.items():\n                    all.append({k: v})\n    return all\n</code></pre>"},{"location":"api/convert/#osm_merge.fieldwork.convert.Convert.convertValue","title":"convertValue","text":"<pre><code>convertValue(tag, value)\n</code></pre> <p>Convert a single tag value.</p> <p>Parameters:</p> Name Type Description Default <code>tag</code> <code>str</code> <p>The tag from the ODK XML file</p> required <code>value</code> <code>str</code> <p>The value from the ODK XML file</p> required <p>Returns:</p> Type Description <code>list</code> <p>The converted values</p> Source code in <code>osm_merge/fieldwork/convert.py</code> <pre><code>def convertValue(\n    self,\n    tag: str,\n    value: str,\n) -&gt; list:\n    \"\"\"\n    Convert a single tag value.\n\n    Args:\n        tag (str): The tag from the ODK XML file\n        value (str): The value from the ODK XML file\n\n    Returns:\n        (list): The converted values\n    \"\"\"\n    all = list()\n\n    vals = self.getValues(tag)\n    # There is no conversion data for this tag\n    if vals is None:\n        return value\n\n    if type(vals) is dict:\n        if value not in vals:\n            all.append({tag: value})\n            return all\n        if type(vals[value]) is bool:\n            entry = dict()\n            if vals[value]:\n                entry[tag] = \"yes\"\n            else:\n                entry[tag] = \"no\"\n            all.append(entry)\n            return all\n        for item in vals[value].split(\",\"):\n            entry = dict()\n            tmp = item.split(\"=\")\n            if len(tmp) == 1:\n                entry[tag] = vals[value]\n            else:\n                entry[tmp[0]] = tmp[1]\n                logging.debug(\"\\tValue %s converted value to %s\" % (value, entry))\n            all.append(entry)\n    return all\n</code></pre>"},{"location":"api/convert/#osm_merge.fieldwork.convert.Convert.convertTag","title":"convertTag","text":"<pre><code>convertTag(tag)\n</code></pre> <p>Convert a single tag.</p> <p>Parameters:</p> Name Type Description Default <code>tag</code> <code>str</code> <p>The tag from the ODK XML file</p> required <p>Returns:</p> Type Description <code>str</code> <p>The new tag</p> Source code in <code>osm_merge/fieldwork/convert.py</code> <pre><code>def convertTag(\n    self,\n    tag: str,\n) -&gt; str:\n    \"\"\"\n    Convert a single tag.\n\n    Args:\n        tag (str): The tag from the ODK XML file\n\n    Returns:\n        (str): The new tag\n    \"\"\"\n    low = tag.lower()\n    if low in self.convert:\n        newtag = self.convert[low]\n        if type(newtag) is str:\n            # logging.debug(\"\\tTag '%s' converted tag to '%s'\" % (tag, newtag))\n            tmp = newtag.split(\"=\")\n            if len(tmp) &gt; 1:\n                newtag = tmp[0]\n        elif type(newtag) is list:\n            logging.error(\"FIXME: list()\")\n            # epdb.st()\n            return low, value\n        elif type(newtag) is dict:\n            # logging.error(\"FIXME: dict()\")\n            return low\n        return newtag.lower()\n    else:\n        logging.debug(f\"Not in convert!: {low}\")\n        return low\n</code></pre>"},{"location":"api/convert/#osm_merge.fieldwork.convert.Convert.convertMultiple","title":"convertMultiple","text":"<pre><code>convertMultiple(value)\n</code></pre> <p>Convert a multiple tags from a select_multiple question..</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str</code> <p>The tags from the ODK XML file</p> required <p>Returns:</p> Type Description <code>list</code> <p>The new tags</p> Source code in <code>osm_merge/fieldwork/convert.py</code> <pre><code>def convertMultiple(\n    self,\n    value: str,\n) -&gt; list:\n    \"\"\"\n    Convert a multiple tags from a select_multiple question..\n\n    Args:\n        value (str): The tags from the ODK XML file\n\n    Returns:\n        (list): The new tags\n    \"\"\"\n    tags = dict()\n    for tag in value.split(\" \"):\n        low = tag.lower()\n        if self.convertData(low):\n            newtag = self.convert[low]\n            if newtag.find(\"=\") &gt; 0:\n                tmp = newtag.split(\"=\")\n                if tmp[0] in tags:\n                    tags[tmp[0]] = f\"{tags[tmp[0]]};{tmp[1]}\"\n                else:\n                    tags.update({tmp[0]: tmp[1]})\n        else:\n            tags.update({low: \"yes\"})\n    # logging.debug(f\"\\tConverted multiple to {tags}\")\n    return tags\n</code></pre>"},{"location":"api/convert/#osm_merge.fieldwork.convert.Convert.parseXLS","title":"parseXLS","text":"<pre><code>parseXLS(xlsfile)\n</code></pre> <p>Parse the source XLSFile if available to look for details we need.</p> Source code in <code>osm_merge/fieldwork/convert.py</code> <pre><code>def parseXLS(\n    self,\n    xlsfile: str,\n):\n    \"\"\"\n    Parse the source XLSFile if available to look for details we need.\n    \"\"\"\n    if xlsfile is not None and len(xlsfile) &gt; 0:\n        self.entries = pd.read_excel(xlsfile, sheet_name=[0])[0]\n        # There will only be a single sheet\n        names = self.entries[\"name\"]\n        defaults = self.entries[\"default\"]\n        i = 0\n        while i &lt; len(self.entries):\n            if type(self.entries[\"type\"][i]) == float:\n                self.types[self.entries[\"name\"][i]] = None\n            else:\n                self.types[self.entries[\"name\"][i]] = self.entries[\"type\"][i].split(\" \")[0]\n            i += 1\n        total = len(names)\n        i = 0\n        while i &lt; total:\n            entry = defaults[i]\n            if str(entry) != \"nan\":\n                pat = re.compile(\"..last-saved.*\")\n                if pat.match(entry):\n                    name = entry.split(\"#\")[1][:-1]\n                    self.saved[name] = None\n                else:\n                    self.defaults[names[i]] = entry\n            i += 1\n    return True\n</code></pre>"},{"location":"api/convert/#osm_merge.fieldwork.convert.Convert.dump","title":"dump","text":"<pre><code>dump()\n</code></pre> <p>Dump internal data structures, for debugging purposes only.</p> Source code in <code>osm_merge/fieldwork/convert.py</code> <pre><code>def dump(self):\n    \"\"\"Dump internal data structures, for debugging purposes only.\"\"\"\n    print(\"YAML file: %s\" % self.filespec)\n    print(\"Convert section\")\n    for key, val in self.convert.items():\n        if type(val) is list:\n            print(\"\\tTag %s is\" % key)\n            for data in val:\n                print(\"\\t\\t%r\" % data)\n        else:\n            print(\"\\tTag %s is %s\" % (key, val))\n\n    print(\"Ignore Section\")\n    for item in self.ignore:\n        print(f\"\\tIgnoring tag {item}\")\n</code></pre>"},{"location":"api/dbextract/","title":"DB Extract","text":"<p>options: show_source: false heading_level: 3</p>"},{"location":"api/dbextract/#osm_merge.dbextract.main","title":"main","text":"<pre><code>main()\n</code></pre> <p>This program queries a postgres database as maintained by Underpass.</p> Source code in <code>osm_merge/dbextract.py</code> <pre><code>def main():\n    \"\"\"\n    This program queries a postgres database as maintained by Underpass.\n    \"\"\"\n    parser = argparse.ArgumentParser(description=\"Query a DB and output to OSM XML format\")\n    parser.add_argument(\"-v\", \"--verbose\", nargs=\"?\", const=\"0\", help=\"verbose output\")\n    parser.add_argument(\"-b\",\"--boundary\", help='Optional boundary to clip the data')\n    parser.add_argument(\"-o\",\"--outfile\", default='out.geojson', help='The output file')\n    parser.add_argument(\"-u\", \"--uri\", default='localhost/underpass', help=\"Database URI\")\n\n    args = parser.parse_args()\n\n    # if verbose, dump to the terminal\n    if args.verbose is not None:\n        logging.basicConfig(\n            level=logging.DEBUG,\n            format=(\"%(threadName)10s - %(name)s - %(levelname)s - %(message)s\"),\n            datefmt=\"%y-%m-%d %H:%M:%S\",\n            stream=sys.stdout,\n        )\n    try:\n        uri = \"dbname=underpass\"\n        pg = psycopg2.connect(uri)\n        curs = pg.cursor()\n    except Exception as e:\n        log.error(f\"Couldn't connect to database: {e}\")\n        quit()\n\n\n    # Make a temporary view to reduce the data size\n    if args.boundary:\n        log.info('Clipping by boundary...')\n        # optionally clip by a boundary\n        file = open(args.boundary, \"r\")\n        data = geojson.load(file)\n        aoi = shape(data[\"geometry\"])\n        file.close()\n        sql = f\"CREATE OR REPLACE VIEW highway_view AS SELECT * FROM ways_line WHERE tags-&gt;&gt;'highway' IS NOT NULL AND ST_CONTAINS(ST_GeomFromEWKT('SRID=4326;{aoi.wkt}'), geom);\"\n        # print(sql)\n        curs.execute(sql)\n        sql = f\"CREATE OR REPLACE VIEW nodes_view AS SELECT * FROM nodes WHERE ST_CONTAINS(ST_GeomFromEWKT('SRID=4326;{aoi.wkt}'), geom);\"\n        # print(sql)\n        curs.execute(sql)\n    else:\n        # By default, get all the highways\n        sql = f\"CREATE OR REPLACE VIEW highway_view AS SELECT * FROM ways_line WHERE tags-&gt;&gt;'highway' IS NOT NULL\"\n        # print(sql)\n        curs.execute(sql)\n        sql = f\"CREATE OR REPLACE VIEW nodes_view AS SELECT * FROM nodes\"\n        # print(sql)\n        curs.execute(sql)\n\n    # Process the nodes\n    features = list()\n    sql = f\"SELECT osm_id,version,timestamp,ST_AsTEXT(geom) FROM nodes_view WHERE tags IS NULL ORDER BY osm_id;\"\n    print(sql)\n    log.info('Processing nodes...')\n    curs.execute(sql)\n    #result = curs.fetchall()\n    breakpoint()\n    for row in curs.fetchone():\n        print(row)\n        # spin.next()\n        osm_id = row[0]\n        version = row[1]\n        timestamp = row[2]\n        geom = shapely.from_wkt(row[3])\n        data = {\"osm_id\": osm_id, \"version\": version, \"timestamp\": timestamp}\n        # print(data)\n        features.append(Feature(geometry=geom, properties=data))\n\n    # Process the highways\n    sql = f\"SELECT osm_id,version,timestamp,refs,tags,ST_AsTEXT(geom) FROM highway_view WHERE tags-&gt;&gt;'highway' IS NOT NULL;\"\n    print(sql)\n    # spin = Spinner('Processing ways...')\n    log.info('Processing ways...')\n    curs.execute(sql)\n    result = curs.fetchall()\n    for row in result:\n        # spin.next()\n        osm_id = row[0]\n        version = row[1]\n        timestamp = row[2]\n        refs = row[3]\n        tags = row[4]\n        geom = shapely.from_wkt(row[5])\n        data = {\"osm_id\": osm_id, \"version\": version, \"refs\": refs}\n        data.update(tags)\n        # print(data)\n        features.append(Feature(geometry=geom, properties=data))\n\n    path = Path(args.outfile)\n\n    if path.suffix == '.geojson':\n        file = open(args.outfile, \"w\")\n        geojson.dump(FeatureCollection(features), file)\n        file.close()\n    elif path.suffix == '.osm':\n        osm = OsmFile()\n        osm.writeOSM(features, args.outfile)\n\n    log.info(f\"Wrote {args.outfile}\")\n</code></pre>"},{"location":"api/filter_data/","title":"filter_data.py","text":"<p>               Bases: <code>object</code></p> <p>Returns:</p> Type Description <code>FilterData</code> <p>An instance of this object</p> Source code in <code>osm_merge/fieldwork/filter_data.py</code> <pre><code>def __init__(\n    self,\n    filespec: str = None,\n    config: QueryConfig = None,\n):\n    \"\"\"Args:\n        filespec (str): The optional data file to read.\n\n    Returns:\n        (FilterData): An instance of this object\n    \"\"\"\n    self.tags = dict()\n    self.qc = config\n    if filespec and config:\n        self.parse(filespec, config)\n</code></pre> <p>options: show_source: false heading_level: 3</p>"},{"location":"api/filter_data/#osm_merge.fieldwork.filter_data.FilterData.parse","title":"parse","text":"<pre><code>parse(filespec, config)\n</code></pre> <p>Read in the XLSForm and extract the data we want.</p> <p>Parameters:</p> Name Type Description Default <code>filespec</code> <code>str</code> <p>The filespec to the XLSForm file</p> required <p>Returns:</p> Name Type Description <code>title</code> <code>str</code> <p>The title from the XLSForm Setting sheet</p> <code>extract</code> <code>str</code> <p>The data extract filename from the XLSForm Survey sheet</p> Source code in <code>osm_merge/fieldwork/filter_data.py</code> <pre><code>def parse(\n    self,\n    filespec: str,\n    config: QueryConfig,\n):\n    \"\"\"Read in the XLSForm and extract the data we want.\n\n    Args:\n        filespec (str): The filespec to the XLSForm file\n\n    Returns:\n        title (str): The title from the XLSForm Setting sheet\n        extract (str): The data extract filename from the XLSForm Survey sheet\n    \"\"\"\n    if config:\n        self.qc = config\n    excel_object = pd.ExcelFile(filespec)\n    entries = excel_object.parse(sheet_name=[0, 1, 2], index_col=0, usercols=[0, 1, 2])\n    entries = pd.read_excel(filespec, sheet_name=[0, 1, 2])\n    title = entries[2][\"form_title\"].to_list()[0]\n    extract = \"\"\n    for entry in entries[0][\"type\"]:\n        if str(entry) == \"nan\":\n            continue\n        if entry[:20] == \"select_one_from_file\":\n            extract = entry[21:]\n            log.info(f'Got data extract filename: \"{extract}\", title: \"{title}\"')\n        else:\n            extract = \"none\"\n    total = len(entries[1][\"list_name\"])\n    index = 1\n    while index &lt; total:\n        key = entries[1][\"list_name\"][index]\n        if key == \"model\" or str(key) == \"nan\":\n            index += 1\n            continue\n        value = entries[1][\"name\"][index]\n        if value == \"&lt;text&gt;\" or str(value) == \"null\":\n            index += 1\n            continue\n        if key not in self.tags:\n            self.tags[key] = list()\n        self.tags[key].append(value)\n        index += 1\n\n    # The yaml config file for the query has a list of columns\n    # to keep in addition to this default set. These wind up\n    # in the SELECT\n    keep = (\n        \"name\",\n        \"name:en\",\n        \"id\",\n        \"operator\",\n        \"addr:street\",\n        \"addr:housenumber\",\n        \"osm_id\",\n        \"title\",\n        \"tags\",\n        \"label\",\n        \"landuse\",\n        \"opening_hours\",\n        \"tourism\",\n    )\n    self.keep = list(keep)\n    if \"keep\" in config.config[\"keep\"]:\n        self.keep.extend(config.config[\"keep\"])\n\n    return title, extract\n</code></pre>"},{"location":"api/filter_data/#osm_merge.fieldwork.filter_data.FilterData.cleanData","title":"cleanData","text":"<pre><code>cleanData(data)\n</code></pre> <p>Filter out any data not in the data_model.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>bytes</code> <p>The input data or filespec to the input data file</p> required <p>Returns:</p> Type Description <code>FeatureCollection</code> <p>The modifed data</p> Source code in <code>osm_merge/fieldwork/filter_data.py</code> <pre><code>def cleanData(\n    self,\n    data,\n):\n    \"\"\"Filter out any data not in the data_model.\n\n    Args:\n        data (bytes): The input data or filespec to the input data file\n\n    Returns:\n        (FeatureCollection): The modifed data\n\n    \"\"\"\n    log.debug(\"Cleaning data...\")\n    if type(data) == str:\n        outfile = open(f\"new-{data}\", \"x\")\n        infile = open(tmpfile, \"r\")\n        indata = geojson.load(infile)\n    elif type(data) == bytes:\n        indata = eval(data.decode())\n    else:\n        indata = data\n    # these just create noise in the log file\n    ignore = (\n        \"timestamp\",\n        \"version\",\n        \"changeset\",\n    )\n    keep = (\"osm_id\", \"id\", \"version\")\n    collection = list()\n    for feature in indata[\"features\"]:\n        # log.debug(f\"FIXME0: {feature}\")\n        properties = dict()\n        for key, value in feature[\"properties\"].items():\n            # log.debug(f\"{key} = {value}\")\n            # FIXME: this is a hack!\n            if True:\n                if key == \"tags\":\n                    for k, v in value.items():\n                        if k[:4] == \"name\":\n                            properties[\"title\"] = value[k]\n                            properties[\"label\"] = value[k]\n                        else:\n                            properties[k] = v\n                else:\n                    if key == \"osm_id\":\n                        properties[\"id\"] = value\n                        properties[\"title\"] = value\n                        properties[\"label\"] = value\n                    else:\n                        properties[key] = value\n                        if key[:4] == \"name\":\n                            properties[\"title\"] = value\n                            properties[\"label\"] = value\n            else:\n                log.debug(f\"FIXME2: {key} = {value}\")\n                if key in keep:\n                    properties[key] = value\n                    continue\n                if key in self.tags:\n                    if key == \"name\" or key == \"name:en\":\n                        properties[\"title\"] = self.tags[key]\n                        properties[\"label\"] = self.tags[key]\n                    if value in self.tags[key]:\n                        properties[key] = value\n                    else:\n                        if value != \"yes\":\n                            log.warning(f\"Value {value} not in the data model!\")\n                        continue\n                else:\n                    if key in ignore:\n                        continue\n                    log.warning(f\"Tag {key} not in the data model!\")\n                    continue\n        if \"title\" not in properties:\n            properties[\"label\"] = properties[\"id\"]\n            properties[\"title\"] = properties[\"id\"]\n        newfeature = Feature(geometry=feature[\"geometry\"], properties=properties)\n        collection.append(newfeature)\n    if type(data) == str:\n        geojson.dump(FeatureCollection(collection), outfile)\n    return FeatureCollection(collection)\n</code></pre>"},{"location":"api/odk2osm/","title":"odk2osm.py","text":"<p>options: show_source: false heading_level: 3</p>"},{"location":"api/odk2osm/#osm_merge.fieldwork.odk2osm.main","title":"main","text":"<pre><code>main()\n</code></pre> <p>This is a program that reads in the ODK Instance file, which is in XML, and converts it to an OSM XML file so it can be viewed in an editor.</p> Source code in <code>osm_merge/fieldwork/odk2osm.py</code> <pre><code>def main():\n    \"\"\"This is a program that reads in the ODK Instance file, which is in XML,\n    and converts it to an OSM XML file so it can be viewed in an editor.\n    \"\"\"\n    parser = argparse.ArgumentParser(description=\"Convert ODK XML instance file to OSM XML format\")\n    parser.add_argument(\"-v\", \"--verbose\", nargs=\"?\", const=\"0\", help=\"verbose output\")\n    parser.add_argument(\"-y\", \"--yaml\", help=\"Alternate YAML file\")\n    parser.add_argument(\"-x\", \"--xlsfile\", help=\"Source XLSFile\")\n    parser.add_argument(\"-i\", \"--infile\", required=True, help=\"The input file\")\n    parser.add_argument(\"-o\",\"--outfile\", default='out.osm',\n                        help='The output file for JOSM')\n    args = parser.parse_args()\n\n    # if verbose, dump to the terminal\n    if args.verbose is not None:\n        logging.basicConfig(\n            level=logging.DEBUG,\n            format=(\"%(threadName)10s - %(name)s - %(levelname)s - %(message)s\"),\n            datefmt=\"%y-%m-%d %H:%M:%S\",\n            stream=sys.stdout,\n        )\n\n    toplevel = Path(args.infile)\n    odk = ODKParsers(args.yaml)\n    odk.parseXLS(args.xlsfile)\n    xmlfiles = list()\n    data = list()\n    # It's a wildcard, used for XML instance files\n    if args.infile.find(\"*\") &gt;= 0:\n        log.debug(f\"Parsing multiple ODK XML files {args.infile}\")\n        toplevel = Path(args.infile[:-1])\n        for dirs in glob.glob(args.infile):\n            xml = os.listdir(dirs)\n            full = os.path.join(dirs, xml[0])\n            xmlfiles.append(full)\n        for infile in xmlfiles:\n            entry = odk.XMLparser(infile)\n            # entry = odk.createEntry(tmp[0])\n            data.append(entry)\n    elif toplevel.suffix == \".xml\":\n        # It's an instance file from ODK Collect\n        log.debug(f\"Parsing ODK XML files {args.infile}\")\n        # There is always only one XML file per infile\n        full = os.path.join(toplevel, os.path.basename(toplevel))\n        xmlfiles.append(full + \".xml\")\n        tmp = odk.XMLparser(args.infile)\n        # entry = odk.createEntry(tmp)\n        data.append(entry)\n    elif toplevel.suffix == \".csv\":\n        log.debug(f\"Parsing csv files {args.infile}\")\n        for entry in odk.CSVparser(args.infile):\n            data.append(entry)\n    elif toplevel.suffix == \".json\":\n        log.debug(f\"Parsing json files {args.infile}\")\n        for entry in odk.JSONparser(args.infile):\n            data.append(entry)\n\n    # Write the data\n    osm = OsmFile()\n    osm.writeOSM(data, args.outfile)\n    log.info(f\"Wrote {args.outfile}\")\n</code></pre>"},{"location":"api/osmfile/","title":"osmfile.py","text":"<p>               Bases: <code>object</code></p> <p>OSM File output.</p> <p>Parameters:</p> Name Type Description Default <code>filespec</code> <code>str</code> <p>The input or output file</p> <code>None</code> <code>options</code> <code>dict</code> <p>Command line options</p> <code>None</code> <code>outdir</code> <code>str</code> <p>The output directory for the file</p> <code>'/tmp/'</code> <p>Returns:</p> Type Description <code>OsmFile</code> <p>An instance of this object</p> Source code in <code>osm_merge/osmfile.py</code> <pre><code>def __init__(\n    self,\n    filespec: str = None,\n    options: dict = None,\n    outdir: str = \"/tmp/\",\n):\n    \"\"\"This class reads and writes the OSM XML formated files.\n\n    Args:\n        filespec (str): The input or output file\n        options (dict): Command line options\n        outdir (str): The output directory for the file\n\n    Returns:\n        (OsmFile): An instance of this object\n    \"\"\"\n    if options is None:\n        options = dict()\n    self.options = options\n    # Read the config file to get our OSM credentials, if we have any\n    # self.config = config.config(self.options)\n    self.version = 3\n    self.visible = \"true\"\n    self.osmid = -1\n    # Open the OSM output file\n    self.file = None\n    if filespec is not None:\n        self.file = open(filespec, \"w\")\n        # self.file = open(filespec + \".osm\", 'w')\n        logging.info(\"Opened output file: \" + filespec)\n    self.header()\n    # logging.error(\"Couldn't open %s for writing!\" % filespec)\n\n    # This is the file that contains all the filtering data\n    # self.ctable = convfile(self.options.get('convfile'))\n    # self.options['convfile'] = None\n    # These are for importing the CO addresses\n    self.full = None\n    self.addr = None\n    # decrement the ID\n    self.start = -1\n    # path = xlsforms_path.replace(\"xlsforms\", \"\")\n    self.data = list()\n</code></pre> <p>options: show_source: false heading_level: 3</p>"},{"location":"api/osmfile/#osm_merge.osmfile.OsmFile.isclosed","title":"isclosed","text":"<pre><code>isclosed()\n</code></pre> <p>Is the OSM XML file open or closed ?</p> <p>Returns:</p> Type Description <code>bool</code> <p>The OSM XML file status</p> Source code in <code>osm_merge/osmfile.py</code> <pre><code>def isclosed(self):\n    \"\"\"Is the OSM XML file open or closed ?\n\n    Returns:\n        (bool): The OSM XML file status\n    \"\"\"\n    return self.file.closed\n</code></pre>"},{"location":"api/osmfile/#osm_merge.osmfile.OsmFile.header","title":"header","text":"<pre><code>header()\n</code></pre> <p>Write the header of the OSM XML file.</p> Source code in <code>osm_merge/osmfile.py</code> <pre><code>def header(self):\n    \"\"\"Write the header of the OSM XML file.\"\"\"\n    if self.file is not None:\n        self.file.write(\"&lt;?xml version='1.0' encoding='UTF-8'?&gt;\\n\")\n        self.file.write('&lt;osm version=\"0.6\" generator=\"osm-merge 0.3\"&gt;\\n')\n        self.file.flush()\n</code></pre>"},{"location":"api/osmfile/#osm_merge.osmfile.OsmFile.footer","title":"footer","text":"<pre><code>footer()\n</code></pre> <p>Write the footer of the OSM XML file.</p> Source code in <code>osm_merge/osmfile.py</code> <pre><code>def footer(self):\n    \"\"\"Write the footer of the OSM XML file.\"\"\"\n    # logging.debug(\"FIXME: %r\" % self.file)\n    if self.file is not None:\n        self.file.write(\"&lt;/osm&gt;\\n\")\n        self.file.flush()\n        if self.file is False:\n            self.file.close()\n    self.file = None\n</code></pre>"},{"location":"api/osmfile/#osm_merge.osmfile.OsmFile.loadFile","title":"loadFile","text":"<pre><code>loadFile(osmfile)\n</code></pre> <p>Read a OSM XML file and convert it to GeoJson for consistency.</p> <p>Parameters:</p> Name Type Description Default <code>osmfile</code> <code>str</code> <p>The OSM XML file to load</p> required <p>Returns:</p> Type Description <code>list</code> <p>The entries in the OSM XML file</p> Source code in <code>osm_merge/osmfile.py</code> <pre><code>def loadFile(\n    self,\n    osmfile: str,\n) -&gt; list:\n    \"\"\"\n    Read a OSM XML file and convert it to GeoJson for consistency.\n\n    Args:\n        osmfile (str): The OSM XML file to load\n\n    Returns:\n        (list): The entries in the OSM XML file\n    \"\"\"\n    alldata = list()\n    size = os.path.getsize(osmfile)\n    with open(osmfile, \"r\") as file:\n        xml = file.read(size)\n        doc = xmltodict.parse(xml)\n        if \"osm\" not in doc:\n            logging.warning(\"No data in this instance\")\n            return False\n        data = doc[\"osm\"]\n        if \"node\" not in data:\n            logging.warning(\"No nodes in this instance\")\n            return False\n\n    nodes = dict()\n    for node in data[\"node\"]:\n        properties = {\n            \"id\": int(node[\"@id\"]),\n        }\n        if \"@version\" not in node:\n            properties[\"version\"] = 1\n        else:\n            properties[\"version\"] = node[\"@version\"]\n\n        if \"@timestamp\" in node:\n            properties[\"timestamp\"] = node[\"@timestamp\"]\n\n        if \"tag\" in node:\n            for tag in node[\"tag\"]:\n                if type(tag) == dict:\n                    # Drop all the TIGER tags based on\n                    # https://wiki.openstreetmap.org/wiki/TIGER_fixup\n                    if tag[\"@k\"] in properties:\n                        if properties[tag[\"@k\"]][:7] == \"tiger:\":\n                            continue\n                    properties[tag[\"@k\"]] = tag[\"@v\"].strip()\n                    # continue\n                else:\n                    properties[node[\"tag\"][\"@k\"]] = node[\"tag\"][\"@v\"].strip()\n                # continue\n        geom = Point((float(node[\"@lon\"]), float(node[\"@lat\"])))\n        # cache the nodes so we can dereference the refs into\n        # coordinates, but we don't need them in GeoJson format.\n        nodes[properties[\"id\"]] = geom\n        if len(properties) &gt; 2:\n            self.data.append(Feature(geometry=geom, properties=properties))\n\n    for way in data[\"way\"]:\n        attrs = dict()\n        properties = {\n            \"id\": int(way[\"@id\"]),\n        }\n        refs = list()\n        if \"nd\" in way:\n            if len(way[\"nd\"]) &gt; 0:\n                for ref in way[\"nd\"]:\n                    refs.append(int(ref[\"@ref\"]))\n            properties[\"refs\"] = refs\n\n        if \"@version\" not in node:\n            properties[\"version\"] = 1\n        else:\n            properties[\"version\"] = node[\"@version\"]\n\n        if \"@timestamp\" in node:\n            attrs[\"timestamp\"] = node[\"@timestamp\"]\n\n        if \"tag\" in way:\n            for tag in way[\"tag\"]:\n                if type(tag) == dict:\n                    properties[tag[\"@k\"]] = tag[\"@v\"].strip()\n                    # continue\n                else:\n                    properties[way[\"tag\"][\"@k\"]] = way[\"tag\"][\"@v\"].strip()\n                # continue\n        # geom =\n        tmp = list()\n        for ref in refs:\n            if ref in nodes:\n                # Only add nodes that have been cached\n                tmp.append(nodes[ref]['coordinates'])\n        geom = LineString(tmp)\n        if geom is None:\n            breakpoint()\n        # log.debug(f\"WAY: {properties}\")\n        self.data.append(Feature(geometry=geom, properties=properties))\n\n    return self.data\n</code></pre>"},{"location":"api/osmfile/#osm_merge.osmfile.OsmFile.writeOSM","title":"writeOSM","text":"<pre><code>writeOSM(data, filespec)\n</code></pre> <p>Write the data to an OSM XML file.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>list</code> <p>The list of GeoJson features</p> required Source code in <code>osm_merge/osmfile.py</code> <pre><code>def writeOSM(self,\n             data: list,\n             filespec: str,\n             ):\n    \"\"\"\n    Write the data to an OSM XML file.\n\n    Args:\n        data (list): The list of GeoJson features\n    \"\"\"\n    negid = -100\n    id = -1\n    out = str()\n    newmvum = list()\n    self.file = open(filespec, \"w\")\n    self.header()\n    for entry in data:\n        version = 1\n        tags = entry[\"properties\"]\n        if \"osm_id\" in tags:\n            id = tags[\"osm_id\"]\n        elif \"id\" in tags:\n            id = tags[\"id\"]\n        elif \"id\" not in tags:\n            # There is no id or version for non OSM features\n            id -= 1\n        if \"version\" in entry[\"properties\"]:\n            version = int(entry[\"properties\"][\"version\"])\n            version += 1\n        # if id == 814085818:\n        #    breakpoint()\n        attrs = {\"id\": id, \"version\": version}\n        # These are OSM attributes, not tags\n        if \"id\" in tags:\n            del tags[\"id\"]\n        if \"version\" in tags:\n            del tags[\"version\"]\n        item = {\"attrs\": attrs, \"tags\": tags}\n        # if entry[\"geometry\"][\"type\"] == \"LineString\" or entry[\"geometry\"][\"type\"] == \"Polygon\":\n        # print(entry)\n        out = str()\n        if entry[\"geometry\"] is not None and entry[\"geometry\"][\"type\"] == \"Point\":\n            # It's a node referenced by a way\n            item[\"attrs\"][\"lon\"] = entry[\"geometry\"][\"coordinates\"][0]\n            item[\"attrs\"][\"lat\"] = entry[\"geometry\"][\"coordinates\"][1]\n            if \"timestamp\" in item[\"tags\"]:\n                item[\"attrs\"][\"timestamp\"] = item[\"tags\"][\"timestamp\"]\n                del item[\"tags\"][\"timestamp\"]\n            # referenced nodes should have no tags\n            # del item[\"tags\"]\n            # FIXME: determine if we need to write nodes\n            out = self.createNode(item, False)\n        else:\n            # OSM ways don't have a geometry, just references to node IDs.\n            # The OSM XML file won't have any nodes, so at first won't\n            # display in JOSM until you do a File-&gt;\"Update modified\",\n            if \"refs\" not in tags:\n                # log.debug(f\"No Refs, so new MVUM road not in OSM {tags}\")\n                # tags[\"fixme\"] = \"New road from MVUM, don't add!\"\n                # FIXME: for now we don't do anything with new roads from\n                # an external dataset, because that would be an import.\n                newmvum.append(entry)\n                continue\n            if len(tags['refs']) &gt; 0:\n                if type(tags[\"refs\"]) != list:\n                    item[\"refs\"] = eval(tags[\"refs\"])\n                else:\n                    item[\"refs\"] = tags[\"refs\"]\n                del tags[\"refs\"]\n                out = self.createWay(item, True)\n        if len(out) &gt; 0:\n            self.file.write(out + \"\\n\")\n    self.footer()\n</code></pre>"},{"location":"api/osmfile/#osm_merge.osmfile.OsmFile.createWay","title":"createWay","text":"<pre><code>createWay(way, modified=False)\n</code></pre> <p>This creates a string that is the OSM representation of a node.</p> <p>Parameters:</p> Name Type Description Default <code>way</code> <code>dict</code> <p>The input way data structure</p> required <code>modified</code> <code>bool</code> <p>Is this a modified feature ?</p> <code>False</code> <p>Returns:</p> Type Description <code>str</code> <p>The OSM XML entry</p> Source code in <code>osm_merge/osmfile.py</code> <pre><code>def createWay(\n    self,\n    way: dict,\n    modified: bool = False,\n):\n    \"\"\"This creates a string that is the OSM representation of a node.\n\n    Args:\n        way (dict): The input way data structure\n        modified (bool): Is this a modified feature ?\n\n    Returns:\n        (str): The OSM XML entry\n    \"\"\"\n    attrs = dict()\n    osm = \"\"\n\n    # Add default attributes\n    if modified:\n        attrs[\"action\"] = \"modify\"\n    if \"osm_way_id\" in way[\"attrs\"]:\n        attrs[\"id\"] = int(way[\"attrs\"][\"osm_way_id\"])\n    elif \"osm_id\" in way[\"attrs\"]:\n        attrs[\"id\"] = int(way[\"attrs\"][\"osm_id\"])\n    elif \"id\" in way[\"attrs\"]:\n        attrs[\"id\"] = int(way[\"attrs\"][\"id\"])\n    else:\n        attrs[\"id\"] = self.start\n        self.start -= 1\n    if \"version\" not in way[\"attrs\"]:\n        attrs[\"version\"] = 1\n    else:\n        attrs[\"version\"] = way[\"attrs\"][\"version\"]\n    attrs[\"timestamp\"] = datetime.now().strftime(\"%Y-%m-%dT%TZ\")\n    # If the resulting file is publicly accessible without authentication, The GDPR applies\n    # and the identifying fields should not be included\n    if \"uid\" in way[\"attrs\"]:\n        attrs[\"uid\"] = way[\"attrs\"][\"uid\"]\n    if \"user\" in way[\"attrs\"]:\n        attrs[\"user\"] = way[\"attrs\"][\"user\"]\n\n    # Make all the nodes first. The data in the track has 4 fields. The first two\n    # are the lat/lon, then the altitude, and finally the GPS accuracy.\n    # newrefs = list()\n    node = dict()\n    node[\"attrs\"] = dict()\n    # The geometry is an EWKT string, so there is no need to get fancy with\n    # geometries, just manipulate the string, as OSM XML it's only strings\n    # anyway.\n    # geom = way['geom'][19:][:-2]\n    # print(geom)\n    # points = geom.split(\",\")\n    # print(points)\n\n    # epdb.st()\n    # loop = 0\n    # while loop &lt; len(way['refs']):\n    #     #print(f\"{points[loop]} {way['refs'][loop]}\")\n    #     node['timestamp'] = attrs['timestamp']\n    #     if 'user' in attrs and attrs['user'] is not None:\n    #         node['attrs']['user'] = attrs['user']\n    #     if 'uid' in attrs and attrs['uid'] is not None:\n    #         node['attrs']['uid'] = attrs['uid']\n    #     node['version'] = 0\n    #     lat,lon = points[loop].split(' ')\n    #     node['attrs']['lat'] = lat\n    #     node['attrs']['lon'] = lon\n    #     node['attrs']['id'] = way['refs'][loop]\n    #     osm += self.createNode(node) + '\\n'\n    #     loop += 1\n\n    # Processs atrributes\n    line = \"\"\n    for ref, value in attrs.items():\n        line += \"%s=%r \" % (ref, str(value))\n    osm += \"  &lt;way \" + line + \"&gt;\"\n\n    if \"refs\" in way:\n        for ref in way[\"refs\"]:\n            osm += '\\n    &lt;nd ref=\"%s\"/&gt;' % ref\n    if \"tags\" in way:\n        for key, value in way[\"tags\"].items():\n            if value is None:\n                continue\n            if key == \"track\":\n                continue\n            if key not in attrs:\n                newkey = html.escape(key)\n                newval = html.escape(str(value))\n                osm += f\"\\n    &lt;tag k='{newkey}' v='{newval}'/&gt;\"\n        if modified:\n            osm += '\\n    &lt;tag k=\"note\" v=\"Do not upload this without validation!\"/&gt;'\n        osm += \"\\n\"\n    osm += \"  &lt;/way&gt;\\n\"\n\n    return osm\n</code></pre>"},{"location":"api/osmfile/#osm_merge.osmfile.OsmFile.featureToNode","title":"featureToNode","text":"<pre><code>featureToNode(feature)\n</code></pre> <p>Convert a GeoJson feature into the data structures used here.</p> <p>Parameters:</p> Name Type Description Default <code>feature</code> <code>dict</code> <p>The GeoJson feature to convert</p> required <p>Returns:</p> Type Description <code>dict</code> <p>The data structure used by this file</p> Source code in <code>osm_merge/osmfile.py</code> <pre><code>def featureToNode(\n    self,\n    feature: dict,\n):\n    \"\"\"Convert a GeoJson feature into the data structures used here.\n\n    Args:\n        feature (dict): The GeoJson feature to convert\n\n    Returns:\n        (dict): The data structure used by this file\n    \"\"\"\n    osm = dict()\n    ignore = (\"label\", \"title\")\n    tags = dict()\n    attrs = dict()\n    for tag, value in feature[\"properties\"].items():\n        if tag == \"id\":\n            attrs[\"osm_id\"] = value\n        elif tag not in ignore:\n            tags[tag] = value\n    coords = feature[\"geometry\"][\"coordinates\"]\n    attrs[\"lat\"] = coords[1]\n    attrs[\"lon\"] = coords[0]\n    osm[\"attrs\"] = attrs\n    osm[\"tags\"] = tags\n    return osm\n</code></pre>"},{"location":"api/osmfile/#osm_merge.osmfile.OsmFile.createNode","title":"createNode","text":"<pre><code>createNode(node, modified=False)\n</code></pre> <p>This creates a string that is the OSM representation of a node.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>dict</code> <p>The input node data structure</p> required <code>modified</code> <code>bool</code> <p>Is this a modified feature ?</p> <code>False</code> <p>Returns:</p> Type Description <code>str</code> <p>The OSM XML entry</p> Source code in <code>osm_merge/osmfile.py</code> <pre><code>def createNode(\n    self,\n    node: dict,\n    modified: bool = False,\n):\n    \"\"\"This creates a string that is the OSM representation of a node.\n\n    Args:\n        node (dict): The input node data structure\n        modified (bool): Is this a modified feature ?\n\n    Returns:\n        (str): The OSM XML entry\n    \"\"\"\n    attrs = dict()\n    # Add default attributes\n    if modified:\n        attrs[\"action\"] = \"modify\"\n\n    if \"id\" in node[\"attrs\"]:\n        attrs[\"id\"] = int(node[\"attrs\"][\"id\"])\n    else:\n        attrs[\"id\"] = self.start\n        self.start -= 1\n    if \"version\" not in node[\"attrs\"]:\n        attrs[\"version\"] = \"1\"\n    else:\n        attrs[\"version\"] = int(node[\"attrs\"][\"version\"]) + 1\n    attrs[\"lat\"] = node[\"attrs\"][\"lat\"]\n    attrs[\"lon\"] = node[\"attrs\"][\"lon\"]\n    attrs[\"timestamp\"] = datetime.now().strftime(\"%Y-%m-%dT%TZ\")\n    # If the resulting file is publicly accessible without authentication, THE GDPR applies\n    # and the identifying fields should not be included\n    if \"uid\" in node[\"attrs\"]:\n        attrs[\"uid\"] = node[\"attrs\"][\"uid\"]\n    if \"user\" in node[\"attrs\"]:\n        attrs[\"user\"] = node[\"attrs\"][\"user\"]\n\n    # Processs atrributes\n    line = \"\"\n    osm = \"\"\n    for ref, value in attrs.items():\n        line += \"%s=%r \" % (ref, str(value))\n    osm += \"  &lt;node \" + line\n\n    if \"tags\" in node:\n        osm += \"&gt;\"\n        for key, value in node[\"tags\"].items():\n            if not value:\n                continue\n            if key not in attrs:\n                newkey = html.escape(key)\n                newval = html.escape(str(value))\n                osm += f\"\\n    &lt;tag k='{newkey}' v='{newval}'/&gt;\"\n        osm += \"\\n  &lt;/node&gt;\\n\"\n    else:\n        osm += \"/&gt;\"\n\n    return osm\n</code></pre>"},{"location":"api/osmfile/#osm_merge.osmfile.OsmFile.createTag","title":"createTag","text":"<pre><code>createTag(field, value)\n</code></pre> <p>Create a data structure for an OSM feature tag.</p> <p>Parameters:</p> Name Type Description Default <code>field</code> <code>str</code> <p>The tag name</p> required <code>value</code> <code>str</code> <p>The value for the tag</p> required <p>Returns:</p> Type Description <code>dict</code> <p>The newly created tag pair</p> Source code in <code>osm_merge/osmfile.py</code> <pre><code>def createTag(\n    self,\n    field: str,\n    value: str,\n):\n    \"\"\"Create a data structure for an OSM feature tag.\n\n    Args:\n        field (str): The tag name\n        value (str): The value for the tag\n\n    Returns:\n        (dict): The newly created tag pair\n    \"\"\"\n    newval = str(value)\n    newval = newval.replace(\"&amp;\", \"and\")\n    newval = newval.replace('\"', \"\")\n    tag = dict()\n    # logging.debug(\"OSM:makeTag(field=%r, value=%r)\" % (field, newval))\n\n    newtag = field\n    change = newval.split(\"=\")\n    if len(change) &gt; 1:\n        newtag = change[0]\n        newval = change[1]\n\n    tag[newtag] = newval\n    return tag\n</code></pre>"},{"location":"api/osmfile/#osm_merge.osmfile.OsmFile.dump","title":"dump","text":"<pre><code>dump()\n</code></pre> <p>Dump internal data structures, for debugging purposes only.</p> Source code in <code>osm_merge/osmfile.py</code> <pre><code>def dump(self):\n    \"\"\"Dump internal data structures, for debugging purposes only.\"\"\"\n    for entry in self.data:\n        if entry[\"geometry\"][\"type\"] == 'Point':\n            print(\"Node\")\n        else:\n            print(\"Way\")\n        for key, value in entry[\"properties\"].items():\n            print(f\"\\t{key} = {value}\")\n</code></pre>"},{"location":"api/osmfile/#osm_merge.osmfile.OsmFile.getFeature","title":"getFeature","text":"<pre><code>getFeature(id)\n</code></pre> <p>Get the data for a feature from the loaded OSM data file.</p> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>int</code> <p>The ID to retrieve the feasture of</p> required <p>Returns:</p> Type Description <code>dict</code> <p>The feature for this ID or None</p> Source code in <code>osm_merge/osmfile.py</code> <pre><code>def getFeature(\n    self,\n    id: int,\n):\n    \"\"\"Get the data for a feature from the loaded OSM data file.\n\n    Args:\n        id (int): The ID to retrieve the feasture of\n\n    Returns:\n        (dict): The feature for this ID or None\n    \"\"\"\n    return self.data[id]\n</code></pre>"},{"location":"api/osmfile/#osm_merge.osmfile.OsmFile.getFields","title":"getFields","text":"<pre><code>getFields()\n</code></pre> <p>Extract all the tags used in this file.</p> Source code in <code>osm_merge/osmfile.py</code> <pre><code>def getFields(self):\n    \"\"\"Extract all the tags used in this file.\"\"\"\n    fields = list()\n    for _id, item in self.data.items():\n        keys = list(item[\"tags\"].keys())\n        for key in keys:\n            if key not in fields:\n                fields.append(key)\n</code></pre>"},{"location":"api/parsers/","title":"OdkParsers","text":"<p>               Bases: <code>Convert</code></p> <p>A class to parse the CSV files from ODK Central.</p> Source code in <code>osm_merge/fieldwork/parsers.py</code> <pre><code>def __init__(\n    self,\n    yaml: str = None,\n):\n    self.fields = dict()\n    self.nodesets = dict()\n    self.data = list()\n    self.osm = None\n    self.json = None\n    self.features = list()\n    if yaml:\n        pass\n    else:\n        pass\n    self.config = super().__init__(yaml)\n    self.saved = dict()\n    self.defaults = dict()\n    self.entries = dict()\n    self.types = dict()\n</code></pre> <p>options: show_source: false heading_level: 3</p>"},{"location":"api/parsers/#osm_merge.fieldwork.parsers.ODKParsers.basename","title":"basename","text":"<pre><code>basename(line)\n</code></pre> <p>Extract the basename of a path after the last -.</p> <p>Parameters:</p> Name Type Description Default <code>line</code> <code>str</code> <p>The path from the json file entry</p> required <p>Returns:</p> Type Description <code>str</code> <p>The last node of the path</p> Source code in <code>osm_merge/fieldwork/parsers.py</code> <pre><code>def basename(\n        self,\n        line: str,\n) -&gt; str:\n    \"\"\"\n    Extract the basename of a path after the last -.\n\n    Args:\n        line (str): The path from the json file entry\n\n    Returns:\n        (str): The last node of the path\n    \"\"\"\n    if line.find(\"-\") &gt; 0:\n        tmp = line.split(\"-\")\n        if len(tmp) &gt; 0:\n            return tmp[len(tmp) - 1]\n    elif line.find(\":\") &gt; 0:\n        tmp = line.split(\":\")\n        if len(tmp) &gt; 0:\n            return tmp[len(tmp) - 1]\n    else:\n        # return tmp[len(tmp) - 1]\n        return line\n</code></pre>"},{"location":"api/parsers/#osm_merge.fieldwork.parsers.ODKParsers.CSVparser","title":"CSVparser","text":"<pre><code>CSVparser(filespec, data=None)\n</code></pre> <p>Parse the CSV file from ODK Central and convert it to a data structure.</p> <p>Parameters:</p> Name Type Description Default <code>filespec</code> <code>str</code> <p>The file to parse.</p> required <code>data</code> <code>str</code> <p>Or the data to parse.</p> <code>None</code> <p>Returns:</p> Type Description <code>list</code> <p>The list of features with tags</p> Source code in <code>osm_merge/fieldwork/parsers.py</code> <pre><code>def CSVparser(\n    self,\n    filespec: str,\n    data: str = None,\n) -&gt; list:\n    \"\"\"\n    Parse the CSV file from ODK Central and convert it to a data structure.\n\n    Args:\n        filespec (str): The file to parse.\n        data (str): Or the data to parse.\n\n    Returns:\n        (list): The list of features with tags\n    \"\"\"\n    props = list()\n    if not data:\n        f = open(filespec, newline=\"\")\n        reader = csv.DictReader(f, delimiter=\",\")\n    else:\n        reader = csv.DictReader(data, delimiter=\",\")\n    for row in reader:\n        geom = None\n        tags = {\"properties\": dict()}\n        lat = str()\n        lon = str()\n        conv = Convert()\n        # log.debug(f\"ROW: {row}\")\n        for keyword, value in row.items():\n            if keyword is None or value is None or keyword in self.ignore:\n                continue\n            if len(value) == 0:\n                continue\n            base = self.basename(keyword).lower()\n            # There's many extraneous fields in the input file which we don't need.\n            if base is None or base in self.ignore or value is None:\n                continue\n            else:\n                # location, there is not always a value if the accuracy is way\n                # off. In this case use the warmup value, which is where we are\n                # hopefully standing anyway.\n                if base == \"latitude\" and len(lat) == 0:\n                    if len(value) &gt; 0:\n                        lat = value\n                    continue\n                if base == \"warmup-latitude\" and len(lat) == 0:\n                    if len(value) == 0:\n                        lat = value\n                    continue\n\n                if base == \"longitude\" and len(lon) == 0:\n                    if len(value) &gt; 0:\n                        lon = value\n                    continue\n                if base == \"warmup-longitude\" and len(lon) == 0:\n                    if len(value) == 0:\n                        lon = value\n                    continue\n                if len(lat) &gt; 0 and len(lon) &gt; 0:\n                    geom = Point((float(lon), float(lat)))\n\n                if base in self.types:\n                    if self.types[base] == \"select_multiple\":\n                        vals = self.convertMultiple(value)\n                        if len(vals) &gt; 0:\n                            tags.update(vals)\n                        continue\n                    else:\n                        new = conv.convertEntry(base, value)\n                        if type(new) == dict:\n                            tags.update(new)\n                        elif type(new) == list:\n                            for item in new:\n                                [[k, v]] = item.items()\n                                tags[k] = v\n                else:\n                    # if keyword != \"SubmissionDate\":  # DEBUG!\n                    new = conv.convertEntry(base, value)\n                    if type(new) == dict:\n                        tags.update(new)\n                    elif type(new) == list:\n                        for item in new:\n                            [[k, v]] = item.items()\n                            tags[k] = v\n                # tags[\"properties\"].update({base: html.unescape(value)})\n                # items = self.convertEntry(base, value)\n                # # log.info(f\"ROW: {base} {value}\")\n                # if len(items) &gt; 0:\n                #     if base in self.saved:\n                #         if str(value) == \"nan\" or len(value) == 0:\n                #             # log.debug(f\"FIXME: {base} {value}\")\n                #             val = self.saved[base]\n                #             if val and len(value) == 0:\n                #                 log.warning(f'Using last saved value for \"{base}\"! Now \"{val}\"')\n                #                 value = val\n                #         else:\n                #             self.saved[base] = value\n                #             log.debug(f'Updating last saved value for \"{base}\" with \"{value}\"')\n                #     # Handle nested dict in list\n                #     if isinstance(items, list):\n                #         items = items[0]\n                #     for k, v in items.items():\n                #         tags[k] = v\n                # else:\n                #     tags[base] = value\n        props.append(Feature(geometry=geom, properties=tags))\n    return props\n</code></pre>"},{"location":"api/parsers/#osm_merge.fieldwork.parsers.ODKParsers.JSONparser","title":"JSONparser","text":"<pre><code>JSONparser(filespec=None, data=None)\n</code></pre> <p>Parse the JSON file from ODK Central and convert it to a data structure. The input is either a filespec to open, or the data itself.</p> <p>Parameters:</p> Name Type Description Default <code>filespec</code> <code>str</code> <p>The JSON or GeoJson input file to convert</p> <code>None</code> <code>data</code> <code>str</code> <p>The data to convert</p> <code>None</code> <p>Returns:</p> Type Description <code>list</code> <p>A list of all the features in the input file</p> Source code in <code>osm_merge/fieldwork/parsers.py</code> <pre><code>def JSONparser(\n    self,\n    filespec: str = None,\n    data: str = None,\n) -&gt; list:\n    \"\"\"Parse the JSON file from ODK Central and convert it to a data structure.\n    The input is either a filespec to open, or the data itself.\n\n    Args:\n        filespec (str): The JSON or GeoJson input file to convert\n        data (str): The data to convert\n\n    Returns:\n        (list): A list of all the features in the input file\n    \"\"\"\n    log.debug(f\"Parsing JSON file {filespec}\")\n    total = list()\n    if not data:\n        file = open(filespec, \"r\")\n        infile = Path(filespec)\n        if infile.suffix == \".geojson\":\n            reader = geojson.load(file)\n        elif infile.suffix == \".json\":\n            reader = json.load(file)\n        else:\n            log.error(\"Need to specify a JSON or GeoJson file!\")\n            return total\n    elif isinstance(data, str):\n        reader = geojson.loads(data)\n    elif isinstance(data, list):\n        reader = data\n\n    # JSON files from Central use value as the keyword, whereas\n    # GeoJSON uses features for the same thing.\n    if \"value\" in reader:\n        data = reader[\"value\"]\n    elif \"features\" in reader:\n        data = reader[\"features\"]\n    else:\n        data = reader\n    for row in data:\n        # log.debug(f\"ROW: {row}\\n\")\n        tags = dict()\n        if \"properties\" in row:\n            row[\"properties\"]  # A GeoJson formatted file\n        else:\n            pass  # A JOSM file from ODK Central\n\n        # flatten all the groups into a single data structure\n        flattened = flatdict.FlatDict(row)\n        # log.debug(f\"FLAT: {flattened}\\n\")\n        for k, v in flattened.items():\n            last = k.rfind(\":\") + 1\n            key = k[last:]\n            # a JSON file from ODK Central always uses coordinates as\n            # the keyword\n            if key is None or key in self.ignore or v is None:\n                continue\n            # log.debug(f\"Processing tag {key} = {v}\")\n            if key == \"coordinates\":\n                if isinstance(v, list):\n                    geom = Point((float(v[0]), float(v[1])))\n                    # tags[\"geometry\"] = poi\n                continue\n\n            if key in self.types:\n                if self.types[key] == \"select_multiple\":\n                    # log.debug(f\"Found key '{self.types[key]}'\")\n                    if v is None:\n                        continue\n                    vals = self.convertMultiple(v)\n                    if len(vals) &gt; 0:\n                        tags.update(vals)\n                    continue\n            items = self.convertEntry(key, v)\n            if items is None or len(items) == 0:\n                continue\n\n            if type(items) == str:\n                log.debug(f\"string Item {items}\")\n            elif type(items) == list:\n                # log.debug(f\"list Item {items}\")\n                tags.update(items[0])\n            elif type(items) == dict:\n                # log.debug(f\"dict Item {items}\")\n                tags.update(items)\n        # log.debug(f\"TAGS: {tags}\")\n        if len(tags) &gt; 0:\n            total.append(Feature(geometry=geom, properties=tags))\n    # log.debug(f\"Finished parsing JSON file {filespec}\")\n    return total\n</code></pre>"},{"location":"api/parsers/#osm_merge.fieldwork.parsers.ODKParsers.XMLparser","title":"XMLparser","text":"<pre><code>XMLparser(filespec, data=None)\n</code></pre> <p>Import an ODK XML Instance file ito a data structure. The input is either a filespec to the Instance file copied off your phone, or the XML that has been read in elsewhere.</p> <p>Parameters:</p> Name Type Description Default <code>filespec</code> <code>str</code> <p>The filespec to the ODK XML Instance file</p> required <code>data</code> <code>str</code> <p>The XML data</p> <code>None</code> <p>Returns:</p> Type Description <code>list</code> <p>All the entries in the OSM XML Instance file</p> Source code in <code>osm_merge/fieldwork/parsers.py</code> <pre><code>def XMLparser(\n    self,\n    filespec: str,\n    data: str = None,\n) -&gt; list:\n    \"\"\"Import an ODK XML Instance file ito a data structure. The input is\n    either a filespec to the Instance file copied off your phone, or\n    the XML that has been read in elsewhere.\n\n    Args:\n        filespec (str): The filespec to the ODK XML Instance file\n        data (str): The XML data\n\n    Returns:\n        (list): All the entries in the OSM XML Instance file\n    \"\"\"\n    row = list()\n    if filespec:\n        logging.info(\"Processing instance file: %s\" % filespec)\n        file = open(filespec, \"rb\")\n        # Instances are small, read the whole file\n        xml = file.read(os.path.getsize(filespec))\n    elif data:\n        xml = data\n    doc = xmltodict.parse(xml)\n\n    json.dumps(doc)\n    props = dict()\n    data = doc[\"data\"]\n    flattened = flatdict.FlatDict(data)\n    geom = None\n    # total = list()\n    # log.debug(f\"FLAT: {flattened}\")\n    pat = re.compile(\"[0-9.]* [0-9.-]* [0-9.]* [0-9.]*\")\n    for key, value in flattened.items():\n        if key[0] == \"@\" or value is None:\n            continue\n        # Get the last element deliminated by a dash\n        # for CSV &amp; JSON, or a colon for ODK XML.\n        base = self.basename(key)\n        if base in self.ignore:\n            continue\n        if re.search(pat, value):\n            gps = value.split(\" \")\n            geom = Point((float(gps[1]), float(gps[0])))\n            continue\n\n        if len(self.types) == 0:\n            props[key] = value\n\n        elif base in self.types:\n            if self.types[base] == \"select_multiple\":\n                # log.debug(f\"Found key '{self.types[base]}'\")\n                vals = self.convertMultiple(value)\n                if len(vals) &gt; 0:\n                    props.update(vals)\n                continue\n            else:\n                item = self.convertEntry(base, value)\n                if item is None or len(item) == 0:\n                    continue\n                if len(props) == 0:\n                    props = item[0]\n                else:\n                    if type(item) == list:\n                        # log.debug(f\"list Item {item}\")\n                        props.update(item[0])\n                    elif type(item) == dict:\n                        # log.debug(f\"dict Item {item}\")\n                        props.update(item)\n    return Feature(geometry=geom, properties=props)\n</code></pre>"},{"location":"api/sqlite/","title":"SQLite","text":"<p>options: show_source: false heading_level: 3</p>"},{"location":"api/sqlite/#osm_merge.fieldwork.sqlite.MapTile","title":"MapTile","text":"<pre><code>MapTile(\n    x=None,\n    y=None,\n    z=None,\n    filespec=None,\n    tile=None,\n    suffix=\"jpg\",\n)\n</code></pre> <p>               Bases: <code>object</code></p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>int</code> <p>The X index for this tile</p> <code>None</code> <code>y</code> <code>int</code> <p>The Y index for this tile</p> <code>None</code> <code>z</code> <code>int</code> <p>The Z index for this tile if there is one</p> <code>None</code> <code>filespec</code> <code>str</code> <p>The location of this within the map tile cache</p> <code>None</code> <code>tile</code> <code>MapTile</code> <p>Make a copy of this object</p> <code>None</code> <code>suffix</code> <code>str</code> <p>The image suffix, jpg or png usually</p> <code>'jpg'</code> <p>Returns:</p> Type Description <code>MapTile</code> <p>An instance of this object</p> Source code in <code>osm_merge/fieldwork/sqlite.py</code> <pre><code>def __init__(\n    self,\n    x: int = None,\n    y: int = None,\n    z: int = None,\n    filespec: str = None,\n    tile: \"MapTile\" = None,\n    suffix=\"jpg\",\n):\n    \"\"\"This is a simple wrapper around mercantile.tile to associate a\n    filespec with the grid coordinates.\n\n    Args:\n        x (int): The X index for this tile\n        y (int): The Y index for this tile\n        z (int): The Z index for this tile if there is one\n        filespec (str): The location of this within the map tile cache\n        tile (MapTile): Make a copy of this object\n        suffix (str): The image suffix, jpg or png usually\n\n    Returns:\n        (MapTile): An instance of this object\n    \"\"\"\n    if tile:\n        self.x = tile.x\n        self.y = tile.y\n        self.z = tile.z\n    else:\n        self.x = x\n        self.y = y\n        self.z = z\n    self.blob = None\n    self.filespec = None\n    if not filespec and self.z:\n        self.filespec = f\"{self.z}/{self.y}/{self.x}.{suffix}\"\n    elif filespec:\n        self.filespec = filespec\n        tmp = filespec.split(\"/\")\n        self.z = tmp[0]\n        self.x = tmp[2]\n        self.y = tmp[1].replace(\".\" + suffix, \"\")\n</code></pre>"},{"location":"api/sqlite/#osm_merge.fieldwork.sqlite.MapTile.readImage","title":"readImage","text":"<pre><code>readImage(base='./')\n</code></pre> <p>Read a map tile out of the disk based map tile cache.</p> <p>Parameters:</p> Name Type Description Default <code>base</code> <code>str</code> <p>The top level directory for the map tile cache</p> <code>'./'</code> Source code in <code>osm_merge/fieldwork/sqlite.py</code> <pre><code>def readImage(self, base: str = \"./\"):\n    \"\"\"Read a map tile out of the disk based map tile cache.\n\n    Args:\n        base (str): The top level directory for the map tile cache\n    \"\"\"\n    file = f\"{base}/{self.filespec}\"\n    logging.debug(\"Adding tile image: %s\" % file)\n    if os.path.exists(file):\n        size = os.path.getsize(file)\n        file = open(file, \"rb\")\n        self.blob = file.read(size)\n</code></pre>"},{"location":"api/sqlite/#osm_merge.fieldwork.sqlite.MapTile.dump","title":"dump","text":"<pre><code>dump()\n</code></pre> <p>Dump internal data structures, for debugging purposes only.</p> Source code in <code>osm_merge/fieldwork/sqlite.py</code> <pre><code>def dump(self):\n    \"\"\"Dump internal data structures, for debugging purposes only.\"\"\"\n    if self.z:\n        print(\"Z: %r\" % self.z)\n    if self.x:\n        print(\"X: %r\" % self.x)\n    if self.y:\n        print(\"Y: %r\" % self.y)\n    print(\"Filespec: %s\" % self.filespec)\n    if self.blob:\n        print(\"Tile size is: %d\" % len(self.blob))\n</code></pre>"},{"location":"api/sqlite/#osm_merge.fieldwork.sqlite.DataFile","title":"DataFile","text":"<pre><code>DataFile(dbname=None, suffix='jpg', append=False)\n</code></pre> <p>               Bases: <code>object</code></p> <p>Parameters:</p> Name Type Description Default <code>dbname</code> <code>str</code> <p>The name of the output sqlite file</p> <code>None</code> <code>suffix</code> <code>str</code> <p>The image suffix, jpg or png usually</p> <code>'jpg'</code> <code>append</code> <code>bool</code> <p>Whether to append to or create the database</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFile</code> <p>An instance of this class</p> Source code in <code>osm_merge/fieldwork/sqlite.py</code> <pre><code>def __init__(\n    self,\n    dbname: str = None,\n    suffix: str = \"jpg\",\n    append: bool = False,\n):\n    \"\"\"Handle the sqlite3 database file.\n\n    Args:\n        dbname (str): The name of the output sqlite file\n        suffix (str): The image suffix, jpg or png usually\n        append (bool): Whether to append to or create the database\n\n    Returns:\n        (DataFile): An instance of this class\n    \"\"\"\n    self.db = None\n    self.cursor = None\n    if dbname:\n        self.createDB(dbname, append)\n    self.dbname = dbname\n    self.metadata = None\n    self.toplevel = None\n    self.suffix = suffix\n</code></pre>"},{"location":"api/sqlite/#osm_merge.fieldwork.sqlite.DataFile.addBounds","title":"addBounds","text":"<pre><code>addBounds(bounds)\n</code></pre> <p>Mbtiles has a bounds field, Osmand doesn't.</p> <p>Parameters:</p> Name Type Description Default <code>bounds</code> <code>int</code> <p>The bounds value for ODK Collect mbtiles</p> required Source code in <code>osm_merge/fieldwork/sqlite.py</code> <pre><code>def addBounds(\n    self,\n    bounds: tuple[float, float, float, float],\n):\n    \"\"\"Mbtiles has a bounds field, Osmand doesn't.\n\n    Args:\n        bounds (int): The bounds value for ODK Collect mbtiles\n    \"\"\"\n    entry = str(bounds)\n    entry = entry[1 : len(entry) - 1].replace(\" \", \"\")\n    self.cursor.execute(f\"INSERT OR IGNORE INTO metadata (name, value) VALUES('bounds', '{entry}') \")\n</code></pre>"},{"location":"api/sqlite/#osm_merge.fieldwork.sqlite.DataFile.addZoomLevels","title":"addZoomLevels","text":"<pre><code>addZoomLevels(zoom_levels)\n</code></pre> <p>Mbtiles has a maxzoom and minzoom fields, Osmand doesn't.</p> <p>Parameters:</p> Name Type Description Default <code>zoom_levels</code> <code>list</code> <p>The zoom levels</p> required Source code in <code>osm_merge/fieldwork/sqlite.py</code> <pre><code>def addZoomLevels(\n    self,\n    zoom_levels: list[int],\n):\n    \"\"\"Mbtiles has a maxzoom and minzoom fields, Osmand doesn't.\n\n    Args:\n        zoom_levels (list)): The zoom levels\n    \"\"\"\n    min_zoom = min(zoom_levels)\n    max_zoom = max(zoom_levels)\n    self.cursor.execute(f\"INSERT OR IGNORE INTO metadata (name, value) VALUES('minzoom', '{min_zoom}') \")\n    self.cursor.execute(f\"INSERT OR IGNORE INTO metadata (name, value) VALUES('maxzoom', '{max_zoom}') \")\n</code></pre>"},{"location":"api/sqlite/#osm_merge.fieldwork.sqlite.DataFile.createDB","title":"createDB","text":"<pre><code>createDB(dbname, append=False)\n</code></pre> <p>Create and sqlitedb in either mbtiles or Osman sqlitedb format.</p> <p>Parameters:</p> Name Type Description Default <code>dbname</code> <code>str</code> <p>The filespec of the sqlite output file</p> required Source code in <code>osm_merge/fieldwork/sqlite.py</code> <pre><code>def createDB(\n    self,\n    dbname: str,\n    append: bool = False,\n):\n    \"\"\"Create and sqlitedb in either mbtiles or Osman sqlitedb format.\n\n    Args:\n        dbname (str): The filespec of the sqlite output file\n    \"\"\"\n    suffix = os.path.splitext(dbname)[1]\n\n    if os.path.exists(dbname) and append == False:\n        os.remove(dbname)\n\n    self.db = sqlite3.connect(dbname)\n    self.cursor = self.db.cursor()\n    self.cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table' AND name='tiles'\")\n    exists = self.cursor.fetchone()\n    if exists and append:\n        logging.info(\"Appending to database file %s\" % dbname)\n        return\n\n    if suffix == \".mbtiles\":\n        self.cursor.execute(\"CREATE TABLE tiles (zoom_level integer, tile_column integer, tile_row integer, tile_data blob)\")\n        self.cursor.execute(\"CREATE INDEX tiles_idx on tiles (zoom_level, tile_column, tile_row)\")\n        self.cursor.execute(\"CREATE TABLE metadata (name text, value text)\")\n        # These get populated later\n        name = dbname\n        description = \"Created by osm_merge/basemapper.py\"\n        self.cursor.execute(\"CREATE UNIQUE INDEX metadata_idx  ON metadata (name)\")\n        self.cursor.execute(\"INSERT INTO metadata (name, value) VALUES('version', '1.1')\")\n        self.cursor.execute(\"INSERT INTO metadata (name, value) VALUES('type', 'baselayer')\")\n        self.cursor.execute(f\"INSERT INTO metadata (name, value) VALUES('name', '{name}')\")\n        self.cursor.execute(f\"INSERT INTO metadata (name, value) VALUES('description', '{description}')\")\n        # self.cursor.execute(f\"INSERT INTO metadata (name, value) VALUES('bounds', '{bounds}')\")\n        self.cursor.execute(\"INSERT INTO metadata (name, value) VALUES('format', 'jpg')\")\n    if \"sqlite\" in suffix:\n        # s is always 0\n        self.cursor.execute(\"CREATE TABLE tiles (x int, y int, z int, s int, image blob, PRIMARY KEY (x,y,z,s));\")\n        self.cursor.execute(\"CREATE INDEX IND on tiles (x,y,z,s)\")\n        # Info is simple \"2|4\" for example, it gets populated later\n        self.cursor.execute(\"CREATE TABLE info (maxzoom Int, minzoom Int);\")\n        # the metadata is the locale as a string\n        loc = locale.getlocale()[0]\n        self.cursor.execute(f\"CREATE TABLE  android_metadata ({loc})\")\n    self.db.commit()\n    logging.info(\"Created database file %s\" % dbname)\n</code></pre>"},{"location":"api/sqlite/#osm_merge.fieldwork.sqlite.DataFile.writeTiles","title":"writeTiles","text":"<pre><code>writeTiles(tiles, base='./', image_format='jpg')\n</code></pre> <p>Write map tiles into the to the map tile cache.</p> <p>Parameters:</p> Name Type Description Default <code>tiles</code> <code>list</code> <p>The map tiles to write to the map tile cache</p> required <code>base</code> <code>str</code> <p>The default local to write tiles to disk</p> <code>'./'</code> Source code in <code>osm_merge/fieldwork/sqlite.py</code> <pre><code>def writeTiles(self, tiles: list, base: str = \"./\", image_format: str = \"jpg\"):\n    \"\"\"Write map tiles into the to the map tile cache.\n\n    Args:\n        tiles (list): The map tiles to write to the map tile cache\n        base (str): The default local to write tiles to disk\n    \"\"\"\n    for tile in tiles:\n        xyz = MapTile(tile=tile, suffix=image_format)\n        xyz.readImage(base)\n        # xyz.dump()\n        self.writeTile(xyz)\n</code></pre>"},{"location":"api/sqlite/#osm_merge.fieldwork.sqlite.DataFile.writeTile","title":"writeTile","text":"<pre><code>writeTile(tile)\n</code></pre> <p>Write a map tile into the sqlite database file.</p> <p>Parameters:</p> Name Type Description Default <code>tile</code> <code>MapTile</code> <p>The map tile to write to the file</p> required Source code in <code>osm_merge/fieldwork/sqlite.py</code> <pre><code>def writeTile(\n    self,\n    tile: MapTile,\n):\n    \"\"\"Write a map tile into the sqlite database file.\n\n    Args:\n        tile (MapTile): The map tile to write to the file\n    \"\"\"\n    if tile.blob is None:\n        logging.error(f\"Map tile {tile.filespec} has no image data!\")\n        # tile.dump()\n        return False\n\n    suffix = os.path.splitext(self.dbname)[1]\n\n    if \"sqlite\" in suffix:\n        # Osmand tops out at zoom level 16, so the real zoom level is inverse,\n        # and can go negative for really high zoom levels.\n        z = 17 - tile.z\n        self.db.execute(\n            \"INSERT INTO tiles (x, y, z, s, image) VALUES (?, ?, ?, ?, ?)\",\n            [tile.x, tile.y, z, 0, sqlite3.Binary(tile.blob)],\n        )\n\n    if suffix == \".mbtiles\":\n        y = (1 &lt;&lt; tile.z) - tile.y - 1\n        self.db.execute(\n            \"INSERT INTO tiles (tile_row, tile_column, zoom_level, tile_data) VALUES (?, ?, ?, ?)\",\n            [y, tile.x, tile.z, sqlite3.Binary(tile.blob)],\n        )\n\n    self.db.commit()\n</code></pre>"},{"location":"api/yamlfile/","title":"YamlFile","text":"<p>options: show_source: false heading_level: 3</p>"},{"location":"api/yamlfile/#osm_merge.yamlfile.YamlFile","title":"YamlFile","text":"<pre><code>YamlFile(filespec)\n</code></pre> <p>               Bases: <code>object</code></p> <p>Config file in YAML format.</p> <p>Parameters:</p> Name Type Description Default <code>filespec</code> <code>str</code> <p>The filespec of the YAML file to read</p> required <p>Returns:</p> Type Description <code>YamlFile</code> <p>An instance of this object</p> Source code in <code>osm_merge/yamlfile.py</code> <pre><code>def __init__(\n    self,\n    filespec: str,\n):\n    \"\"\"This parses a yaml file into a dictionary for easy access.\n\n    Args:\n        filespec (str): The filespec of the YAML file to read\n\n    Returns:\n        (YamlFile): An instance of this object\n    \"\"\"\n    self.filespec = None\n    # if data == str:\n    self.filespec = filespec\n    self.file = open(filespec, \"rb\").read()\n    self.yaml = yaml.load(self.file, Loader=yaml.Loader)\n    self.data = dict()\n</code></pre>"},{"location":"api/yamlfile/#osm_merge.yamlfile.YamlFile.get","title":"get","text":"<pre><code>get(keyword, tag=None)\n</code></pre> <p>Get the values for a top level keyword Args:     keyword (str): The top level keyword to get the values for     tag (str): The category for the tag/values</p> <p>Returns:</p> Type Description <code>dict</code> <p>The values for the top level keyword</p> Source code in <code>osm_merge/yamlfile.py</code> <pre><code>def get(self,\n        keyword: str,\n        tag: str = None,\n        ):\n    \"\"\"\n    Get the values for a top level keyword\\\n\n    Args:\n        keyword (str): The top level keyword to get the values for\n        tag (str): The category for the tag/values\n\n    Returns:\n        (dict): The values for the top level keyword\n    \"\"\"\n    return self.yaml[keyword][tag]\n</code></pre>"},{"location":"api/yamlfile/#osm_merge.yamlfile.YamlFile.getEntries","title":"getEntries","text":"<pre><code>getEntries()\n</code></pre> <p>Convert the list from the YAML file into a searchable data structure</p> <p>Returns:</p> Type Description <code>dict</code> <p>The parsed config file</p> Source code in <code>osm_merge/yamlfile.py</code> <pre><code>def getEntries(self):\n    \"\"\"\n    Convert the list from the YAML file into a searchable data structure\n\n    Returns:\n        (dict): The parsed config file\n    \"\"\"\n    columns = list()\n    for entry in self.yaml:\n        for key, values in entry.items():\n            self.data[key] = dict()\n            # values is a list of dicts which are tag/value pairs\n            for item in values:\n                [[k, v]] = item.items()\n                if type(v) == str:\n                    self.data[key][k] = v\n                elif type(v) == list:\n                    self.data[key][k] = dict()                        \n                    for convert in v:\n                        self.data[key][k].update(convert)\n                else:\n                    log.error(f\"{type(v)} is not suported.\")\n\n    return self.data\n</code></pre>"},{"location":"api/yamlfile/#osm_merge.yamlfile.YamlFile.dump","title":"dump","text":"<pre><code>dump()\n</code></pre> <p>Dump internal data structures, for debugging purposes only.</p> Source code in <code>osm_merge/yamlfile.py</code> <pre><code>def dump(self):\n    \"\"\"Dump internal data structures, for debugging purposes only.\"\"\"\n    if self.filespec:\n        print(\"YAML file: %s\" % self.filespec)\n    for key, values in self.data.items():\n        print(f\"Key is: {key}\")\n        for k, v in values.items():\n            print(f\"\\t{k} = {v}\")\n        print(\"------------------\")\n</code></pre>"}]}